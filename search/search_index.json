{"config":{"lang":["en"],"separator":"[\\s\\-\\.]"},"docs":[{"title":"Welcome to Quix Docs","text":"","location":""},{"title":"Tutorials","text":"<p>Try our guides to help you build data-driven apps and integrate Quix with external systems.</p>  <ul> <li> <p>Sentiment Analysis</p>  <p>Stream data from Twitter and build a Sentiment analysis pipeline</p> <p> Sentiment Analysis</p> </li> <li> <p>Image Processing</p>  <p>Real time image processing using Londons 'Jam Cams'</p> <p> Image Processing</p> </li> </ul>","location":"#tutorials"},{"title":"Core resources","text":"<p>Take a look under the hood and get to know our SDK and APIs</p>  <ul> <li> <p>Connect to Quix - SDK</p>  <p>Discover how to connect Quix and your application using our SDK</p> <p> Learn more</p> </li> <li> <p>Writing data - SDK</p>  <p>Read how to send real-time data to Kafka topics using the Quix SDK</p> <p> Learn more</p> </li> <li> <p>Reading data - SDK</p>  <p>Learn how to receive real-time data in your application using the Quix SDK</p> <p> Learn more</p> </li> <li> <p>Streaming Writer API</p>  <p>Stream data to Quix Kafka topics via HTTP with this API</p> <p> Learn more</p> </li> <li> <p>Streaming Reader API</p>  <p>Work with this API to receive live data in your Web applications from Quix Kafka topics via Websockets</p> <p> Learn more</p> </li> <li> <p>Data Catalogue API</p>  <p>Query historic time-series data in Quix using HTTP interface</p> <p> Learn more</p> </li> </ul>","location":"#core-resources"},{"title":"Index","text":"<p>The Quix Platform provides the following APIs.</p>","location":"apis/"},{"title":"Data Catalogue","text":"<p>The Data Catalogue HTTP API allows you to fetch data stored in the Quix platform. You can use it for exploring the platform, prototyping applications, or working with stored data in any language with HTTP capabilities.</p>","location":"apis/#data-catalogue"},{"title":"Streaming Writer","text":"<p>The Streaming Writer API allows you to stream data into the Quix platform via HTTP. It\u2019s an alternative to using our C# and Python SDKs. You can use the Streaming Writer API from any HTTP-capable language.</p>","location":"apis/#streaming-writer"},{"title":"Streaming Reader","text":"<p>As an alternative to the SDK, the Quix platform supports real-time data streaming over WebSockets, via the Streaming Reader API. Clients can receive updates on data and definitions for parameters and events, as they happen. The examples use the Microsoft SignalR JavaScript client library.</p>","location":"apis/#streaming-reader"},{"title":"Portal API","text":"<p>The Portal API gives access to the Portal interface allowing you to automate access to data including Users, Workspaces, and Projects.</p>","location":"apis/#portal-api"},{"title":"Portal api","text":"<p>Portal API gives access to the Portal interface allowing you to automate access to data including Users, Workspaces, and Projects.</p> <p>Refer to Portal API Swagger for more information.</p>","location":"apis/portal-api/"},{"title":"Aggregate data by tags","text":"<p>If you need to compare data across different values for a given tag, you\u2019ll want to group results by that tag. You can do so via the <code>/parameters/data</code> endpoint.</p>","location":"apis/data-catalogue-api/aggregate-tags/"},{"title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can     use any Source of our Quix     Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>","location":"apis/data-catalogue-api/aggregate-tags/#before-you-begin"},{"title":"Using the groupBy property","text":"<p>You can supply a list of Tags in the <code>groupBy</code> array to aggregate results by. For example, you could group a set of Speed readings by the LapNumber they occurred on using something like:</p> <pre><code>{\n    \"from\": 1612191286000000000,\n    \"to\":   1612191386000000000,\n    \"numericParameters\": [{\n        \"parameterName\": \"Speed\"\n    }],\n    \"groupBy\": [ \"LapNumber\" ]\n}\n</code></pre> <p>With these settings alone, we\u2019ll get the <code>LapNumber</code> tag included in our results, alongside the existing timestamps and requested parameters, e.g.</p> <pre><code>{\n    \"timestamps\": [\n        1612191286000000000,\n        1612191287000000000,\n        ...\n    ],\n    \"numericValues\": {\n        \"Speed\": [\n            307.8333333333333,\n            313.8421052631579,\n            ...\n        ]\n    },\n    \"tagValues\": {\n        \"LapNumber\": [\n            \"3.0\",\n            \"4.0\",\n            ...\n        ]\n    }\n}\n</code></pre>","location":"apis/data-catalogue-api/aggregate-tags/#using-the-groupby-property"},{"title":"Using aggregationType","text":"<p>For meaningful aggregations, you should specify a type of aggregation function for each parameter. When specifying the parameters to receive, include the <code>aggregationType</code> in each parameter object like so:</p> <pre><code>\"numericParameters\": [{\n    \"parameterName\": \"Speed\",\n    \"aggregationType\": \"mean\"\n}]\n</code></pre> <p>Ten standard aggregation functions are provided including <code>max</code>, <code>count</code>, and <code>spread</code>. When you group by a tag and specify how to aggregate parameter values, the result will represent that aggregation. For example, the following results demonstrate the average speed that was recorded against each lap:</p> <pre><code>{\n  \"timestamps\": [\n    1612191286000000000,\n    1612191286000000000\n  ],\n  \"numericValues\": {\n    \"mean(Speed)\": [\n      213.36765142704252,\n      173.77710595934278\n    ]\n  },\n  \"stringValues\": {},\n  \"binaryValues\": {},\n  \"tagValues\": {\n    \"LapNumber\": [\n      \"3.0\",\n      \"4.0\"\n    ]\n  }\n}\n</code></pre>","location":"apis/data-catalogue-api/aggregate-tags/#using-aggregationtype"},{"title":"Aggregate data by time","text":"<p>You can downsample and upsample data from the catalogue using the <code>/parameters/data</code> endpoint.</p>","location":"apis/data-catalogue-api/aggregate-time/"},{"title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can     use any Source of our Quix     Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>","location":"apis/data-catalogue-api/aggregate-time/#before-you-begin"},{"title":"Aggregating and interpolating","text":"<p>The JSON payload can include a <code>groupByTime</code> property, an object with the following members:</p> <ul> <li> <p><code>timeBucketDuration</code>     The duration, in nanoseconds, for one aggregated value.</p> </li> <li> <p><code>interpolationType</code>     Specify how additional values should be generated when     interpolating.</p> </li> </ul> <p>For example, imagine you have a set of speed data, with values recorded at 1-second intervals. You can group such data into 2-second intervals, aggregated by mean average, with the following:</p> <pre><code>{\n    \"groupByTime\": {\n        \"timeBucketDuration\": 2000000000,\n    },\n    \"numericParameters\": [{\n        \"parameterName\": \"Speed\",\n        \"aggregationType\": \"Mean\"\n    }]\n}\n</code></pre> <p>You can specify an <code>interpolationType</code> to define how any missing values are generated. <code>Linear</code> will provide a value in linear proportion, whilst <code>Previous</code> will repeat the value before the one that was missing.</p> <pre><code>{\n    \"from\": 1612191286000000000,\n    \"to\":   1612191295000000000,\n    \"numericParameters\": [{\n        \"parameterName\": \"Speed\",\n        \"aggregationType\": \"First\"\n    }],\n    \"groupByTime\": {\n        \"timeBucketDuration\": 2000000000,\n        \"interpolationType\": \"None\"\n    },\n    \"streamIds\": [ \"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\" ]\n}\n</code></pre>","location":"apis/data-catalogue-api/aggregate-time/#aggregating-and-interpolating"},{"title":"Before you begin","text":"<ul> <li>Sign up on the Quix Portal</li> </ul>","location":"apis/data-catalogue-api/authenticate/"},{"title":"Get a Personal Access Token","text":"<p>You should authenticate requests to the Catalogue API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary.</p> <p>Follow these steps to generate a PAT:</p> <ol> <li> <p>Click the user icon in the top-right of the Portal and select the     Tokens menu.</p> </li> <li> <p>Click GENERATE TOKEN.</p> </li> <li> <p>Choose a name to describe the token\u2019s purpose, and an expiration     date, then click CREATE.</p> </li> <li> <p>Copy the token and store it in a secure place.</p> </li> </ol>  <p>Warning</p> <p>You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy.</p>   <p>Warning</p> <p>Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs.</p>","location":"apis/data-catalogue-api/authenticate/#get-a-personal-access-token"},{"title":"Sign all your requests using this token","text":"<p>Make sure you accompany each request to the API with an <code>Authorization</code> header using your PAT as a bearer token, as follows:</p> <pre><code>Authorization: bearer &lt;token&gt;\n</code></pre> <p>Replace <code>&lt;token&gt;</code> with your Personal Access Token. For example, if you\u2019re using curl on the command line, you can set the header using the <code>-H</code> flag:</p> <pre><code>curl -H \"Authorization: bearer &lt;token&gt;\" ...\n</code></pre>  <p>Warning</p> <p>If you fail to send a valid Authorization header, the API will respond with a <code>401 UNAUTHORIZED</code> status code.</p>","location":"apis/data-catalogue-api/authenticate/#sign-all-your-requests-using-this-token"},{"title":"Tag filtering","text":"<p>If you supply Tags with your parameter data, they will act as indexes, so they can be used to efficiently filter data.</p>","location":"apis/data-catalogue-api/filter-tags/"},{"title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can     use any Source of our Quix     Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>","location":"apis/data-catalogue-api/filter-tags/#before-you-begin"},{"title":"Using tag filters","text":"<p>When calling the <code>/parameters/data</code> endpoint, you can include a <code>tagFilters</code> property in your payload. This property references an array of objects, each with the following structure:</p> <ul> <li> <p>tag     The name of the tag to filter by</p> </li> <li> <p>operator     A comparison operator</p> </li> <li> <p>value     The value to compare against</p> </li> </ul> <p>For example, to fetch only the data recorded on the second lap, we can filter on a <code>LapNumber</code> tag as follows:</p> <pre><code>{\n    \"tagFilters\": [{\n        \"tag\": \"LapNumber\",\n        \"operator\": \"Equal\",\n        \"value\": \"2.0\"\n    }]\n}\n</code></pre> <p>Note that the value can also be an array, in which case data that matches the chosen operator for any value is returned:</p> <pre><code>{\n    \"tagFilters\": [{\n        \"tag\": \"LapNumber\",\n        \"operator\": \"Equal\",\n        \"value\": [ \"2.0\", \"4.0\" ]\n    }]\n}\n</code></pre> <p>But also note that multiple filters for the same tag apply in combination, so:</p> <pre><code>{\n    \"tagFilters\": [{\n        \"tag\": \"LapNumber\",\n        \"operator\": \"Equal\",\n        \"value\": \"2.0\"\n    },{\n        \"tag\": \"LapNumber\",\n        \"operator\": \"Equal\",\n        \"value\": \"4.0\"\n    }]\n}\n</code></pre> <p>Is useless because a LapNumber cannot be both \"2.0\" and \"4.0\".</p>","location":"apis/data-catalogue-api/filter-tags/#using-tag-filters"},{"title":"Supported operators","text":"<p>Each object in the <code>tagFilters</code> array can support the following <code>operator</code> values:</p> <ul> <li> <p>Equal</p> </li> <li> <p>NotEqual</p> </li> <li> <p>Like</p> </li> <li> <p>NotLike</p> </li> </ul> <p><code>Equal</code> and <code>NotEqual</code> test for true/false exact string matches.</p> <p><code>Like</code> and <code>NotLike</code> will perform a regular expression match, so you can search by pattern. For example, to get the Speed parameter values tagged with a LapNumber which is either 2 or 4, you can use the expression \"^[24]\\.\" to match values 2.0 and 4.0:</p> <pre><code>curl \"https://telemetry-query-testing-quickstart.platform.quix.ai/parameters/data\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n         \"tagFilters\": [{\n             \"tag\": \"LapNumber\",\n             \"operator\": \"Like\",\n             \"value\": \"^[24]\\\\.\"\n         }],\n         \"numericParameters\": [{\"parameterName\": \"Speed\"}],\n         \"from\": 1612191182000000000,\n         \"to\": 1612191189000000000\n     }'\n</code></pre>","location":"apis/data-catalogue-api/filter-tags/#supported-operators"},{"title":"Getting Swagger url","text":"<p>You can access documentation and a basic playground for the Data Catalogue API via Swagger. The exact URL is workspace-specific, and follows this pattern:</p> <pre><code>https://telemetry-query-${organisation}-${workspace}.platform.quix.ai/\n</code></pre> <p>The workspace ID is a combination based on your organisation and workspace names. For example, for an \"acme\" organisation with a \"weather\" workspace, the final URL might look something like:</p> <pre><code>https://telemetry-query-acme-weather.platform.quix.ai/\n</code></pre> <p>To determine the final URL, you can find out how to get a workspace id, or follow these instructions:</p> <ol> <li> <p>Click the Data Explorer icon Data Explorer in the main menu.</p> </li> <li> <p>Click on CODE tab in the right top corner.</p> </li> <li> <p>Using the \"Select code language\" drop down on the right of this     page, choose \"Swagger\".</p> </li> </ol> <p></p> <ol> <li>Click the link to access the Swagger documentation.</li> </ol>","location":"apis/data-catalogue-api/get-swagger/"},{"title":"Introduction","text":"<p>The Data Catalogue HTTP API allows you to fetch data stored in the Quix platform. You can use it for exploring the platform, prototyping applications, or working with stored data in any language with HTTP capabilities.</p> <p>The API is fully described in our Swagger documentation. Read on for a guide to using the API, including real-world examples you can execute from your language of choice, or via the command line using <code>curl</code>.</p>","location":"apis/data-catalogue-api/intro/"},{"title":"Preparation","text":"<p>Before using any of the endpoints, you\u2019ll need to know how to authenticate your requests and how to form a typical request to the API.</p> <p>You\u2019ll also need to have some data stored in the Quix platform for API use to be meaningful. You can use any Source of our Quix Library to do this using the Quix portal.</p>","location":"apis/data-catalogue-api/intro/#preparation"},{"title":"Topics covered","text":"Topic Endpoint Examples   Streams, paged <code>/streams</code> Get all streams in groups of ten per page   Streams, filtered <code>/streams</code> Get a single stream, by ID     Get only the streams with LapNumber data   Streams &amp; models <code>/streams/models</code> Get stream hierarchy   Raw data <code>/parameters/data</code> Get all the <code>Speed</code> readings     Get <code>Speed</code> data between timestamps   Aggregated data by time <code>/parameters/data</code> Downsample or upsample data   Aggregated by tags <code>/parameters/data</code> Show average Speed by LapNumber   Tag filtering <code>/parameters/data</code> Get data for just one Lap","location":"apis/data-catalogue-api/intro/#topics-covered"},{"title":"Raw data","text":"<p>Access persisted raw data by specifyng the parameters you\u2019re interested in. Add restrictions based on Stream or timings for finer-grained results.</p>","location":"apis/data-catalogue-api/raw-data/"},{"title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can     use any Source of our Quix     Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>","location":"apis/data-catalogue-api/raw-data/#before-you-begin"},{"title":"Using the /parameters/data endpoint","text":"<p>Raw telemetry data is available via the <code>/parameters/data</code> endpoint.</p>","location":"apis/data-catalogue-api/raw-data/#using-the-parametersdata-endpoint"},{"title":"Fetching specific parameters","text":"","location":"apis/data-catalogue-api/raw-data/#fetching-specific-parameters"},{"title":"Request","text":"<p>You can filter by a number of different factors but, at minimum, you\u2019ll need to supply one or more parameters to fetch:</p> <pre><code>{\n    \"numericParameters\": [{\n        \"parameterName\": \"Speed\"\n    }]\n}\n</code></pre> <p>In this example, we\u2019re requesting a single numeric parameter, <code>Speed</code>. Each array of parameters is indexed based on parameter type, which can be <code>numericParameters</code>, <code>stringParameters</code> or <code>binaryParameters</code>. Parameters are returned in a union, so if you request several, you\u2019ll get back all parameters that match.</p>","location":"apis/data-catalogue-api/raw-data/#request"},{"title":"Example","text":"<pre><code>curl \"https://${domain}.platform.quix.ai/parameters/data\" \\\n    -H \"accept: text/plain\" \\\n    -H \"Authorization: bearer &lt;token&gt;\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"numericParameters\":[{\"parameterName\":\"Speed\"}]}'\n</code></pre> <p>If you just had a single parameter value in the catalogue, the response from the above call might look something like this:</p> <pre><code>{\n    \"timestamps\": [\n        1612191100000000000\n    ],\n    \"numericValues\": {\n        \"Speed\": [\n            104.22222222222224\n        ]\n    },\n    \"stringValues\": {},\n    \"binaryValues\": {},\n    \"tagValues\": {},\n}\n</code></pre>","location":"apis/data-catalogue-api/raw-data/#example"},{"title":"Restricting by Stream or time","text":"<p>In reality, you\u2019ll have far more data in the catalogue, so you\u2019ll want to filter it. Three remaining properties of the request object allow you to do so:</p> <ul> <li> <p><code>streamIds</code></p> </li> <li> <p><code>from</code></p> </li> <li> <p><code>to</code></p> </li> </ul> <p>Each stream you create has a unique ID. You can view the ID of a persisted via the Data section of the Quix Portal. Supply a list of stream IDs to restrict fetched data to just those streams:</p> <pre><code>{\n    \"streamIds\": [\n        \"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\",\n        \"9feb07ac-b0b2-4591-bc7f-8f0c1295ed7c\"\n    ]\n}\n</code></pre> <p>You can also restrict data to a certain time span using the <code>from</code> and <code>to</code> properties. These each expect a timestamp in nanoseconds, for example:</p> <pre><code>{\n    \"from\": 1612191286000000000,\n    \"to\":   1612191386000000000\n}\n</code></pre> <p>These timestamps cover a range of 100 seconds.</p>","location":"apis/data-catalogue-api/raw-data/#restricting-by-stream-or-time"},{"title":"Forming a request","text":"<p>How you send requests to the Data Catalogue API will vary depending on the client or language you\u2019re using. But the API still has behaviour and expectations that is common across all clients.</p>  <p>Tip</p> <p>The examples in this section show how to use the popular <code>curl</code> command line tool.</p>","location":"apis/data-catalogue-api/request/"},{"title":"Before you begin","text":"<ul> <li> <p>Sign up on the Quix Portal</p> </li> <li> <p>Read about Authenticating with the Data Catalogue     API</p> </li> </ul>","location":"apis/data-catalogue-api/request/#before-you-begin"},{"title":"Endpoint URLs","text":"<p>The Data Catalogue API is available on a per-workspace basis, so the subdomain is based on a combination of your organisation and workspace names. See How to get a workspace ID to find out how to get the exact hostname required. It will be in this format:</p> <pre><code>https://telemetry-query-${organisation}-${workspace}.platform.quix.ai/\n</code></pre> <p>So your final endpoint URL will look something like:</p> <pre><code>https://telemetry-query-acme-weather.platform.quix.ai/\n</code></pre>","location":"apis/data-catalogue-api/request/#endpoint-urls"},{"title":"Method","text":"<p>Most endpoints use the <code>POST</code> method, even those that just fetch data. Ensure your HTTP client sends <code>POST</code> requests as appropriate.</p> <p>Using <code>curl</code>, the <code>-X POST</code> flag specifies a POST request. Note that this is optional if you\u2019re using the <code>-d</code> flag to send a payload (see below).</p> <pre><code>curl -X POST ...\n</code></pre>","location":"apis/data-catalogue-api/request/#method"},{"title":"Payload","text":"<p>For most methods, you\u2019ll need to send a JSON object containing supported parameters. You\u2019ll also need to set the appropriate content type for the payload you\u2019re sending:</p> <pre><code>curl -H \"Content-Type: application/json\" ...\n</code></pre>  <p>Warning</p> <p>You must specify the content type of your payload. Failing to include this header will result in a <code>415 UNSUPPORTED MEDIA TYPE</code> status code.</p>  <p>You can send data via a POST request using the <code>curl</code> flag <code>-d</code>. This should be followed by either a string of JSON data, or a string starting with the @ symbol, followed by a filename containing the JSON data.</p> <pre><code>curl -d '{\"key\": \"value\"}' ...\ncurl -d \"@data.json\" ...\n</code></pre>","location":"apis/data-catalogue-api/request/#payload"},{"title":"Complete curl example","text":"<p>You should structure most of your requests to the API around this pattern:</p> <pre><code>curl -H \"Authorization: ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d \"@data.json\" \\\n     https://${domain}.platform.quix.ai/${endpoint}\n</code></pre>","location":"apis/data-catalogue-api/request/#complete-curl-example"},{"title":"Filtered streams","text":"<p>To fetch specific streams, you can include various filters with your request to the <code>/streams</code> endpoint.</p>","location":"apis/data-catalogue-api/streams-filtered/"},{"title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can     use any Source of our Quix     Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>","location":"apis/data-catalogue-api/streams-filtered/#before-you-begin"},{"title":"Fetch a single stream via ID","text":"<p>The most basic filter matches against a stream\u2019s ID.</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"streamIds\": [\"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\"]}'\n</code></pre> <p>Note that you can supply multiple IDs in the <code>streamIds</code> array to match multiple streams.</p>","location":"apis/data-catalogue-api/streams-filtered/#fetch-a-single-stream-via-id"},{"title":"Filtering streams on basic properties","text":"<p>The location of a stream defines its position in a hierarchy. A stream location looks just like a filesystem path. You can filter streams based on the start of this path, so you can easily find streams contained within any point in the hierarchy. For example, this query will find streams with a location of <code>/one</code> but it will also find streams with a <code>/one/two</code> location:</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"location\": \"/one\"}'\n</code></pre>  <p>Warning</p> <p>Since this is just a basic prefix match, filtering on a location named <code>/one</code> will also bring back matches for the location <code>/one111</code> as well as the location <code>/one/111</code>. If you want to strictly filter on an exact directory (and below), make sure to  include a trailing slash, e.g. <code>/one/</code>.</p>   <p>Note</p> <p>Filtering on topic uses a case insensitive Equals match. Filtering on a topic named \"MyTopic\" will match \"mytopic\" but will not match \"MyTopic123\"</p>  <p>You can filter streams based on their use of a given parameter with the <code>parameterIds</code> property. For example, to find all streams that contain at least one single occurence of <code>Gear</code> data:</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"parameterIds\": [ \"Gear\"] }'\n</code></pre> <p>You can filter based on the presence or absence of a certain stream status, for example, if the stream is <code>Open</code> or was <code>Interrupted</code>. The <code>includeStatuses</code> and <code>excludeStatuses</code> properties each take an array of values to act on. So to get all streams that aren\u2019t Interrupted or Closed, use this query:</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"excludeStatuses\": [ \"Interrupted\", \"Closed\" ]}'\n</code></pre>","location":"apis/data-catalogue-api/streams-filtered/#filtering-streams-on-basic-properties"},{"title":"Filtering streams on metadata","text":"<p>You can associate metadata with your streams. This can be used, for example, to store the circuit a car has travelled around, or the player of a particular run of a game.</p> <p>To filter on metadata, include the <code>metadata</code> property in the JSON object in your request body. This property\u2019s value is an array of objects, each of which has two properties, <code>key</code> and <code>value</code>:</p> <ul> <li> <p><code>key</code>     The exact, case-sensitive key of the metadata you\u2019re interested in.</p> </li> <li> <p><code>value</code>     The exact, case-sensitive value of the metadata to match on</p> </li> </ul> <p>If you have a metadata entry keyed as \"circuit\", you can match against it for an example value with this payload:</p> <pre><code>\"metadata\": [{\n    \"key\": \"circuit\",\n    \"value\": \"Sakhir Short\"\n}]\n</code></pre> <p>As before, the response is an array of Stream objects:</p> <pre><code>[{\n    \"streamId\":\"e6545c18-d20d-47bd-8997-f3f825c1a45c\",\n    \"name\":\"cardata\",\n    \"topic\":\"cardata\",\n    \"createdAt\":\"2021-03-31T13:04:43.368Z\",\n    \"lastUpdate\":\"2021-03-31T13:04:44.53Z\",\n    \"dataStart\":1612191099000000000,\n    \"dataEnd\":1612191371000000000,\n    \"status\":\"Closed\",\n    \"metadata\":{\n        \"circuit\":\"Sakhir Short\",\n        \"player\":\"Swal\",\n        \"game\":\"Codemasters F1 2019\"\n    },\n    \"parents\":[],\n    \"location\":\"/static data/\"\n}]\n</code></pre>","location":"apis/data-catalogue-api/streams-filtered/#filtering-streams-on-metadata"},{"title":"Ordering results","text":"<p>Calls to the <code>/streams</code> endpoint can include an <code>ordering</code> property in the payload. This references an array of properties to sort on, each one an object with the following properties:</p> <ul> <li> <p>by     A string representing the property to order by.</p> </li> <li> <p>direction     A string, either \"Asc\" or \"Desc\", to define the sort direction.</p> </li> </ul> <p>For example, to sort all streams in ascending order by topic:</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"ordering\": [{ \"by\": \"topic\", \"direction\": \"asc\" }]}'\n</code></pre>","location":"apis/data-catalogue-api/streams-filtered/#ordering-results"},{"title":"Streams with models","text":"<p>One stream can derive from another, for example, acting as a model in a pipeline. This relationship can be inspected using the <code>/streams/models</code> endpoint.</p>","location":"apis/data-catalogue-api/streams-models/"},{"title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can     use any Source of our Quix     Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>","location":"apis/data-catalogue-api/streams-models/#before-you-begin"},{"title":"Fetching model data","text":"<p>The hierarchy is represented as a parent/child structure where a stream can have an optional parent and any number of children.</p> <p>The <code>/streams/models</code> endpoint will return data in the same structure as the <code>/streams</code> endpoint, with an additional property for each stream: <code>children</code>. This is an array of stream objects which may have their own children.</p> <p>The payload requirements are the same as those for <code>/streams</code>. You can fetch model information across all streams with an empty payload:</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams/models\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d \"{}\"\n</code></pre> <p>Here\u2019s an example result for a stream with two children:</p> <pre><code>[{\n    \"children\": [{\n        \"children\": [],\n        \"streamId\": \"79bbed17-5c71-4b0e-99f6-3596577b46d8\",\n        \"name\": \"new-child\",\n        \"topic\": \"cars\",\n        \"createdAt\": \"2021-04-08T15:27:09.19Z\",\n        \"lastUpdate\": \"2021-04-13T10:21:52.572Z\",\n        \"status\": \"Open\",\n        \"metadata\": {},\n        \"parents\": [\n            \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n        ],\n        \"location\": \"/\"\n    },{\n        \"children\": [],\n        \"streamId\": \"f003c1dd-9abe-49dd-afd2-f194d3d96035\",\n        \"name\": \"example1\",\n        \"topic\": \"cars\",\n        \"createdAt\": \"2021-04-12T11:50:38.504Z\",\n        \"lastUpdate\": \"2021-04-12T12:00:40.482Z\",\n        \"status\": \"Interrupted\",\n        \"metadata\": {\n            \"rain\": \"light\"\n        },\n        \"parents\": [\n            \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n        ],\n        \"location\": \"/examples/first/\"\n    }],\n    \"streamId\": \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\",\n    \"topic\": \"cars\",\n    \"createdAt\": \"2021-04-08T14:12:29.807Z\",\n    \"lastUpdate\": \"2021-04-12T13:45:08.377Z\",\n    \"timeOfRecording\": \"2021-04-12T00:00:00Z\",\n    \"dataStart\": 0,\n    \"dataEnd\": 1618233869000000000,\n    \"status\": \"Interrupted\",\n    \"metadata\": {},\n    \"parents\": [],\n    \"location\": \"/\"\n}]\n</code></pre> <p>And here\u2019s an example with a child and a grandchild:</p> <pre><code>[{\n    \"children\": [{\n        \"children\": [{\n            \"children\": [],\n            \"streamId\": \"79bbed17-5c71-4b0e-99f6-3596577b46d8\",\n            \"name\": \"new-child\",\n            \"topic\": \"cars\",\n            \"createdAt\": \"2021-04-08T15:27:09.19Z\",\n            \"lastUpdate\": \"2021-04-13T10:30:11.495Z\",\n            \"status\": \"Open\",\n            \"metadata\": {},\n            \"parents\": [\n                \"f003c1dd-9abe-49dd-afd2-f194d3d96035\"\n            ],\n            \"location\": \"/\"\n          }\n        ],\n        \"streamId\": \"f003c1dd-9abe-49dd-afd2-f194d3d96035\",\n        \"name\": \"example1\",\n        \"topic\": \"cars\",\n        \"createdAt\": \"2021-04-12T11:50:38.504Z\",\n        \"lastUpdate\": \"2021-04-12T12:00:40.482Z\",\n        \"status\": \"Interrupted\",\n        \"metadata\": {\n            \"rain\": \"light\"\n        },\n        \"parents\": [\n            \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n        ],\n        \"location\": \"/examples/first/\"\n      }\n    ],\n    \"streamId\": \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\",\n    \"topic\": \"cars\",\n    \"createdAt\": \"2021-04-08T14:12:29.807Z\",\n    \"lastUpdate\": \"2021-04-12T13:45:08.377Z\",\n    \"timeOfRecording\": \"2021-04-12T00:00:00Z\",\n    \"dataStart\": 0,\n    \"dataEnd\": 1618233869000000000,\n    \"status\": \"Interrupted\",\n    \"metadata\": {},\n    \"parents\": [],\n    \"location\": \"/\"\n}]\n</code></pre>","location":"apis/data-catalogue-api/streams-models/#fetching-model-data"},{"title":"Paged streams","text":"<p>You can fetch all streams within a workspace, across topics and locations, with a single call. If you\u2019re working with a large number of streams, you can use pagination parameters to group the results into smaller pages.</p>","location":"apis/data-catalogue-api/streams-paged/"},{"title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can     use any Source of our Quix     Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>","location":"apis/data-catalogue-api/streams-paged/#before-you-begin"},{"title":"Fetching all streams","text":"<p>The <code>/streams</code> endpoint provides read access to all streams within the workspace. Sending an empty JSON object in your request body will return all streams.</p>  <p>Warning</p> <p>Even if you\u2019re not supplying any parameters, you must still send a valid empty object as JSON data in the body of your request.</p>","location":"apis/data-catalogue-api/streams-paged/#fetching-all-streams"},{"title":"Example request","text":"<pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d \"{}\"\n</code></pre>","location":"apis/data-catalogue-api/streams-paged/#example-request"},{"title":"Example response","text":"<p>The JSON returned consists of an array of Stream objects:</p> <pre><code>[{\n    \"streamId\":\"e6545c18-d20d-47bd-8997-f3f825c1a45c\",\n    \"name\":\"cardata\",\n    \"topic\":\"cardata\",\n    \"createdAt\":\"2021-03-31T13:04:43.368Z\",\n    \"lastUpdate\":\"2021-03-31T13:04:44.53Z\",\n    \"dataStart\":1612191099000000000,\n    \"dataEnd\":1612191371000000000,\n    \"status\":\"Closed\",\n    \"metadata\":{},\n    \"parents\":[],\n    \"location\":\"/static data/\"\n}]\n</code></pre>","location":"apis/data-catalogue-api/streams-paged/#example-response"},{"title":"Fetching streams page by page","text":"<p>To reduce the size of the response, you should page these results with the <code>paging</code> property. Include this in the JSON object you send in the body of your request. The value of this property is an object with two members, <code>index</code> and <code>length</code>:</p> <ul> <li> <p><code>index</code>     The index of the page you want returned.</p> </li> <li> <p><code>length</code>     The number of items (i.e. streams) per page.</p> </li> </ul> <p>For example, to group all streams in pages of 10 and receive the 2nd page, use this value:</p> <pre><code>\"paging\": {\n    \"index\": 1,\n    \"length\": 10\n}\n</code></pre>","location":"apis/data-catalogue-api/streams-paged/#fetching-streams-page-by-page"},{"title":"Example request","text":"<pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"paging\":{\"index\": 1,\"length\": 10}}'\n</code></pre>","location":"apis/data-catalogue-api/streams-paged/#example-request_1"},{"title":"Before you begin","text":"<ul> <li>Sign up on the Quix Portal</li> </ul>","location":"apis/streaming-reader-api/authenticate/"},{"title":"Get a Personal Access Token","text":"<p>You should authenticate requests to the Streaming Reader API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary.</p> <p>Follow these steps to generate a PAT:</p> <ol> <li> <p>Click the user icon in the top-right of the Portal and select the     Tokens menu.</p> </li> <li> <p>Click GENERATE TOKEN.</p> </li> <li> <p>Choose a name to describe the token\u2019s purpose, and an expiration     date, then click CREATE.</p> </li> <li> <p>Copy the token and store it in a secure place.</p> </li> </ol>  <p>Warning</p> <p>You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy.</p>   <p>Warning</p> <p>Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs.</p>","location":"apis/streaming-reader-api/authenticate/#get-a-personal-access-token"},{"title":"Introduction","text":"<p>As an alternative to the SDK, the Quix platform supports real-time data streaming over WebSockets. Clients can receive updates on data and definitions for parameters and events, as they happen. The examples shown use the Microsoft SignalR JavaScript client library.</p>","location":"apis/streaming-reader-api/intro/"},{"title":"Topics","text":"<ul> <li> <p>Set up SignalR</p> </li> <li> <p>Authenticate</p> </li> <li> <p>Reading data</p> </li> <li> <p>Subscription &amp; Event     reference</p> </li> </ul>","location":"apis/streaming-reader-api/intro/#topics"},{"title":"Reading data","text":"<p>Before you can read data from a stream, you need to subscribe to an event of the Streaming Reader service like ParameterData or EventData.</p> <p>You can get a full list of Subscriptions and Events available in this of section.</p>","location":"apis/streaming-reader-api/reading-data/"},{"title":"Example","text":"<p>The following code sample shows how to use the SignalR client library to:</p> <ol> <li> <p>Establish a connection to Quix</p> </li> <li> <p>Subscribe to a parameter data stream</p> </li> <li> <p>Receive data from that stream</p> </li> <li> <p>Unsubscribe from the event</p> </li> </ol>  <pre><code>var signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () =&gt; 'YOUR_ACCESS_TOKEN'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://reader-YOUR_WORKSPACE_ID.platform.quix.ai/hub\", options)\n    .build();\n\n// Establish connection\nconnection.start().then(() =&gt; {\n    console.log(\"Connected to Quix.\");\n\n    // Subscribe to parameter data stream.\n    connection.invoke(\"SubscribeToParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n\n    // Read data from the stream.\n    connection.on(\"ParameterDataReceived\", data =&gt; {\n        let model = JSON.parse(data);\n        console.log(\"Received data from stream: \" + model.streamId);\n\n        // Unsubscribe from stream.\n        connection.invoke(\"UnsubscribeFromParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n    });\n});\n</code></pre>","location":"apis/streaming-reader-api/reading-data/#example"},{"title":"Before you begin","text":"<ul> <li> <p>Get a PAT for     Authentication</p> </li> <li> <p>Ensure you know your workspace ID</p> </li> </ul>","location":"apis/streaming-reader-api/signalr/"},{"title":"Installation","text":"<p>If you are using a package manager like npm, you can install SignalR using <code>npm install @microsoft/signalr</code>. For other installation options that don\u2019t depend on a platform like Node.js, such as consuming SignalR from a CDN, please refer to SignalR documentation.</p>","location":"apis/streaming-reader-api/signalr/#installation"},{"title":"Testing the connection","text":"<p>Once you\u2019ve installed the SignalR library, you can test it\u2019s set up correctly with the following code snippet. This opens a connection to the hub running on your custom subdomain, and checks authentication.</p> <p>You should replace the text <code>YOUR_ACCESS_TOKEN</code> with the PAT obtained from Authenticating with the Streaming Reader API.</p> <p>You should also replace <code>YOUR_WORKSPACE_ID</code> with the appropriate identifier, a combination of your organistation and workspace names. This can be located in one of the following ways:</p> <ul> <li> <p>Portal URL   Look in the browsers URL when you are logged into the Portal and   inside the Workspace you want to work with. The URL contains the   workspace id. e.g everything after \"workspace=\" till the next &amp;</p> </li> <li> <p>Topics Page   In the Portal, inside the Workspace you want to work with, click the   Topics menu    and then   click the expand icon    on any   topic. Here you will see a Username under the Broker Settings.     This Username is also the Workspace Id.</p> </li> </ul> <pre><code>var signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () =&gt; 'YOUR_ACCESS_TOKEN'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://reader-YOUR_WORKSPACE_ID.platform.quix.ai/hub\", options)\n    .build();\n\nconnection.start().then(() =&gt; console.log(\"SignalR connected.\"));\n</code></pre> <p>If the connection is successful, you should see the console log \u201cSignalR connected\u201d.</p>","location":"apis/streaming-reader-api/signalr/#testing-the-connection"},{"title":"Subscription & Event reference","text":"<p>The Quix SignalR hub provides the following subscriptions and events.</p>","location":"apis/streaming-reader-api/subscriptions/"},{"title":"Subscriptions","text":"<p>You can subscribe to the following hub methods via the <code>invoke</code> method of a <code>HubConnection</code>:</p> <ul> <li> <p><code>SubscribeToParameter(topicName, streamId, parameterId)</code>: Subscribe     to a parameter data stream.</p> </li> <li> <p><code>SubscribeToEvent(topicName, streamId, eventId)</code>: Subscribes to an     event data stream.</p> </li> <li> <p><code>IList&lt;ActiveStream&gt; SubscribeToActiveStreams(topicName)</code>: Subscribe     to Active Streams List changes. The subscription method returns an     initial list of the active streams existing in the topic.</p> </li> <li> <p><code>IList&lt;TopicMetrics&gt; SubscribeToTopicMetrics(topicName)</code>: Subscribe     to Topic metrics updates. The subscription method returns an initial     list of the last 5 minutes of topic metrics.</p> </li> <li> <p><code>SubscribeToPackages(string topicName)</code>: Subscribe to Topic     packages. A package is an abstraction for any message received in     the topic.</p> </li> </ul> <p>Each Subscribe method has its own Unsubscribe. Use them once you don\u2019t need the subscriptions anymore to avoid receiving data unnecessarily:</p> <ul> <li> <p><code>UnsubscribeFromParameter(topicname, streamId, parameterId)</code>:     Unsubscribe from a parameter data stream.</p> </li> <li> <p><code>UnsubscribeFromEvent(topicName, streamId, eventId)</code> Unsubscribe     from an event data stream.</p> </li> <li> <p><code>UnsubscribeFromActiveStreams(string topicName)</code>: Unsubscribe from     Streams List changes.</p> </li> <li> <p><code>UnsubscribeFromTopicMetrics(topicName)</code>: Unsubscribe from Topic     metrics updates.</p> </li> <li> <p><code>UnsubscribeFromPackages(string topicName)</code>: Unsubscribe from Topic     packages.</p> </li> <li> <p><code>UnsubscribeFromStream(topicName, streamId)</code>: Unsubscribes from all     subscriptions of the specified stream.</p> </li> </ul>  <p>Tip</p> <p>You should pass the method\u2019s name as the first argument to <code>invoke</code>, followed by the method-specific arguments. For example, to call:</p> <p><code>SubscribeToParameter(topicName, streamId, parameterId)</code></p> <p>Use the following:</p> <pre><code>connection.invoke(\"SubscribeToParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n</code></pre>","location":"apis/streaming-reader-api/subscriptions/#subscriptions"},{"title":"SignalR events","text":"<p>You can register a handler for SignalR events using the <code>on</code> method of a <code>HubConnection</code>. The following events are available:</p> <ul> <li> <p><code>ParameterDataReceived(parameterData)</code></p> </li> <li> <p><code>EventDataReceived(eventData)</code></p> </li> <li> <p><code>ActiveStreamsChanged(stream, action)</code></p> </li> <li> <p><code>TopicMetricsUpdated(metrics)</code></p> </li> <li> <p><code>PackageReceived(package)</code></p> </li> </ul>  <p>Tip</p> <p>You should pass the event\u2019s name as the first argument to <code>on</code>, followed by a function callback. For example, to react to the <code>ParameterDataReceived</code> event, use the following:</p> <pre><code>connection.on(\"ParameterDataReceived\", data =&gt; {\n    // process payload data\n});\n</code></pre>","location":"apis/streaming-reader-api/subscriptions/#signalr-events"},{"title":"ParameterDataReceived","text":"<p>Add a listener to <code>ParameterDataReceived</code> event to receive data from a <code>SubscribeToParameter</code> subscription.</p> <p>One event is generated each time a ParameterData package is received in the Topic and the data contains the Parameter the user has subscribed for.</p> <p>Example payload:</p> <pre><code>{\n    topicName: 'topic-1',\n    streamId: 'b45969d2-4624-4ab7-9779-c8f90ce79420',\n    timestamps: [ 1591733989000000000, 1591733990000000000, 1591733991000000000 ],\n    numericValues: { ParameterA: [ 1, 2, 3 ] },\n    stringValues: {},\n    tagValues: { ParameterA: [ null, null, 'tag-1' ] }\n}\n</code></pre>","location":"apis/streaming-reader-api/subscriptions/#parameterdatareceived"},{"title":"EventDataReceived","text":"<p>Add a listener to <code>EventDataReceived</code> event to receive data from a <code>SubscribeToEvent</code> subscription.</p> <p>One event is generated each time a EventData package is received in the Topic and the data contains the Event the user has subscribed for.</p> <p>Example payload:</p> <pre><code>{\n    topicName: 'topic-1',\n    streamId: 'b45969d2-4624-4ab7-9779-c8f90ce79420'\n    id: 'EventA',\n    timestamp: 1591733990000000000,\n    value: 'val-a',\n    tags: {\n        tag1: 'val1'\n    }\n}\n</code></pre>","location":"apis/streaming-reader-api/subscriptions/#eventdatareceived"},{"title":"ActiveStreamChanged","text":"<p>This event is generated each time a change has been produced in the list of Active streams of a Topic.</p> <p>Add a listener to <code>ActiveStreamChanged</code> event to receive data from a <code>SubscribeToActiveStreams</code> subscription. This SignalR event contains 2 arguments on it:</p> <ul> <li> <p><code>stream</code>: Payload of the stream that has been changed.</p> </li> <li> <p><code>action</code>: It describes the type of operation has been applied to the     list of active streams:</p> <ul> <li> <p><code>AddUpdate</code>: Stream added or updated</p> </li> <li> <p><code>Remove</code>: Stream removed</p> </li> </ul> </li> </ul> <p>Stream payload example:</p> <pre><code>{\n  \"streamId\": \"5ecfc7ce-906c-4d3a-811c-a85ea75a24b3\",\n  \"topicName\": \"f1-data\",\n  \"name\": \"F1 Game - Swal - Sakhir Short 2022-04-08-09:00:39\",\n  \"location\": \"/Game/Codemasters/F1-2019/Sakhir Short\",\n  \"metadata\": {\n    \"GameVersion\": \"1.22\",\n    \"PacketFormat\": \"2019\",\n    \"Track\": \"Sakhir Short\",\n    \"SessionId\": \"237322236454500810\",\n    \"Player0_Name\": \"Swal\",\n    \"Player0_Id\": \"100\"\n  },\n  \"parents\": [],\n  \"timeOfRecording\": \"2022-04-08T09:00:39.3971666Z\",\n  \"parameters\": {\n    \"EngineRPM\": {\n      \"dataType\": \"Numeric\",\n      \"minimumValue\": 0,\n      \"maximumValue\": 20000,\n      \"location\": \"/Player/Telemetry/Engine\"\n    },\n    \"LapDistance\": {\n      \"dataType\": \"Numeric\",\n      \"unit\": \"m\",\n      \"location\": \"/Player/Telemetry/Misc\"\n    },\n    \"Brake\": {\n      \"description\": \"Amount of brake applied\",\n      \"dataType\": \"Numeric\",\n      \"minimumValue\": 0,\n      \"maximumValue\": 1,\n      \"location\": \"/Player/Telemetry/Input\"\n    },\n    \"Throttle\": {\n      \"dataType\": \"Unknown\",\n      \"minimumValue\": 0,\n      \"maximumValue\": 1,\n      \"location\": \"/Player/Telemetry/Input\"\n    },\n    \"Gear\": {\n      \"dataType\": \"Numeric\",\n      \"minimumValue\": -1,\n      \"maximumValue\": 8,\n      \"location\": \"/Player/Telemetry/Engine\"\n    },\n    \"Speed\": {\n      \"dataType\": \"Numeric\",\n      \"minimumValue\": 0,\n      \"maximumValue\": 400,\n      \"location\": \"/Player/Telemetry/Engine\"\n    },\n    \"Steer\": {\n      \"dataType\": \"Numeric\",\n      \"minimumValue\": -1,\n      \"maximumValue\": 1,\n      \"location\": \"/Player/Telemetry/Input\"\n    },\n  },\n  \"events\": {\n    \"Player_NewLap\": {\n      \"name\": \"Player NewLap\",\n      \"level\": \"Information\",\n      \"location\": \"\"\n    },\n    \"Player_Position_Changed\": {\n      \"name\": \"Player Position Changed\",\n      \"level\": \"Critical\",\n      \"location\": \"\"\n    },\n    \"RaceWinner\": {\n      \"name\": \"Race Winner\",\n      \"level\": \"Critical\",\n      \"location\": \"\"\n    },\n  },\n  \"firstSeen\": \"2022-04-08T08:57:40.3406586Z\",\n  \"lastSeen\": \"2022-04-08T09:00:39.6308255Z\",\n  \"status\": \"Receiving\",\n  \"lastData\": \"2022-04-08T09:00:39.6237312Z\"\n}\n</code></pre>","location":"apis/streaming-reader-api/subscriptions/#activestreamchanged"},{"title":"TopicMetricsUpdated","text":"<p>This event is generated periodically by the service to provide basic metrics about a Topic, like \"Bytes per Second\" or \"Number of Active Streams\".</p> <p>Add a listener to <code>TopicMetricsUpdated</code> event to receive data from a <code>SubscribeToTopicMetrics</code> subscription.</p> <p>Topic Metrics payload example:</p> <pre><code>{\n    \"timestamp\": \"2022-04-10T19:26:49.1417825Z\",\n    \"topicName\": \"f1-data\",\n    \"bytesPerSecond\": 14877,\n    \"activeStreams\": 1\n}\n</code></pre>","location":"apis/streaming-reader-api/subscriptions/#topicmetricsupdated"},{"title":"PackageReceived","text":"<p>Add a listener to <code>PackageReceived</code> event to receive data from a <code>SubscribeToPackages</code> subscription.</p> <p>One event is generated each time a package is received in the topic.</p> <ul> <li> <p>Type: Indicates the Quix Sdk model used to deserialize the package.</p> </li> <li> <p>Value: Deserialized package object represented as a Json string     format.</p> </li> </ul> <p>Package payload example:</p> <pre><code>{\n    \"topicName\": \"f1-data\",\n    \"streamId\": \"dec481d7-7ae4-403a-9d20-a1cabdcd3275\",\n    \"type\": \"Quix.Sdk.Process.Models.ParameterDataRaw\",\n    \"value\": \"{\\\"Epoch\\\":0,\\\"Timestamps\\\":[1649623155716050700],\\\"NumericValues\\\":{\\\"LapDistance\\\":[542.504638671875],\\\"TotalLapDistance\\\":[4368.53271484375]},\\\"StringValues\\\":{},\\\"BinaryValues\\\":{},\\\"TagValues\\\":{\\\"LapValidity\\\":[\\\"Valid\\\"],\\\"LapNumber\\\":[\\\"2\\\"],\\\"PitStatus\\\":[\\\"None\\\"],\\\"Sector\\\":[\\\"0\\\"],\\\"DriverStatus\\\":[\\\"Flying_lap\\\"]}}\",\n    \"dateTime\": \"2022-04-10T20:39:16.63Z\"\n}\n</code></pre>","location":"apis/streaming-reader-api/subscriptions/#packagereceived"},{"title":"Before you begin","text":"<ul> <li>Sign up on the Quix Portal</li> </ul>","location":"apis/streaming-writer-api/authenticate/"},{"title":"Get a Personal Access Token","text":"<p>You should authenticate requests to the Streaming Writer API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary.</p> <p>Follow these steps to generate a PAT:</p> <ol> <li> <p>Click the user icon in the top-right of the Portal and select the     Tokens menu.</p> </li> <li> <p>Click GENERATE TOKEN.</p> </li> <li> <p>Choose a name to describe the token\u2019s purpose, and an expiration     date, then click CREATE.</p> </li> <li> <p>Copy the token and store it in a secure place.</p> </li> </ol>  <p>Warning</p> <p>You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy.</p>   <p>Warning</p> <p>Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs.</p>","location":"apis/streaming-writer-api/authenticate/#get-a-personal-access-token"},{"title":"Sign all your requests using this token","text":"<p>Make sure you accompany each request to the API with an <code>Authorization</code> header using your PAT as a bearer token, as follows:</p> <pre><code>Authorization: bearer &lt;token&gt;\n</code></pre> <p>Replace <code>&lt;token&gt;</code> with your Personal Access Token. For example, if you\u2019re using curl on the command line, you can set the header using the <code>-H</code> flag:</p> <pre><code>curl -H \"Authorization: bearer &lt;token&gt;\" ...\n</code></pre>  <p>Warning</p> <p>If you fail to send a valid Authorization header, the API will respond with a <code>401 UNAUTHORIZED</code> status code.</p>","location":"apis/streaming-writer-api/authenticate/#sign-all-your-requests-using-this-token"},{"title":"Create a new Stream","text":"<p>You can create a new stream by specifying a topic to create it in, and supplying any other additional properties required.</p>  <p>Tip</p> <p>This method is optional. You can also create a stream implicitly by sending data to a stream that doesn\u2019t already exist. But creating a stream using the method on this page avoids having to determine a unique stream id yourself.</p>","location":"apis/streaming-writer-api/create-stream/"},{"title":"Before you begin","text":"<ul> <li> <p>You should have a Workspace set     up with at least one     Topic.</p> </li> <li> <p>Get a Personal Access     Token to authenticate each     request.</p> </li> </ul>","location":"apis/streaming-writer-api/create-stream/#before-you-begin"},{"title":"Using the /streams endpoint","text":"<p>To create a new stream, send a <code>POST</code> request to:</p> <pre><code>/topics/${topicName}/streams\n</code></pre> <p>You should replace <code>$\\{topicName}</code> in the endpoint URL with the name of the Topic you wish to create the stream in. For example, if your topic is named \u201ccars\u201d, your endpoint url will be <code>/topics/cars/streams</code>.</p>","location":"apis/streaming-writer-api/create-stream/#using-the-streams-endpoint"},{"title":"Example request","text":"<p>You can create a new Stream with an absolute minimum of effort by passing an empty JSON object in the payload:</p> <ul> <li> <p>curl</p> <pre><code>curl \"https://${domain}.platform.quix.ai/topics/${topicName}/streams\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{}'\n</code></pre> </li> <li> <p>Node.js</p> <pre><code>const https = require('https');\n\nconst data = \"{}\";\n\nconst options = {\n    hostname: domain + '.platform.quix.ai',\n    path: '/topics/' + topicName + '/streams',\n    method: 'POST',\n    headers: {\n        'Authorization': 'Bearer ' + token,\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options, res =&gt; {\n    res.on('data', d =&gt; {\n        let streamId = JSON.parse(d).streamId;\n        console.log(streamId);\n    });\n});\n\nreq.write(data);\nreq.end();\n</code></pre> </li> </ul> <p>For most real-world cases, you\u2019ll also want to provide some or all of the following:</p> <ul> <li> <p><code>name</code></p> </li> <li> <p><code>location</code></p> </li> <li> <p><code>metadata</code></p> </li> <li> <p><code>parents</code></p> </li> <li> <p><code>timeOfRecording</code></p> </li> </ul> <p>For example, here\u2019s a more useful payload:</p> <pre><code>{\n    \"name\": \"cardata\",\n    \"location\": \"simulations/trials\",\n    \"metadata\": {\n        \"rain\": \"light\"\n    }\n}\n</code></pre>","location":"apis/streaming-writer-api/create-stream/#example-request"},{"title":"Example response","text":"<p>The JSON returned is an object with a single property, <code>streamId</code>. This contains the unique identifier of your newly created stream, and will look something like this:</p> <pre><code>{\n    \"streamId\": \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n}\n</code></pre>  <p>Tip</p> <p>If you\u2019re following these guides in order, you\u2019ll want to take note of that stream id. For curl examples, it\u2019s convenient to keep it in an environment variable, e.g.</p> <pre><code>$ streamId=66fb0a2f-eb70-494e-9df7-c06d275aeb7c\n</code></pre>","location":"apis/streaming-writer-api/create-stream/#example-response"},{"title":"Using SignalR","text":"<pre><code>// Also available as JsFiddle at https://jsfiddle.net/QuixAI/cLno68fs/\nvar signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst workspaceId = \"YOUR_WORKSPACE_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\n\nconst options = {\n    accessTokenFactory: () =&gt; token\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://writer-\" + workspaceId + \".platform.quix.ai/hub\", options)\n    .build();\n\n// Establish connection\nconnection.start().then(async () =&gt; {\n    console.log(\"Connected to Quix.\");\n\n    // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\n    let streamDetails = {\n        \"name\": \"cardata\",\n        \"location\": \"simulations/trials\",\n        \"metadata\": {\n            \"rain\": \"light\"\n        }\n    }\n\n    // Send create details\n    console.log(\"Creating stream\");\n    let createdDetails = await connection.invoke(\"CreateStream\", topic, streamDetails);\n    let streamId = createdDetails.streamId\n    console.log(\"Created stream \" + streamId);\n});\n</code></pre>","location":"apis/streaming-writer-api/create-stream/#using-signalr"},{"title":"Getting Swagger url","text":"<p>You can access documentation and a basic playground for the Streaming Writer API via Swagger. The exact URL is workspace-specific, and follows this pattern:</p> <pre><code>https://writer-${organisation}-${workspace}.platform.quix.ai/\n</code></pre> <p>The workspace ID is a combination based on your organisation and workspace names. For example, for an \"acme\" organisation with a \"weather\" workspace, the final URL might look something like:</p> <pre><code>https://writer-acme-weather.platform.quix.ai/\n</code></pre> <p>To determine the final URL, you can find out how to get a workspace id, or follow these instructions:</p> <ol> <li> <p>Click the Library icon Library in the main menu.</p> </li> <li> <p>Search for \"HTTP API - Shell Script\" item in the search bar and     select the item.</p> </li> <li> <p>Find in the Code Preview the Url on any of the POST methods of the     sample.</p> </li> </ol>","location":"apis/streaming-writer-api/get-swagger/"},{"title":"Introduction","text":"<p>The Streaming Writer API allows you to stream data into the Quix platform via HTTP endpoints or SignalR. It\u2019s an alternative to using our C# and Python SDKs. You can use the Streaming Writer API from any HTTP-capable language.</p> <p>The API is fully documented in our Swagger documentation. Read on for a guide to using the API, including real-world examples you can execute from your language of choice, or via the command line using curl.</p>","location":"apis/streaming-writer-api/intro/"},{"title":"Preparation","text":"<p>If you plan on using the HTTP endpoins, then you\u2019ll need to know how to authenticate your requests and how to form a typical request to the API.</p> <p>If you would rather use the SignalR api, which is suggested for high frequency data streaming, then see SignalR setup.</p>","location":"apis/streaming-writer-api/intro/#preparation"},{"title":"Topics covered","text":"<ul> <li> <p>Stream</p> <ul> <li> <p>Create a new Stream</p> </li> <li> <p>Add Stream     metadata</p> </li> </ul> </li> <li> <p>Parameters</p> <ul> <li>Send Parameter data</li> </ul> </li> <li> <p>Events</p> <ul> <li>Send an Event</li> </ul> </li> </ul>","location":"apis/streaming-writer-api/intro/#topics-covered"},{"title":"Forming a request","text":"<p>How you send requests to the Streaming Writer API will vary depending on the client or language you\u2019re using. But the API still has behaviour and expectations that is common across all clients.</p>  <p>Tip</p> <p>The examples in this section show how to use the popular <code>curl</code> command line tool.</p>","location":"apis/streaming-writer-api/request/"},{"title":"Before you begin","text":"<ul> <li> <p>Sign up on the Quix Portal</p> </li> <li> <p>Read about Authenticating with the Streaming Writer     API</p> </li> </ul>","location":"apis/streaming-writer-api/request/#before-you-begin"},{"title":"Endpoint URLs","text":"<p>The Streaming Writer API is available on a per-workspace basis, so the subdomain is based on a combination of your organisation and workspace names. See the Swagger documentation to find out how to get the exact hostname required. It will be in this format:</p> <p>https://writer-${organisation}-${workspace}.platform.quix.ai/\\&lt;/programlisting&gt;</p> <p>So your final endpoint URL will look something like:</p> <p>https://writer-acme-weather.platform.quix.ai/\\&lt;/programlisting&gt;</p>","location":"apis/streaming-writer-api/request/#endpoint-urls"},{"title":"Method","text":"<p>Endpoints in this API use the <code>POST</code> and <code>PUT</code> methods. Ensure your HTTP client sends the correct request method.</p> <p>Using <code>curl</code>, you can specify the request method with the <code>-X &lt;POST|PUT&gt;</code> flag, for example:</p> <pre><code>curl -X PUT ...\n</code></pre>","location":"apis/streaming-writer-api/request/#method"},{"title":"Payload","text":"<p>For most methods, you\u2019ll need to send a JSON object containing supported parameters. You\u2019ll also need to set the appropriate content type for the payload you\u2019re sending:</p> <pre><code>curl -H \"Content-Type: application/json\" ...\n</code></pre>  <p>Warning</p> <p>You must specify the content type of your payload. Failing to include this header will result in a <code>415 UNSUPPORTED MEDIA TYPE</code> status code.</p>  <p>You can send data using the <code>curl</code> flag <code>-d</code>. This should be followed by either a string of JSON data, or a string starting with the @ symbol, followed by a filename containing the JSON data.</p> <pre><code>curl -d '{\"key\": \"value\"}' ...\ncurl -d \"@data.json\" ...\n</code></pre>  <p>Tip</p> <p>By default, <code>-d</code> will send a <code>POST</code> request, so <code>-X POST</code> becomes unnecessary.</p>","location":"apis/streaming-writer-api/request/#payload"},{"title":"Complete curl example","text":"<p>You should structure most of your requests to the API around this pattern:</p> <pre><code>curl -H \"Authorization: ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d \"@data.json\" \\\n     https://${domain}.platform.quix.ai/${endpoint}\n</code></pre>","location":"apis/streaming-writer-api/request/#complete-curl-example"},{"title":"Send Parameter data","text":"<p>You can send telemetry data using the Streaming Writer API. Select a topic and a stream to send the data to. In your payload, you can include numeric, string, or binary parameter data, with nanosecond-level timestamps.</p>","location":"apis/streaming-writer-api/send-data/"},{"title":"Before you begin","text":"<ul> <li> <p>You should have a Workspace set     up with at least one     Topic.</p> </li> <li> <p>Get a Personal Access     Token to authenticate each     request.</p> </li> </ul>","location":"apis/streaming-writer-api/send-data/#before-you-begin"},{"title":"Sending structured data to the endpoint","text":"<p>Send a POST request together with a JSON payload representing the data you\u2019re sending to:</p> <p>/topics/${topicName}/streams/${streamId}/parameters/data\\&lt;/programlisting&gt;</p> <p>You should replace <code>$\\{topicName}</code> with the name of the topic your stream belongs to, and <code>$\\{streamId}</code> with the id of the stream you wish to send data to. For example:</p> <p>/topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c/parameters/data\\&lt;/programlisting&gt;</p>  <p>Tip</p> <p>You can create a new stream by supplying a <code>$\\{streamId}</code> that doesn\u2019t already exist. This avoids the need to call the create stream endpoint separately.</p>","location":"apis/streaming-writer-api/send-data/#sending-structured-data-to-the-endpoint"},{"title":"Example request","text":"<p>Your payload should include an array of <code>timestamps</code> with one timestamp for each item of data you\u2019re sending. Actual data values should be keyed on their name, in the object that corresponds to their type, one of <code>numericValues</code>, <code>stringValues</code>, or <code>binaryValues</code>. The payload is in this structure:</p> <pre><code>{\n    \"timestamps\": [...],\n    \"numericValues\": {...},\n    \"stringValues\": {...},\n    \"binaryValues\": {...},\n    \"tagValues\": {...}\n}\n</code></pre> <p>Any data types that are unused can be omitted. So a final request using curl might look something like this:</p> <ul> <li> <p>curl</p> <pre><code>curl -X POST \"https://${domain}.platform.quix.ai/topics/${topicName}/streams/${streamId}/parameters/data\" \\\n     -H \"Authorization: Bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n             \"timestamps\": [1591733989000000000, 1591733990000000000, 1591733991000000000],\n             \"numericValues\": {\n                 \"SomeParameter1\": [10.01, 202.02, 303.03],\n                 \"SomeParameter2\": [400.04, 50.05, 60.06]\n             }\n        }'\n</code></pre> </li> <li> <p>Node.js</p> <pre><code>const https = require('https');\n\nconst data = JSON.stringify({\n    \"timestamps\": [1591733989000000000, 1591733990000000000, 1591733991000000000],\n    \"numericValues\": {\n        \"SomeParameter1\": [10.01, 202.02, 303.03],\n        \"SomeParameter2\": [400.04, 50.05, 60.06]\n    }\n});\n\nconst options = {\n    hostname: domain + '.platform.quix.ai',\n    path: '/topics/' + topicName + '/streams/' + streamId + '/parameters/data',\n    method: 'POST',\n    headers: {\n        'Authorization': 'Bearer ' + token,\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options);\n\nreq.write(data);\nreq.end();\n</code></pre> </li> </ul>","location":"apis/streaming-writer-api/send-data/#example-request"},{"title":"Response","text":"<p>No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong.</p>","location":"apis/streaming-writer-api/send-data/#response"},{"title":"Using SignalR","text":"<pre><code>// Also available as JsFiddle at https://jsfiddle.net/QuixAI/a41b8x0t/\nvar signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst workspaceId = \"YOUR_WORKSPACE_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\nconst streamId = \"ID_OF_STREAM_TO_WRITE_TO\"\n\nconst options = {\n    accessTokenFactory: () =&gt; token\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://writer-\" + workspaceId + \".platform.quix.ai/hub\", options)\n    .build();\n\n// Establish connection\nconnection.start().then(async () =&gt; {\n    console.log(\"Connected to Quix.\");\n\n    // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\n    let parameterData = {\n      \"epoch\": Date.now() * 1000000, // set now as time starting point, in nanoseconds\n      \"timestamps\": [\n        0,\n        5000000000, // 5 seconds from now (see epoch)\n        8000000000\n      ],\n      \"numericValues\": {\n        \"NumericParameter1\": [\n          13.37,\n          42,\n          24.72\n        ]\n      },\n      \"stringValues\": {\n        \"StringParameter1\": [\n          \"Hello\",\n          \"World\",\n          \"!\"\n        ]\n      },\n      \"binaryValues\": {\n        \"BinaryParameter1\": [\n          btoa(\"Hello\"), // send binary array as base64\n          btoa(\"World\"),\n          btoa(\"!\")\n        ]\n      },\n      \"tagValues\": {\n        \"Tag1\": [\n          \"A\",\n          \"B\",\n          null\n        ]\n      }\n    }\n\n    // Send stream update details\n    console.log(\"Sending parameter data\");\n    await connection.invoke(\"SendParameterData\", topic, streamId, parameterData);\n    console.log(\"Sent parameter data\");\n});\n</code></pre>","location":"apis/streaming-writer-api/send-data/#using-signalr"},{"title":"Send an Event","text":"<p>You can add Events to your stream data to record discrete actions for future reference.</p>","location":"apis/streaming-writer-api/send-event/"},{"title":"Before you begin","text":"<ul> <li> <p>Get a Personal Access     Token to authenticate each     request.</p> </li> <li> <p>If you don\u2019t already have a Stream in your workspace, add one using     the API.</p> </li> </ul>","location":"apis/streaming-writer-api/send-event/#before-you-begin"},{"title":"Sending event data","text":"<p>To send event data to a stream, use the <code>POST</code> method with this endpoint:</p> <pre><code>/topics/${topicName}/streams/${streamId}/events/data\n</code></pre> <p>You should replace <code>$\\{topicName}</code> with the name of the topic your stream belongs to, and <code>$\\{streamId}</code> with the id of the stream you wish to send data to. For example:</p> <p>/topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c/events/data\\&lt;/programlisting&gt;</p>  <p>Tip</p> <p>You can create a new stream by supplying a <code>$\\{streamId}</code> that doesn\u2019t already exist. This avoids the need to call the create stream endpoint separately.</p>  <p>Your payload should be an array of events. Each event is an object containing the following properties:</p> <ul> <li> <p>id     a unique identifier for the event</p> </li> <li> <p>timestamp     the nanosecond-precise timestamp at which the event occurred</p> </li> <li> <p>tags     a object containing key-value string pairs representing tag values</p> </li> <li> <p>value     a string value associated with the event</p> </li> </ul>","location":"apis/streaming-writer-api/send-event/#sending-event-data"},{"title":"Example request","text":"<p>This example call adds a single event to a stream. The event has an example value and demonstrates use of a tag to include additional information.</p> <ul> <li> <p>curl</p> <pre><code>curl -i \"https://${domain}.platform.quix.ai/topics/${topicName}/streams/${streamId}/events/data\" \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '[{\n             \"id\": \"Alert\",\n             \"timestamp\": 1618133869000000000,\n             \"tags\": {\n                 \"capacity\": \"over\"\n             },\n             \"value\": \"Help\"\n     }]'\n</code></pre> </li> <li> <p>Node.js</p> <pre><code>const https = require('https');\n\nconst data = JSON.stringify({\n    \"id\": \"Alert\",\n    \"timestamp\": 1618133869000000000,\n    \"tags\": {\n        \"capacity\": \"over\"\n    },\n    \"value\": \"Help\"\n});\n\nconst options = {\n    hostname: domain + '.platform.quix.ai',\n    path: '/topics/' + topicName + '/streams/' + streamId + '/events/data',\n    method: 'POST',\n    headers: {\n        'Authorization': 'Bearer ' + token,\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options);\n\nreq.write(data);\nreq.end();\n</code></pre> </li> </ul>","location":"apis/streaming-writer-api/send-event/#example-request"},{"title":"Response","text":"<p>No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong.</p>","location":"apis/streaming-writer-api/send-event/#response"},{"title":"Using SignalR","text":"<pre><code>// Also available as JsFiddle at https://jsfiddle.net/QuixAI/h4fztrns/\nvar signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst workspaceId = \"YOUR_WORKSPACE_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\nconst streamId = \"ID_OF_STREAM_TO_WRITE_TO\"\n\nconst options = {\n    accessTokenFactory: () =&gt; token\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://writer-\" + workspaceId + \".platform.quix.ai/hub\", options)\n    .build();\n\n// Establish connection\nconnection.start().then(async () =&gt; {\n    console.log(\"Connected to Quix.\");\n\n    // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\n    let eventData = [\n        {\n            \"timestamp\": Date.now() * 1000000, // set now in nanoseconds,\n            \"tags\": {\n                \"capacity\": \"over\"\n            },\n            \"id\": \"Alert\",\n            \"value\": \"Successful sample run\"\n        }\n    ]\n\n    // Send stream update details\n    console.log(\"Sending event data\");\n    await connection.invoke(\"SendEventData\", topic, streamId, eventData);\n    console.log(\"Sent event data\");\n});\n</code></pre>","location":"apis/streaming-writer-api/send-event/#using-signalr"},{"title":"Before you begin","text":"<ul> <li> <p>Get a PAT for     Authentication</p> </li> <li> <p>Ensure you know your workspace ID</p> </li> </ul>","location":"apis/streaming-writer-api/signalr/"},{"title":"Installation","text":"<p>If you are using a package manager like npm, you can install SignalR using <code>npm install @microsoft/signalr</code>. For other installation options that don\u2019t depend on a platform like Node.js, such as consuming SignalR from a CDN, please refer to SignalR documentation.</p>","location":"apis/streaming-writer-api/signalr/#installation"},{"title":"Testing the connection","text":"<p>Once you\u2019ve installed the SignalR library, you can test it\u2019s set up correctly with the following code snippet. This opens a connection to the hub running on your custom subdomain, and checks authentication.</p> <p>You should replace the text <code>YOUR_ACCESS_TOKEN</code> with the PAT obtained from Authenticating with the Streaming Writer API.</p> <p>You should also replace <code>YOUR_WORKSPACE_ID</code> with the appropriate identifier, a combination of your organistation and workspace names. This can be located in one of the following ways:</p> <ul> <li> <p>Portal URL     Look in the browsers URL when you are logged into the Portal and     inside the Workspace you want to work with. The URL contains the     workspace id. e.g everything after \"workspace=\" till the next &amp;</p> </li> <li> <p>Topics Page     In the Portal, inside the Workspace you want to work with, click the     Topics menu      and then     click the expand icon      on any     topic. Here you will see a Username under the Broker Settings.     This Username is also the Workspace Id.</p> </li> </ul> <pre><code>// Also available as JsFiddle at https://jsfiddle.net/QuixAI/L9ha4p5j/\nvar signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst workspaceId = \"YOUR_WORKSPACE_ID\"\n\nconst options = {\n    accessTokenFactory: () =&gt; token\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://writer-\" + workspaceId + \".platform.quix.ai/hub\", options)\n    .build();\n\nconnection.start().then(() =&gt; console.log(\"SignalR connected.\"));\n</code></pre> <p>If the connection is successful, you should see the console log \u201cSignalR connected\u201d.</p>","location":"apis/streaming-writer-api/signalr/#testing-the-connection"},{"title":"Add Stream metadata","text":"<p>You can add arbitrary string metadata to any stream. You can also create a new stream by sending metadata using a stream id that does not already exist.</p>","location":"apis/streaming-writer-api/stream-metadata/"},{"title":"Before you begin","text":"<ul> <li> <p>You should have a Workspace set     up with at least one     Topic.</p> </li> <li> <p>Get a Personal Access     Token to authenticate each     request.</p> </li> </ul>","location":"apis/streaming-writer-api/stream-metadata/#before-you-begin"},{"title":"How to add metadata to a stream","text":"<p>Send a <code>PUT</code> request to the following endpoint to update a stream with the given properties:</p> <pre><code>/topics/${topicName}/streams/${streamId}\n</code></pre> <p>You should replace <code>$\\{topicName}</code> with the name of the topic your stream belongs to, and <code>$\\{streamId}</code> with the id of the stream you wish to update. For example:</p> <pre><code>/topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c\n</code></pre>  <p>Tip</p> <p>You can create a new stream by supplying a <code>$\\{streamId}</code> that doesn\u2019t already exist. It will be initialised with the data you provide in the payload, and the id you use in the endpoint. This avoids the need to call the create stream endpoint separately.</p>  <p>Your request should contain a payload consisting of JSON data containing the desired metadata.</p>","location":"apis/streaming-writer-api/stream-metadata/#how-to-add-metadata-to-a-stream"},{"title":"Example request","text":"<p>Below is an example payload demonstrating how to set a single item of metadata. Note that the <code>metadata</code> property references an object which contains key/value string-based metadata.</p> <ul> <li> <p>curl</p> <pre><code>curl \"https://${domain}.platform.quix.ai/topics/${topicName}/streams/${streamId}\" \\\n     -X PUT \\\n     -H \"Authorization: bearer ${token}\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"metadata\":{\"fruit\":\"apple\"}}'\n</code></pre> </li> <li> <p>Node.js</p> <pre><code>const https = require('https');\n\nconst data = JSON.stringify({ metadata: { fruit: \"apple\" }});\n\nconst options = {\n    hostname: domain + '.platform.quix.ai',\n    path: '/topics/' + topicName + '/streams/' + streamId,\n    method: 'PUT',\n    headers: {\n        'Authorization': 'Bearer ' + token,\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options);\n\nreq.write(data);\nreq.end();\n</code></pre> </li> </ul> <p>Since this is a PUT request, it will replace all the stream data with the payload contents. To maintain existing data, you should include it in the payload alongside your metadata, e.g.</p> <pre><code>{\n    \"name\": \"Example stream\",\n    \"location\": \"/sub/dir\",\n    \"metadata\": {\n        \"fruit\": \"apple\"\n    }\n}\n</code></pre>","location":"apis/streaming-writer-api/stream-metadata/#example-request"},{"title":"Response","text":"<p>No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong. For example, you\u2019ll see a 405 code if you forget to specify the correct <code>PUT</code> method.</p>","location":"apis/streaming-writer-api/stream-metadata/#response"},{"title":"Using SignalR","text":"<pre><code>// Also available as JsFiddle at https://jsfiddle.net/QuixAI/ruywnz28/\nvar signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst workspaceId = \"YOUR_WORKSPACE_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\nconst streamId = \"ID_OF_STREAM_TO_UPDATE\"\n\nconst options = {\n    accessTokenFactory: () =&gt; token\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://writer-\" + workspaceId + \".platform.quix.ai/hub\", options)\n    .build();\n\n// Establish connection\nconnection.start().then(async () =&gt; {\n    console.log(\"Connected to Quix.\");\n\n    // Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\n    let streamDetails = {\n        \"name\": \"Example stream\",\n        \"location\": \"/sub/dir\",\n        \"metadata\": {\n            \"fruit\": \"apple\"\n        }\n    }\n\n    // Send stream update details\n    console.log(\"Updating stream\");\n    await connection.invoke(\"UpdateStream\", topic, streamDetails);\n    console.log(\"Updated stream\");\n});\n</code></pre>","location":"apis/streaming-writer-api/stream-metadata/#using-signalr"},{"title":"MLOps","text":"<p>There are a number of barriers that prevent companies from successfully implementing data and ML projects. It\u2019s generally considered to be significantly harder than implementing software projects due to the cross functional complexity of data and ML pipelines. As a result, even if you hire the best data scientists, their work often fails and many companies give-up on their projects before they\u2019ve begun to see the value of the technologies.</p> <p>Solving these challenges is a new field of expertise called MLOps^</p> <p>We are working to incorporate MLOps into Quix so that your data team has a seamless journey from concept to production. The key steps are:</p>","location":"platform/MLOps/"},{"title":"Discover and access data","text":"<p>Any member of any team can quickly access data in the Catalogue without support from software or regulatory teams.</p>","location":"platform/MLOps/#discover-and-access-data"},{"title":"Develop features in historic data","text":"<p>Use Visualise to discover, segment, label and store significant features in the catalogue.</p>","location":"platform/MLOps/#develop-features-in-historic-data"},{"title":"Build &amp; train models on historic data","text":"<p>Use Develop and Deploy to:</p> <ul> <li> <p>Write model code in Python using their favourite IDE.</p> </li> <li> <p>Train models on historic data.</p> </li> <li> <p>Evaluate results against raw data and results from other models.</p> </li> <li> <p>Rapidly iterate models with GIT version control.</p> </li> </ul>","location":"platform/MLOps/#build-train-models-on-historic-data"},{"title":"Test models on live data","text":"<p>Connect models to live input topics to test them against live data sources. Review the results in Visualise.</p>","location":"platform/MLOps/#test-models-on-live-data"},{"title":"Build a production pipeline","text":"<p>Use Develop and Deploy to:</p> <ul> <li> <p>Connect validated models to live output topics.</p> </li> <li> <p>Daisy-chain models using input and output topics.</p> </li> <li> <p>Work seamlessly with engineers to hook-up software services.</p> </li> </ul>","location":"platform/MLOps/#build-a-production-pipeline"},{"title":"Deploy production models","text":"<p>With one click, data engineers can deploy their Python models to production without support from software engineering or DevOps teams.</p>","location":"platform/MLOps/#deploy-production-models"},{"title":"Monitor production models","text":"<p>Data teams can:</p> <ul> <li> <p>Ensure that components in a production pipeline operate correctly     through the product lifecycle.</p> </li> <li> <p>Build and deploy services that detect data drift or unexpected     results.</p> </li> </ul>","location":"platform/MLOps/#monitor-production-models"},{"title":"Definitions","text":"<p>The following is a list of definitions to aid understanding of how to work with Quix and streaming data.</p>","location":"platform/definitions/"},{"title":"Workspace","text":"<p>A Workspace is an instance of a complete streaming infrastructure isolated from the rest of your Organization in terms of performance and security. It contains his own dedicated API instances and Quix internal services.</p> <p>You can imagine a Workspace as the streaming infrastructure of your company or your team, where you don\u2019t want other operations except the ones being developed in that workspace affecting the performance or the stability of your application.</p> <p>You can also have different workspaces to separate different stages of your development process like Development, Staging, and Production.</p>","location":"platform/definitions/#workspace"},{"title":"Topics","text":"<p>A Topic is a channel of real-time data. You can imagine a topic as the pipe we use to interconnect our streaming applications.</p> <p>It is highly recommended to organize the data of a topic with some kind of grouping context for the telemetry data coming from a single source. Very simplified, a topic is similar to a folder in a filesystem, the streams are the files in that folder, and your data is the contents of each file.</p> <p>For example:</p> <ul> <li> <p>Car engine data</p> </li> <li> <p>Game data</p> </li> <li> <p>Telemetry from one ECU on a Boeing 737</p> </li> </ul> <p>Topics are key for scalability and good data governance. Use them to organise your data by:</p> <ul> <li> <p>Grouping incoming data by type or source</p> </li> <li> <p>Maintaining separate topics for raw, clean or processed data</p> </li> </ul>","location":"platform/definitions/#topics"},{"title":"Stream","text":"<p>A stream is a collection of data (parameters, events, binary blobs and metadata) that belong to a single session of a single source. For example:</p> <ul> <li> <p>One journey for one car</p> </li> <li> <p>One game session for one player</p> </li> <li> <p>One flight for one aeroplane</p> </li> </ul>","location":"platform/definitions/#stream"},{"title":"Timestamp","text":"<p>A timestamp is the primary key for all data in a stream.</p> <p>We support nanosecond precision; that\u2019s 1 x 10-9 seconds or one-billionth of a second!</p> <p>Nanosecond precision is at the bleeding edge of real-time computing and is primarily driven by innovation with hardware and networking technology; kudos to you if you have an application for it!</p>","location":"platform/definitions/#timestamp"},{"title":"Data Types","text":"<p>We currently support any parameter, event, metadata or blob that consist of numeric (double precision), string (UTF-8) and binary data (blobs).</p>","location":"platform/definitions/#data-types"},{"title":"Parameters","text":"<p>Parameters are values that develop over time. The Quix SDK supports numeric and string values.</p> <p>For example:</p> <ul> <li> <p>Crank revolution and oil temperature are two discrete engine     parameters that begin to define the engine system.</p> </li> <li> <p>Player position in X, Y and Z are three discreet parameters that     begin to define the player location in a game.</p> </li> <li> <p>Altitude, GPS LAT, GPS LONG and Speed are four parameters that begin     to define the location and velocity of a plane in the sky.</p> </li> <li> <p>Referring back to topics as a grouping context: we would recommend     that each of these examples would be grouped into a single topic to     maintain context.</p> </li> </ul>","location":"platform/definitions/#parameters"},{"title":"Events","text":"<p>Events are a discrete occurrence of a thing that happens or takes place.</p> <p>For example:</p> <ul> <li> <p>Engine start, engine stop, warning light activated</p> </li> <li> <p>Game started, match made, kill made, player won the race, lap     completed, track limits exceeded, task completed</p> </li> <li> <p>Takeoff, landing, missile launched, fuel low, autopilot engaged,     pilot ejected</p> </li> </ul> <p>Events are typically things that occur less frequently. They are streamed into the same topics as their respective parameters and act to provide some context to what is happening.</p> <p>Start and stop events mark the beginning and end of data streams.</p>","location":"platform/definitions/#events"},{"title":"Metadata","text":"<p>Metadata describes additional information or context about a stream.</p> <p>For example:</p> <ul> <li> <p>License plate number, car manufacturer, car model, car engine type,     driver ID,</p> </li> <li> <p>Game version, player name, game session type, game session settings,     race car set-up</p> </li> <li> <p>Flight number, destination, airport of origin, pilot ID, airplane     type</p> </li> </ul> <p>Metadata typically has no time context, rather it exists as a constant throughout one or more streams. For example, your metadata could be the configuration of a car that is sold from a dealership (such as engine size, transmission type, wheel size, tyre model etc); you could create a stream every time that car is driven by the owner, but the engine size and transmission type won\u2019t change.</p> <p>Metadata is key to data governance and becomes very useful in down-stream data processing and analytics.</p>","location":"platform/definitions/#metadata"},{"title":"Binary data","text":"<p>Quix also supports any binary blob data.</p> <p>With this data you can stream, process and store any type of audio, image, video or lidar data, or anything that isn\u2019t supported with our parameter, event or metadata types.</p>","location":"platform/definitions/#binary-data"},{"title":"Project","text":"<p>A set of code which can be edited, compiled, executed and deployed as one Docker image.</p>","location":"platform/definitions/#project"},{"title":"Online IDE","text":"<p>We provide an online integrated development environment for python projects. When you open any python project, you will see the Run button and a console during runtime in addition to the intellisense for python files.</p>","location":"platform/definitions/#online-ide"},{"title":"Deployment","text":"<p>An instance of a Project running in the serverless environment.</p>","location":"platform/definitions/#deployment"},{"title":"Service","text":"<p>Any application code that is continuously running in the serverless environment. For example, a bridge, a function, a backend operation, or an integration to a third party service like Twilio.</p>","location":"platform/definitions/#service"},{"title":"Job","text":"<p>Any application code that is run once. For example, use a job to run a batch import of data from an existing data store (CSV, DB or DataLake etc).</p>","location":"platform/definitions/#job"},{"title":"SDK","text":"<p>Quix SDK is the main library we use to send and receive real-time data in our streaming applications.</p>","location":"platform/definitions/#sdk"},{"title":"API\u2019s","text":"","location":"platform/definitions/#apis"},{"title":"Streaming Writer API","text":"<p>A HTTP API used to send telemetry data from any source to a topic in the Quix platform. It should be used when it is not possible to use directly our SDK.</p>","location":"platform/definitions/#streaming-writer-api"},{"title":"Streaming Reader API","text":"<p>A Websockets API used to stream any data directly from a topic to an external application. Most commonly used to read the results of a model or service to a real-time web application.</p>","location":"platform/definitions/#streaming-reader-api"},{"title":"Data Catalogue API","text":"<p>An HTTP API used to query historic data in the Data Catalogue. Most commonly used for dashboarding, analytics and training ML models. Also useful to call historic data when running an ML model or to call historic data from an external application.</p>","location":"platform/definitions/#data-catalogue-api"},{"title":"Portal API","text":"<p>An HTTP API used to interact with most portal-related features such as creation of Workspaces, Users, Deployments, etc.</p>","location":"platform/definitions/#portal-api"},{"title":"What is Quix","text":"<p>Quix is a platform for developing and deploying applications with streaming data.</p> <p>We architected Quix natively around a message broker (specifically Kafka) because we know databases are in the way of building low-latency applications that scale cost-effectively. Instead of working with data on a disk, developers could work with live data in-memory, if broker technologies were easier to use.</p> <p>But they are not easy to use, especially for Python developers who are at the forefront of data science but cannot easily work with streaming data.</p> <p>Quix provides everything a developer needs to build applications with streaming data. By using Quix you can build new products faster whilst keeping your data in-memory, helping to achieve lower latencies and lower operating costs.</p> <p>From the top-down, our stack provides a Web UI, API\u2019s and SDK that abstract developers off our underlying infrastructure, including fully-managed Kafka topics, serverless compute environment and a metadata-driven data catalogue (time-series database with steroids).</p> <p></p>","location":"platform/intro/"},{"title":"Web UI","text":"<p>With the Quix Portal we are striving to make a beautiful software experience that facilitates DevOps/MLOps best-practices for less-experienced development teams. Our goals are to:</p> <ol> <li> <p>Help less expert people access live data</p> </li> <li> <p>Help them create and manage complex infrastructure and write     application code without support from expert engineering teams, and</p> </li> <li> <p>Help to accelerate the development lifecycle by enabling developers     to test and iterate code in an always-live environment.</p> </li> </ol> <p>To achieve these goals Quix Portal includes the following features:</p> <ul> <li> <p>Online IDE: Develop and Run your streaming applications directly     on the browser without setting up a local environment.</p> </li> <li> <p>Library: Choose between hundreds of autogenerated code examples     ready to run and deploy from our Online IDE.</p> </li> <li> <p>One click deployments: Deploy and manage your streaming     applications on production with a simple user interface.</p> </li> <li> <p>Monitoring tools: Monitor in real-time the status and the data     flow of your streaming applications.</p> </li> <li> <p>Broker management: Create, Delete, Explore or Configure your     message broker infrastructure with just a click of a button.</p> </li> <li> <p>Pipeline view: Visualize your pipeline architecture with the     information provided from the deployment variables.</p> </li> <li> <p>Data Explorer: Explore Live and Historical data of your     applications to test that your code is working as expected.</p> </li> </ul>","location":"platform/intro/#web-ui"},{"title":"API\u2019s","text":"<p>We have provided four API\u2019s to help you work with streaming data. These include:</p> <p>Stream Writer API: helps you send any data to a Kafka topic in Quix using HTTP. This API handles encryption, serialisation and conversion to the Quix SDK format ensuring efficiency and performance of down-stream processing regardless of the data source.</p> <p>Stream Reader API: helps you push live data from a Quix topic to your application ensuring super low latency by avoiding any disk operations.</p> <p>Data Catalogue API: lets you query historic data streams in the data catalogue to train ML models, build dashboards and export data to other systems.</p> <p>Portal API: lets you automate Portal tasks like creating workspaces, topics and deployments.</p>","location":"platform/intro/#apis"},{"title":"SDK","text":"<p>Python is the dominant language for data science and machine learning, but it is quite incompatible with streaming technologies (like Kafka) which are predominantly written in Java and Scala.</p> <p>Our Quix streaming SDK is a client library that abstracts Python developers off streaming-centric complexities like learning Java or dealing with buffering, serialisation and encryption.</p> <p>Instead, SDK serves you streaming data in a data frame so you can write any simple or complex data processing logic and connect it directly to the broker. There are just a few key streaming concepts that you must learn. You can read about them here.</p>","location":"platform/intro/#sdk"},{"title":"Serverless compute","text":"<p>Quix provides an easy way to run code in an elastic serverless compute environment. It automatically builds code in GIT into a docker image and deploys containers to Kubernetes. This otherwise very complicated procedure is done by a couple of clicks in the Quix web portal.</p>","location":"platform/intro/#serverless-compute"},{"title":"Architecture","text":"<p></p>","location":"platform/intro/#architecture"},{"title":"Git integration","text":"<p>Source code for workspace projects (models, connectors and services) is hosted in GIT repositories. Developers can check out repositories and develop locally and collaborate using GIT protocol. Code is deployed to the Quix serverless environment using tags in GIT. Quix builds service will build selected GIT commit into a docker image.</p>","location":"platform/intro/#git-integration"},{"title":"Docker integration","text":"<p>Each code example generated using the Quix library is shipped with a <code>Dockerfile</code> that is designed to work in the Quix serverless compute environment powered by Kubernetes. You can alter this file if necessary. When you deploy a service with Quix, a code reference to GIT with a build request is sent to the build queue. The build service will build a docker image and save it in the docker registry. In the next step, this image is deployed to Kubernetes.</p>  <p>Tip</p> <p>If there is any problem with the docker build process, you can check the build logs.</p>   <p>Tip</p> <p>Hover over the deployment in the deployments page to download the docker image of the deployed service for local testing or custom deployment.</p>","location":"platform/intro/#docker-integration"},{"title":"Kubernetes integration","text":"<p>Quix manages an elastic compute environment so you don\u2019t need to worry about servers, nodes, memory, CPU, etc. Quix will make sure that your container is deployed to the right server in the cluster.</p> <p>We provide the following integrations with Kubernetes:</p> <ul> <li> <p>Logs from container accessible in the portal or via portal API.</p> </li> <li> <p>Environment variables allows passing variables into the docker     image deployment. So code can be parameterized.</p> </li> <li> <p>Replica number for horizontal scale.</p> </li> <li> <p>CPU limit.</p> </li> <li> <p>Memory limit.</p> </li> <li> <p>Deployment type - Options of one-time job or continuously     running service,</p> </li> <li> <p>Ingress - Optional ingress mapped to port 80.</p> </li> </ul>  <p>Tip</p> <p>If a deployment reference is already built and deployed to a service, the build process is skipped and the docker image from the container registry is used instead.</p>","location":"platform/intro/#kubernetes-integration"},{"title":"DNS integration","text":"<p>The Quix serverless environment offers DNS routing for services on port 80. That means that any API or frontend can be hosted in Quix with no extra complexity. Load balancing is provided out of the box, just increase the replica count to provide resiliency to your deployed API or frontend.</p>  <p>Warning</p> <p>A newly deployed service with DNS routing takes up to 10 minutes to propagate to all DNS servers in the network.</p>","location":"platform/intro/#dns-integration"},{"title":"Managed Kafka topics","text":"<p>Quix provides fully managed Kafka topics which are used to stream data and build data processing pipelines by daisy-chaining models together.</p> <p>Our topics are multi-tenant which means you don\u2019t have to build and maintain an entire cluster to stream a few bytes of data. Instead, you can start quickly and cheaply by creating one topic for your application and only pay for the resources consumed when streaming that data. When your solution grows in data volume or complexity you can just add more topics without concern for the underlying infrastructure which is handled by us.</p> <p>Together with our SDK and serverless compute, you can connect your models directly to our topics to read and write data using the pub/sub pattern. This keeps the data in-memory to deliver low-latency and cost effective stream processing capabilities.</p>  <p>Note</p> <p>Quix also provides the ability to connect external infrastructure components like your own message broker infrastructure.</p>","location":"platform/intro/#managed-kafka-topics"},{"title":"Data Catalogue","text":"<p>We provide a data catalogue for long-term storage, analytics and data science activities.</p> <p>We have combined what we know to be the best database technologies for each data type into a unified catalogue. There\u2019s a timeseries database for recording your events and parameter values, blob storage for your binary data, and a NoSQL DB for recording your metadata.</p> <p>Our data catalogue technology has two advantages:</p> <ol> <li> <p>It allocates each data type to the optimal database technology for     that type. This increases read/write and query performance which     reduces operating costs.</p> </li> <li> <p>It uses your metadata to record your context. This makes your data     more usable for more people across your organisation who only need     to know your business context to navigate vast quantities of data.</p> </li> </ol>","location":"platform/intro/#data-catalogue"},{"title":"Get Started","text":"<p>If you\u2019re new to Quix, these resources are the best place to get you up and running quickly.</p> <p>What is Quix?.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p> <p>Quick start guides.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p> <p>Code sample library.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p>","location":"platform/landing-page/"},{"title":"Core resources","text":"<p>Take a look under the hood of Quix and get to know our SDK and APIs.</p> <p>Streaming SDK.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p> <p>Streaming APIs.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p> <p>Automation API.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p>","location":"platform/landing-page/#core-resources"},{"title":"Build your project","text":"<p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Docs.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p> <p>Model deployment.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p> <p>Stream live data.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p> <p>Work with the catalogue.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p> <p>Model training.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p> <p>Integrate to client apps.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p>","location":"platform/landing-page/#build-your-project"},{"title":"Learn from the community","text":"<p>Take a look at our community resources to learn how other developers and data teams are using Quix to solve their software engineering and data science problems.</p> <p>Discourse.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p> <p>Slack.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p> <p>Github.</p> <p>Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.</p> <p>Learn More</p>","location":"platform/landing-page/#learn-from-the-community"},{"title":"Create a Dead Letter Queue","text":"<p>When your code identifies a dead letter, or unprocessable message, simply send the message to the dead letter topic and continue processing other messages.</p> <p>What you do with messages in the dead letter topic is up to you.</p>","location":"platform/how-to/create-dlq/"},{"title":"Example","text":"<p>Take a look at the short example. It describes how to open topics, create streams and also send messages to a dedicated dead letter topic or queue.</p> PythonC#   <pre><code># open topics\ninput_topic = client.open_input_topic('INPUT_DATA')\noutput_topic = client.open_output_topic('OUTPUT_DATA')\ndead_letter_topic = client.open_output_topic('UNPROCESSABLE')\n\n# create streams\noutput_stream = output_topic.create_stream()\ndlq_stream = dead_letter_topic.create_stream()\n\n# open the input stream\n# and start handling messages\ndef on_stream_received_handler(new_stream: StreamReader):\n        def on_parameter_data_handler(data_in: ParameterData):\n\n                try:\n                        # get a data value\n                        data_to_process = data_in.timestamps[0].parameters['ParameterA'].numeric_value\n\n                        # prepare the data packet for onward processing\n                        data_out = ParameterData()\n                        data_out.add_timestamp_nanoseconds(1) \\\n                                .add_value(\"Speed\", data_to_process)\n\n                        # it was ok so pass to the output stream\n                        # to be processed by the next stage in the pipeline\n                        output_stream.parameters.write(data_out)\n\n                except Exception:\n                        # There was an error during processing.\n            # Print the error and forward data into dead letter queue.\n            print(traceback.format_exc())\n\n                        dlq_stream.parameters.write(data_in)\n\n        # hook up on read handler\n        new_stream.on_read += on_parameter_data_handler\n\ninput_topic.on_stream_received += on_stream_received_handler\ninput_topic.start_reading()\n</code></pre>   <pre><code>var inputTopic = client.OpenInputTopic(\"INPUT_DATA\");\nvar outputTopic = client.OpenOutputTopic(\"OUTPUT_DATA\");\nvar deadLetterTopic = client.OpenOutputTopic(\"UNPROCESSABLE\");\n\n// create streams\nvar outputStream = outputTopic.CreateStream();\nvar dlqStream = deadLetterTopic.CreateStream();\n\n// open the input stream\ninputTopic.OnStreamReceived += (s, streamReader) =&gt;\n{\n    streamReader.Parameters.OnRead += dataIn =&gt;\n    {\n        try\n        {\n            // get the data to process\n            var numValue = dataIn.Timestamps[0].Parameters['ParameterA'].NumericValue;\n\n            // create the data for onward processing\n            var data = new ParameterData();\n\n            data.AddTimestampNanoseconds(1)\n                .AddValue(\"Speed\", numValue);\n\n            // it was ok so pass to the output stream\n            // to be processed by the next stage in the pipeline\n            outputStream.Parameters.Write(data);\n        }\n        catch (Exception e)\n        {\n            // There was an error during processing.\n            // Print the error and forward data into dead letter queue.\n            System.Console.WriteLine(e);\n\n            dlqStream.Parameters.Write(dataIn);\n        }\n    };\n};\n\ninputTopic.StartReading();\n</code></pre>","location":"platform/how-to/create-dlq/#example"},{"title":"Get Workspace ID","text":"<p>Occasionally, you\u2019ll need to obtain an ID based on a specific workspace. For example, endpoints for the Data Catalogue API use a domain with the following pattern:</p> <pre><code>https://telemetry-query-${workspace-id}.platform.quix.ai/\n</code></pre> <p>The workspace ID is a combination of your organisation and workspace names, converted to URL friendly values. The easist way to get hold of it is as follows:</p> <ol> <li> <p>Go to the Portal home.</p> </li> <li> <p>Locate the workspace you\u2019re interested in and click OPEN.</p> </li> <li> <p>At this point, take note of the URL. It should be in the form:</p> </li> </ol>  <pre><code>https://portal.platform.quix.ai/home?workspace=**{workspace-id}**\n</code></pre> <p>Copy that value and use it wherever you need a workspace ID.</p>","location":"platform/how-to/get-workspace-id/"},{"title":"Use Jupyter notebooks","text":"<p>In this article, you will learn how to use Jupyter Notebook to analyse data persisted in the Quix platform</p>","location":"platform/how-to/jupyter-nb/"},{"title":"Why this is important","text":"<p>Although Quix is a realtime platform, to build realtime in-memory models and data processing pipelines, we need to understand data first. To do that, Quix offers a Data catalogue that makes data discovery and analysis so much easier.</p>","location":"platform/how-to/jupyter-nb/#why-this-is-important"},{"title":"Preparation","text":"<p>You\u2019ll need some data stored in the Quix platform. You can use any of our Data Sources available in the samples Library, or just follow the onboarding process when you sign-up to Quix,window=_blank.</p> <p>You will also need Python 3 environment set up in your local environment.</p>","location":"platform/how-to/jupyter-nb/#preparation"},{"title":"Install required libraries","text":"<pre><code>python3 -m pip install jupyter\npython3 -m pip install requests\npython3 -m pip install pandas\n</code></pre>","location":"platform/how-to/jupyter-nb/#install-required-libraries"},{"title":"Run Jupyter notebook server","text":"<pre><code>jupyter-notebook\n</code></pre>","location":"platform/how-to/jupyter-nb/#run-jupyter-notebook-server"},{"title":"Create a new notebook file","text":"<p>Run jupyter with the following command:</p> <p>``` jupyter notebook ```</p> <p>Then create a new Python3 notebook</p> <p></p>","location":"platform/how-to/jupyter-nb/#create-a-new-notebook-file"},{"title":"Connecting Jupyter notebook to Data Catalogue","text":"<p>The Quix web application has a python code generator to help you connect your Jupyter notebook with Quix.</p> <ol> <li> <p>Go to the platform</p> </li> <li> <p>Select workspace</p> </li> <li> <p>Go to the Data catalogue</p> </li> <li> <p>Select data to visualize</p> </li> <li> <p>Select parameters, events, aggregation and time range</p> </li> <li> <p>Press Connect button</p> </li> <li> <p>Select Python language</p> </li> </ol> <p></p> <p>Copy Python code to your Jupyter notebook and execute.</p> <p></p>  <p>Tip</p> <p>If you want to use this generated code for a long time, replace the temporary token with PAT token. See authenticate your requests how to do that.</p>","location":"platform/how-to/jupyter-nb/#connecting-jupyter-notebook-to-data-catalogue"},{"title":"Too much data","text":"<p>If you find that the query results in more data than can be handled by Jupyter Notebooks try using the aggregation feature to reduce the amount of data returned.</p> <p>For more info on aggregation check out this short video.</p>","location":"platform/how-to/jupyter-nb/#too-much-data"},{"title":"Use SDK token","text":"<p>SDK token is a type of bearer token that can be used to authenticate against some of our APIs to access functionality necessary for streaming actions. Think of SDK tokens like a token you use to access portal but very limited in scope.</p> <p>Each workspace comes with two of these tokens, limited in use for that specific workspace. We call them <code>Token 1</code> and <code>Token 2</code>, also known as <code>Current</code> and <code>Next</code> token.</p>","location":"platform/how-to/use-sdk-token/"},{"title":"How to find","text":"<p>You can access these tokens by going to your topics and clicking on broker settings.</p>","location":"platform/how-to/use-sdk-token/#how-to-find"},{"title":"How to use","text":"<p>These tokens can be used to authenticate against the API, but their primary intended use is to be used with the SDK QuixStreamingClient. When using it with QuixStreamingClient, you no longer need to provide all broker credentials manually, they\u2019ll be acquired when needed and set up automatically.</p> <p>When deploying or running an online IDE, among other environment variables <code>Quix__Sdk__Token</code> is injected with value of <code>Token 1</code>.</p> <p>You should always use <code>Token 1</code>, unless you\u2019re rotating.</p>  <p>Caution</p> <p>Your tokens do not have an expiration date. Treat them as you would a password. If you think they\u2019re exposed, rotate them.</p>","location":"platform/how-to/use-sdk-token/#how-to-use"},{"title":"Rotating","text":"<p>Having two keys lets you update your services without interruption, as both <code>Token 1</code> and <code>Token 2</code> are always valid. Rotating deactivates <code>Token 1</code>, <code>Token 2</code> takes its place and a new <code>Token 2</code> will be generated.</p> <p>You have two main options regarding how you rotate.</p> <p>The easiest way to rotate comes with some service downtime. This assumes you do not directly set the token for your QuixStreamingClient, instead you let the platform take care of it for you by using the default <code>Quix__Sdk__Token</code> environment variable. In this scenario all you have to do is rotate keys, stop and start all your deployments. Until a service is restarted it\u2019ll try to communicate with the platform using the deactivated token. If you\u2019re using local envinroments, those need updating manually.</p> <p>The alternative option is a bit more labour intense, but you can achieve no downtime. This requires you to set a new environment variable you control. This should point to the token to be used. Provide the value of this environment variable to QuixStreamingClient by passing it as an argument. Once you have that, set the value of this environment variable to <code>Token 2</code> and start your services. When you\u2019re sure you replaced the tokens for all services, rotate your keys.</p>  <p>Note</p> <p>Only users with Admin role can rotate.</p>","location":"platform/how-to/use-sdk-token/#rotating"},{"title":"Develop web applications with Quix","text":"<p>Quix can bring real-time web functionality to you client applications. Following types of applications are good candidates to use Quix as their data plane.</p> <ul> <li> <p>Dashboard and real-time monitoring applications that show updates as     they happen to users like cloud/edge monitoring tools.</p> </li> <li> <p>Applications that require data to be pushed from a backend at high     frequency like games and simulations.</p> </li> <li> <p>Social networking applications that require broadcasting updates to     many users at high frequency like live sharing of Strava data.</p> </li> </ul>","location":"platform/how-to/webapps/"},{"title":"NodeJs","text":"<p>NodeJs applications can update parameter and event definitions and write data to streams using RESTful APIs. Quix supports WebSockets for clients that want to receive telemetry data and parameters/events updates in real-time. NodeJs clients must authenticate with Quix using personal access tokens.</p>","location":"platform/how-to/webapps/#nodejs"},{"title":"Run ML model in realtime environment","text":"<p>In this article, you will learn how to use pickle file trained on historic data in a realtime environment.</p>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/"},{"title":"Why this is important","text":"<p>With the Quix platform, you can run and deploy ML models to the leading edge reacting to data coming from the source with milliseconds latency.</p>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#why-this-is-important"},{"title":"End result","text":"<p>At the end of this article, we will end up with a live model using pickle file from How to train ML model to process live data on the edge.</p> <p></p>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#end-result"},{"title":"Preparation","text":"<p>You\u2019ll need to complete How to train ML model article to get pickle file with trained model logic.</p>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#preparation"},{"title":"Set up topics","text":"<p>You should already have a topic containing car engine data. You\u2019ll use this as an input topic for the model. The model also needs an output topic, to send the processed data.</p> <ol> <li> <p>Click the Topics icon  in the     left-hand menu.</p> </li> <li> <p>Click the CREATE TOPIC button, give your new topic a name \u2014     brake-prediction \u2014 and click CREATE.</p> </li> <li> <p>Change the Persistence toggle to \u201cON\u201d, to ensure data in the topic     is persisted to the Data     Catalogue.</p> </li> </ol>  <p>Note</p> <p>At this point, you should have at least two topics: cars and brake-prediction.</p>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#set-up-topics"},{"title":"Creating Git project in workspace","text":"<p>You can use our sample model project as a template. You\u2019ll get code assets for a basic model, customised to your specific environment. This includes certificates for remote sdk access, and details of the two topics you\u2019ve set up.</p> <ol> <li> <p>Click the Topics icon  in the     left-hand menu.</p> </li> <li> <p>Hover over the cars topic and click the Connect icon     . You\u2019ll see the     connect\" button which will launch the Easy connect wizard which     allows you to quickly create a project based on our sample code.</p> </li> <li> <p>Select PYTHON as your code language, then click NEXT.</p> </li> <li> <p>Select MODEL as your Template type, then click NEXT.</p> </li> <li> <p>Select cars as your input topic, then click NEXT.</p> </li> <li> <p>Select brake-prediction as your output topic, then click     FINISH.</p> </li> </ol> <p>At this point, you\u2019ll be taken to the Model tab within the Library section. You should see the Data filtering project, with custom values for your username, workspace, topics, etc.</p> <p></p>  <p>Tip</p> <p>In the code screenshot above, lines 30-40 (highlighted) contain code that drives the model. We will call trained model in this section.</p>  <ol> <li>Click SAVE AS PROJECT and name your project brake-model,     then click CREATE.</li> </ol> <p>You\u2019ll now be taken to the Develop section, where you should see a full file view of your project, alongside the Quix web IDE.</p>  <p>Tip</p> <p>In your new project you get security constants filled to work with your workspace.</p>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#creating-git-project-in-workspace"},{"title":"Clone Git project locally","text":"<p>To get information how to clone Git project, go to your project and click CLONE.</p> <p>You\u2019ll also need to setup Python environment to use Quix SDK.</p> <p>We are going to use library <code>sklearn</code>.</p> <p>Install it in your local environment by:</p> <pre><code>python3 -m pip install sklearn\n</code></pre> <p>and add it to <code>requirements.txt</code> file in your project to run this code in Quix serverless environment.</p> <p>quixstreaming==0.2.2a202101281720 sklearn\\&lt;/programlisting&gt;</p>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#clone-git-project-locally"},{"title":"Using Pickle file in realtime model","text":"<p>We have pickle file from How to train ML model. Create a <code>model</code> folder in root of the project and copy the <code>decision_tree_5_depth.sav</code> file into it. Then push to git.</p> <p>Than we load it to memory in code:</p> <pre><code>import pickle\n\n# Import ML model from file\nmodel = pickle.load(open('../model/decision_tree_5_depth.sav', 'rb'))\n</code></pre>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#using-pickle-file-in-realtime-model"},{"title":"Full code example","text":"<p>Here is a full example. We will explain sections of code in more details bellow.</p> <pre><code>from quixstreaming import *\nfrom quixstreaming.app import App\nfrom quixstreaming.models.parametersbufferconfiguration import ParametersBufferConfiguration\nimport threading\nimport signal\nimport pandas as pd\nimport traceback\nimport math\nimport pickle\n\n# Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument.\nclient = QuixStreamingClient()\n\ninput_topic = client.open_input_topic('INPUT_TOPIC', 'brake-prediction')\noutput_topic = client.open_output_topic('OUTPUT_TOPIC')\n\n\n# Import ML model from file\nmodel = pickle.load(open('../model/decision_tree_5_depth.sav', 'rb'))\n\n# To get the correct output, we preprocess data before we feed them to the trained model\ndef preprocess(df):\n    signal_limits = {\n        \"Throttle\": (0, 1),\n        \"Steer\": (-1, 1),\n        \"Motion_Yaw\": (-math.pi, math.pi),\n        \"Motion_Roll\": (-math.pi, math.pi),\n        \"Motion_Pitch\": (-math.pi, math.pi),\n        \"Brake\": (0, 1),\n    }\n\n    def clamp(n, minn, maxn):\n        return max(min(maxn, n), minn)\n\n    for signal, limits in signal_limits.items():\n        df[signal] = df[signal].map(lambda x: clamp(x, limits[0], limits[1]))\n\n    df[\"Motion_Yaw_sin\"] = df[\"Motion_Yaw\"].map(lambda x: math.sin(x))\n    df[\"Motion_Yaw_cos\"] = df[\"Motion_Yaw\"].map(lambda x: math.cos(x))\n\n    return df\n\n# Callback called for each incoming stream\ndef read_stream(new_stream: StreamReader):\n    print(\"New stream read:\" + new_stream.stream_id)\n\n    # Create a new stream to output data\n    stream_writer = output_topic.create_stream(new_stream.stream_id + \"-brake-prediction\")\n\n    # Reference output stream as child model of input stream.\n    stream_writer.properties.parents.append(new_stream.stream_id)\n\n    # Publish metedata about new parameter created by this model.\n    stream_writer.parameters \\\n        .add_definition(\"brake-prediction\", \"Brake prediction\", \"Probability of braking 5 seconds from now.\") \\\n        .set_range(0.0, 1.0)\n\n    buffer_options = ParametersBufferConfiguration()\n    buffer_options.time_span_in_milliseconds = 100  # React to incoming data in 100ms windows.\n\n    buffer = new_stream.parameters.create_buffer(buffer_options)\n\n    # Each 100ms window of data callback.\n    def on_parameter_data_handler(data: ParameterData):\n        try:\n            brake_predictions = pd.DataFrame()  # Output data frame.\n            df = data.to_panda_frame()  # Input data frame\n\n            # Preprocessing\n            df = preprocess(df)\n            features = [\"Motion_Yaw_cos\", \"Motion_Yaw_sin\", \"Steer\", \"Speed\", \"Gear\"]\n            X = df[features]\n\n            # Lets shift data into the feature by 5 seconds. (Note that time column is in nanoseconds).\n            brake_predictions[\"time\"] = df[\"time\"].apply(lambda x: int(x) + int((5 * 1000 * 1000 * 1000)))\n            brake_predictions[\"brake-prediction\"] = model.predict(X)\n\n            # Send data frame to output topic\n            stream_writer.parameters.write(brake_predictions)\n        except Exception:\n            traceback.print_exc()\n\n    buffer.on_read += on_parameter_data_handler  # React to new 100ms window received from input topic.\n\n    def on_stream_close(end_type: StreamEndType):\n        stream_writer.close(end_type)\n        print(\"Stream closed:\" + stream_writer.stream_id)\n\n    new_stream.on_stream_closed += on_stream_close  # React to input stream closed event and close output as well.\n\n    def stream_properties_changed():\n        # When stream name property is sent, we pass that to output stream with suffix.\n        stream_writer.properties.name = new_stream.properties.name + \" braking prediction\"\n\n    new_stream.properties.on_changed += stream_properties_changed\n\n\n# Hook up events before initiating read to avoid losing out on any data\ninput_topic.on_stream_received += read_stream\n\n# Hook up to termination signal (for docker image) and CTRL-C\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n\n# Handle graceful exit of the model.\nApp.run()\n</code></pre>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#full-code-example"},{"title":"Pub &amp; Sub","text":"<p>We will use the same approach as any other Transformation model available in Quix Library. We will read from input topic, call trained model and write the result to output topic.</p> <pre><code>from quixstreaming import *\nfrom quixstreaming.app import App\nfrom quixstreaming.models.parametersbufferconfiguration import ParametersBufferConfiguration\nfrom preprocessing import preprocess\nimport threading\nimport signal\nimport pandas as pd\nimport traceback\nimport pickle\n\n# Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument.\nclient = QuixStreamingClient()\n\ninput_topic = client.open_input_topic('INPUT_TOPIC', 'brake-prediction')\noutput_topic = client.open_output_topic('OUTPUT_TOPIC')\n</code></pre>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#pub-sub"},{"title":"Listen to new streams","text":"<p>We will register a callback to handle each incoming stream from input topic:</p> <pre><code># Callback called for each incoming stream\ndef read_stream(new_stream: StreamReader):\n    # Here implement stream processing.\n    ...\n\n\n# Hook up events before initiating read to avoid losing out on any data\ninput_topic.on_stream_received += read_stream\ninput_topic.start_reading()  # initiate read\n\n# Hook up to termination signal (for docker image) and CTRL-C\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n\nevent = threading.Event()\n\ndef signal_handler(sig, frame):\n    print('Exiting...')\n    event.set()\n\n\nsignal.signal(signal.SIGINT, signal_handler)\nsignal.signal(signal.SIGTERM, signal_handler)\nevent.wait()\n</code></pre>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#listen-to-new-streams"},{"title":"Stream processing","text":"<p>In each individual stream, we will subscribe for parameter data.</p> <pre><code># Callback called for each incoming stream\ndef read_stream(new_stream: StreamReader):\n    print(\"New stream read:\" + new_stream.stream_id)\n\n    # Create a new stream to output data\n    stream_writer = output_topic.create_stream(new_stream.stream_id + \"-brake-prediction\")\n\n    # Reference output stream as child model of input stream.\n    stream_writer.properties.parents.append(new_stream.stream_id)\n\n    buffer_options = ParametersBufferConfiguration()\n    buffer = new_stream.parameters.create_buffer(buffer_options)\n\n    # Each 100ms window of data callback.\n    def on_parameter_data_handler(data: ParameterData):\n        # Implement data processing here.\n        ....\n</code></pre>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#stream-processing"},{"title":"Parameter data processing","text":"<p>Now we get to the most important part of the model. We will feed incoming data to the trained model to get a result that we send to the output topic.</p> <pre><code># Each 100ms window of data callback.\ndef on_parameter_data_handler(data: ParameterData):\n    brake_predictions = pd.DataFrame()  # Output data frame.\n    df = data.to_panda_frame()  # Input data frame\n\n    # Preprocessing\n    df = preprocess(df)\n    features = [\"Motion_Yaw_cos\", \"Motion_Yaw_sin\", \"Steer\", \"Speed\", \"Gear\"]\n    X = df[features]\n\n    # Lets shift data into the feature by 5 seconds. (Note that time column is in nanoseconds).\n    brake_predictions[\"time\"] = df[\"time\"].apply(lambda x: int(x) + int((5 * 1000 * 1000 * 1000)))\n    brake_predictions[\"brake-prediction\"] = model.predict(X)\n\n    # Send data frame to output topic\n    stream_writer.parameters.write(brake_predictions)\n</code></pre>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#parameter-data-processing"},{"title":"Preprocessing","text":"<p>To get the correct output, we preprocess data before we feed them to the trained model:</p> <pre><code>def preprocess(df):\n    signal_limits = {\n        \"Throttle\": (0, 1),\n        \"Steer\": (-1, 1),\n        \"Motion_Yaw\": (-math.pi, math.pi),\n        \"Motion_Roll\": (-math.pi, math.pi),\n        \"Motion_Pitch\": (-math.pi, math.pi),\n        \"Brake\": (0, 1),\n    }\n\n    def clamp(n, minn, maxn):\n        return max(min(maxn, n), minn)\n\n    for signal, limits in signal_limits.items():\n        df[signal] = df[signal].map(lambda x: clamp(x, limits[0], limits[1]))\n\n    df[\"Motion_Yaw_sin\"] = df[\"Motion_Yaw\"].map(lambda x: math.sin(x))\n    df[\"Motion_Yaw_cos\"] = df[\"Motion_Yaw\"].map(lambda x: math.cos(x))\n\n    return df\n</code></pre>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#preprocessing"},{"title":"Deployment","text":"<p>And that\u2019s it! Now is time to run the model, run the data generator and see it in action!</p>","location":"platform/how-to/train-and-deploy-ml/deploy-ml/#deployment"},{"title":"How to train an ML model","text":"<p>In this article, you will learn how to manage ML model training with Quix. In this example, we will train a model to predict car braking on a racing circuit 5 seconds ahead of time.</p>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/"},{"title":"Why this is important","text":"<p>With the Quix platform, you can leverage historic data to train your model to react to data coming from source with milliseconds latency.</p>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#why-this-is-important"},{"title":"End result","text":"<p>At the end of this article, we will end up with a pickle file trained on historic data.</p> <p></p>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#end-result"},{"title":"Preparation","text":"<p>You will need Python3 installed.</p> <p>You\u2019ll need some data stored in the Quix platform. You can use any of our Data Sources available in the samples Library, or just follow the onboarding process when you sign-up to Quix,window=_blank.</p> <p>You\u2019ll also need a Jupyter notebook environment to run your experiments and load data for training. Please use \"How to work with Jupyter notebook\".</p>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#preparation"},{"title":"Install required libraries","text":"<pre><code>python3 -m pip install seaborn\npython3 -m pip install sklearn\npython3 -m pip install mlflow\npython3 -m pip install matplotlib\n</code></pre>  <p>Tip</p> <p>If you don\u2019t see Python3 kernel in your Jupyter notebook, execute the following commands in your python environment:  <pre><code>python3 -m pip install ipykernel\npython3 -m ipykernel install --user\n</code></pre></p>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#install-required-libraries"},{"title":"Necessary imports","text":"<p>To execute all code blocks below, you need to start with importing these libraries. Add this code to the top of you Jupyter notebook.</p> <pre><code>import math\nimport matplotlib.pyplot as plt\nimport mlflow\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport seaborn as sns\n\nfrom sklearn import tree\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n</code></pre>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#necessary-imports"},{"title":"Training ML model","text":"","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#training-ml-model"},{"title":"Getting training data","text":"<p>Quix web application has a python code generator to help you connect your Jupyter notebook to Quix.</p> <ol> <li> <p>Go to the platform</p> </li> <li> <p>Select workspace</p> </li> <li> <p>Go to the Data catalogue </p> </li> <li> <p>Select car data stream to visualize     </p> </li> <li> <p>Select <code>Brake</code>, <code>Motion_Yaw</code>, <code>Steer</code>, <code>Speed</code>, <code>Gear</code> parameters</p> </li> <li> <p>Press Connect button</p> </li> <li> <p>Select Python language</p> </li> <li> <p>Copy Python code to your Jupyter notebook and execute.</p> </li> </ol> <p></p> <p></p>  <p>Tip</p> <p>If you want to use this generated code for a long time, replace the temporary token with a PAT token. See authenticate your requests for how to do that.</p>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#getting-training-data"},{"title":"Preprocessing of features","text":"<p>We will prepare data for training by applying some transformation on downloaded data.</p> <pre><code># Convert yaw angle to continuous values\ndf[\"Motion_Yaw_sin\"] = df[\"Motion_Yaw\"].map(lambda x: math.sin(x))\ndf[\"Motion_Yaw_cos\"] = df[\"Motion_Yaw\"].map(lambda x: math.cos(x))\n</code></pre>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#preprocessing-of-features"},{"title":"Preprocessing of label","text":"<p>Here we simplify breaking to boolean.</p> <pre><code># Conversion of label\ndf[\"Brake_bool\"] = df[\"Brake\"].map(lambda x: round(x))\n</code></pre>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#preprocessing-of-label"},{"title":"Generate advanced brake signal for training","text":"<p>Now we need to shift breaking 5 seconds ahead to train model to predict breaking 5 seconds ahead.</p> <pre><code># Offset dataset and trim it\nNUM_PERIODS = -round(5e9/53852065.77281786)\n\ndf[\"Brake_shifted_5s\"] = df[\"Brake_bool\"].shift(periods=NUM_PERIODS)\ndf = df.dropna(axis='rows')\n</code></pre> <p>Lets review it in plot:</p> <pre><code>plt.figure(figsize=(15, 8))\nplt.plot(df[\"Brake_shifted_5s\"])\nplt.plot(df[\"Brake_bool\"])\nplt.legend(['Shifted', 'Unshifted'])\n</code></pre> <p></p>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#generate-advanced-brake-signal-for-training"},{"title":"Fit, predict and score a model","text":"<p>Calculate class weighting in case we gain any accuracy by performing class balancing.</p> <pre><code>Y = df[\"Brake_shifted_5s\"]\n\ncw = {}\nfor val in set(Y):\n    cw[val] = np.sum(Y != val)\n\nprint(cw)\n</code></pre>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#fit-predict-and-score-a-model"},{"title":"Experiment","text":"<p>In the following code snippet we are executing an experiment using MLflow. Notice in last 3 lines that each experiment is logging MLflow metrics for experiments comparison later.</p> <pre><code>model_accuracy = pd.DataFrame(columns=[\n    'Baseline Training Accuracy',\n    'Model Training Accuracy',\n    'Baseline Testing Accuracy',\n    'Model Testing Accuracy',\n])\n\nkfold = KFold(5, shuffle=True, random_state=1)\n\nwith mlflow.start_run():\n    class_weight = None\n    max_depth = 5\n    features = [\"Motion_Yaw_cos\", \"Motion_Yaw_sin\", \"Steer\", \"Speed\", \"Gear\"]\n\n    mlflow.log_param(\"class_weight\", class_weight)\n    mlflow.log_param(\"max_depth\", max_depth)\n    mlflow.log_param(\"features\", features)\n    mlflow.log_param(\"model_type\", \"DecisionTreeClassifier\")\n\n    X = df[features]\n    decision_tree = DecisionTreeClassifier(class_weight=class_weight, max_depth=max_depth)\n\n    for train, test in kfold.split(X):\n        X_train = X.iloc[train]\n        Y_train = Y.iloc[train]\n        X_test = X.iloc[test]\n        Y_test = Y.iloc[test]\n\n        # Train model\n        decision_tree.fit(X_train, Y_train)\n        Y_pred = decision_tree.predict(X_test)\n\n        # Assess accuracy\n        train_accuracy = round(decision_tree.score(X_train, Y_train) * 100, 2)\n        test_accuracy = round(decision_tree.score(X_test, Y_test) * 100, 2)\n\n        Y_baseline_zeros = np.zeros(Y_train.shape)\n        baseline_train_accuracy = round(accuracy_score(Y_train, Y_baseline_zeros) * 100, 2)\n        Y_baseline_zeros = np.zeros(Y_test.shape)\n        baseline_test_accuracy = round(accuracy_score(Y_test, Y_baseline_zeros) * 100, 2)\n\n        model_accuracy = model_accuracy.append({\n            \"Baseline Training Accuracy\": baseline_train_accuracy,\n            \"Model Training Accuracy\": train_accuracy,\n            \"Baseline Testing Accuracy\": baseline_test_accuracy,\n            \"Model Testing Accuracy\": test_accuracy\n        }, ignore_index=True)\n\n    mlflow.log_metric(\"train_accuracy\", model_accuracy[\"Model Training Accuracy\"].mean())\n    mlflow.log_metric(\"test_accuracy\", model_accuracy[\"Model Testing Accuracy\"].mean())\n    mlflow.log_metric(\"fit_quality\", 1/abs(model_accuracy[\"Model Training Accuracy\"].mean() - model_accuracy[\"Model Testing Accuracy\"].mean()))\n</code></pre> <p>We review experiment model accuracy:</p> <pre><code>model_accuracy\n</code></pre>             Depth Baseline Training Accuracy Model Training Accuracy Baseline Testing Accuracy Model Testing Accuracy   0 88.97 97.93 86.49 86.49   1 87.59 97.24 91.89 83.78   2 89.04 96.58 86.11 88.89   3 88.36 97.95 88.89 83.33   4 88.36 97.95 88.89 80.56    <p>Table with model accuracy preview</p>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#experiment"},{"title":"Prediction preview","text":"<p>Let\u2019s plot actual vs predicted braking using a trained model:</p> <pre><code>f, (ax1, ax2) = plt.subplots(2, 1, sharey=True, figsize=(50,8))\nax1.plot(Y)\nax1.plot(X[\"Speed\"]/X[\"Speed\"].max())\n\nax2.plot(decision_tree.predict(X))\nax2.plot(X[\"Speed\"]/X[\"Speed\"].max())\n</code></pre> <p></p>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#prediction-preview"},{"title":"Saving model","text":"<p>When you are confident with the results, save the model into a file and commit it to GIT.</p> <pre><code>pickle.dump(decision_tree, open('./decision_tree_5_depth.sav', 'wb'))\n</code></pre>  <p>Tip</p> <p>Pickle file will be located in folder where jupyter notebook command was executed</p>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#saving-model"},{"title":"MLflow","text":"<p>To help you with experiments management, you can review experiments in MLflow.</p>  <p>Warning</p> <p>MLflow is working only in MacOS, Linux or Windows linux subsystem.</p>   <p>Tip</p> <p>To have some meaningful data, run the experiment with 3 different <code>max_depth</code> parameter.</p>  <p>Let\u2019s leave Jupyter notebook for now and go back to command line and run MLflow server:</p> <pre><code>mlflow ui\n</code></pre> <p>Select experiments to compare:</p> <p></p> <p>Plot metrics from experiments:</p> <p></p>","location":"platform/how-to/train-and-deploy-ml/train-ml-model/#mlflow"},{"title":"Read from Quix with NodeJs","text":"<p>Quix supports real-time data streaming over WebSockets. JavaScript clients can receive updates on parameter and event definition updates, parameter data and event data as they happen. Following examples use SignalR client library to connect to Quix over WebSockets.</p>","location":"platform/how-to/webapps/read/"},{"title":"Setting up SignalR","text":"<p>If you are using a package manager like npm, you can install SignalR using <code>npm install @microsoft/signalr</code>. For other installation options that don\u2019t depend on a platform like Node.js such as consuming SignalR from a CDN please refer to SignalR documentation.</p> <p>Following code snippet shows how you can connect to Quix after SignalR has been setup.</p> <pre><code>var signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () =&gt; 'your_access_token'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://reader-your-workspace-id.portal.quix.ai/hub\", options)\n    .build();\n\nconnection.start().then(() =&gt; console.log(\"SignalR connected.\"));\n</code></pre> <p>If the connection is successful, you should see the console log \"SignalR connected\".</p>","location":"platform/how-to/webapps/read/#setting-up-signalr"},{"title":"Reading data from a stream","text":"<p>Before you can read data from a stream, you need to subscribe to an event like parameter definition, event definition, parameter data or event data.</p> <p>Following is an example of establishing a connection to Quix, subscribing to a parameter data stream, reading data from that stream, and unsubscribing from the event using a SignalR client.</p> <pre><code>var signalR = require(\"@microsoft/signalr\");\n\nconst options = {\n    accessTokenFactory: () =&gt; 'your_access_token'\n};\n\nconst connection = new signalR.HubConnectionBuilder()\n    .withUrl(\"https://reader-your-workspace-id.portal.quix.ai/hub\", options)\n    .build();\n\n// Establish connection\nconnection.start().then(() =&gt; {\n    console.log(\"Connected to Quix.\");\n\n    // Subscribe to parameter data stream.\n    connection.invoke(\"SubscribeToParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n\n    // Read data from the stream.\n    connection.on(\"ParameterDataReceived\", data =&gt; {\n        let model = JSON.stringify(data);\n        console.log(\"Received data from stream: \" + model);\n\n        // Unsubscribe from stream.\n        connection.invoke(\"UnsubscribeFromParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n    });\n});\n</code></pre> <p>Following is a list of subscriptions available for SignalR clients.</p> <ul> <li> <p><code>SubscribeToParameter(topicName, streamId, parameterId)</code>: Subscribes     to a parameter data stream. Use <code>UnsubscribeFromParameter(topicname,     streamId, parameterId)</code> to unsubscribe.</p> </li> <li> <p><code>SubscribeToParameterDefinitions(topicName, streamId)</code>: Subscribes     to parameter definition updates.</p> </li> <li> <p><code>SubscribeToEvent(topicName, streamId, eventId)</code>: Subscribes to an     event data stream. Use <code>UnsubscribeFromEvent(topicName, streamId,     eventId)</code> to unsubscribe.</p> </li> <li> <p><code>SubscribeToEventDefinitions(topicName, streamId)</code>: Subscribes to     event definition updates.</p> </li> <li> <p><code>UnsubscribeFromStream(topicName, streamId)</code>: Unsubscribes from all     subscriptions of the specified stream.</p> </li> </ul> <p>Following is a list of SignalR events supported by Quix and their payloads.</p> <ul> <li><code>ParameterDataReceived</code>: Add a listener to this event to receive     parameter data from a stream. Following is a sample payload for this     event.</li> </ul>  <pre><code>{\n  topicName: 'topic-1',\n  streamId: 'b45969d2-4624-4ab7-9779-c8f90ce79420',\n  timestamps: [ 1591733989000000000, 1591733990000000000, 1591733991000000000 ],\n  numericValues: { ParameterA: [ 1, 2, 3 ] },\n  stringValues: {},\n  tagValues: { ParameterA: [ null, null, 'tag-1' ] }\n}\n</code></pre> <ul> <li><code>ParameterDefinitionsUpdated</code>: Add a listener to this event to     receive data from <code>SubscribeToParameterDefinitions</code> subscription.     Following is a sample payload of this event.</li> </ul>  <pre><code>{\n    topicName: 'topic-1',\n    streamId: 'b45969d2-4624-4ab7-9779-c8f90ce79420',\n    definitions: [\n        {\n            Id: 'ParameterA',\n            Name: 'Parameter A',\n            Description: 'Description of parameter A',\n            MinimumValue: null,\n            MaximumValue: 100.0,\n            Unit: 'kmh',\n            CustomProperties: null,\n            Localtion: '/car/general'\n        }\n    ]\n}\n</code></pre> <ul> <li><code>EventDataReceived</code>: Add a listener to this event to receive data     from <code>SubscribeToEvent</code> subscription. Following is a sample payload     of this event.</li> </ul>  <pre><code>{\n    topicName: 'topic-1',\n    streamId: 'b45969d2-4624-4ab7-9779-c8f90ce79420'\n    id: 'EventA',\n    timestamp: 1591733990000000000,\n    value: 'val-a',\n    tags: {\n        tag1: 'val1'\n    }\n}\n</code></pre>","location":"platform/how-to/webapps/read/#reading-data-from-a-stream"},{"title":"Write to Quix with NodeJs","text":"<p>Clients write data to Quix using streams opened on existing topics. Therefore, you need to first create a topic in the Portal to hold your data streams.</p> <p>Once you have a topic, your clients can start writing data to Quix by</p> <ul> <li> <p>creating a stream in your topic</p> </li> <li> <p>sending data to that stream</p> </li> <li> <p>closing the stream</p> </li> </ul>","location":"platform/how-to/webapps/write/"},{"title":"Creating a stream","text":"<p>To write data to Quix, you need to open a stream to your topic. Following is an example of creating a stream using JavaScript and Node.js.</p> <pre><code>const https = require(https);\n\nconst data = JSON.stringify({\n    Name: \"Your Stream Name\",\n    Location: \"your/location\",\n    Metadata: {\n        Property1: \"Value 1\",\n        Property2: \"Value 2\"\n    },\n    Parents = [\"parent-stream-1\", \"parent-stream-2\"],\n    TimeOfRecording: \"2021-02-06T00:15:15Z\"\n});\n\nconst options = {\n    hostname: 'your-workspace-id.portal.quix.ai',\n    path: '/topics/your-topic-name/streams',\n    method: 'POST',\n    headers: {\n        'Authorization': 'Bearer your_access_token',\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options, res =&gt; {\n    res.on('data', d =&gt; {\n        let json = JSON.parse(d);\n        let streamId = json.streamId;\n    });\n});\n\nreq.write(data);\nreq.end();\n</code></pre> <p>Upon completing the request successfully, you will receive the stream id in the response body. You are going to need this stream id when you are writing data to the stream.</p> <p>In the request data, <code>Location</code> is also an optional, but an important property. Location allows you to organise your streams under directories in the Data Catalogue.</p> <p>When you are creating the stream, you can add optional metadata about the stream to the stream definition like <code>Property1</code> and <code>Property2</code> in the preceding example.</p> <p>Field <code>Parents</code> is also optional. If the current stream is derived from one or more streams (e.g. by transforming data from one stream using an analytics model), you can reference the original streams using this field.</p> <p><code>TimeOfRecording</code> is an optional field that allows you to specify the actual time the data was recorded. This field is useful if you are streaming data that was recorded in the past.</p>","location":"platform/how-to/webapps/write/#creating-a-stream"},{"title":"Writing parameter data to a stream","text":"<p>After you have created the stream, you can start writing data to that stream using the following HTTP request.</p> <pre><code>const https = require(https);\n\nconst data = JSON.stringify({\n    Timestamps: [1591733989000000000, 1591733990000000000, 1591733991000000000],\n    NumericValues: {\n        \"ParameterA\": [1, 2, 3],\n        \"ParameterB\": [5, 8, 9]\n    },\n    StringValues: {\n        \"ParameterC\": [\"hello\", \"world\", \"!\"]\n    },\n    BinaryValues: {\n        \"ParameterD\": [\n            Buffer.from(\"hello\").toString('base64'),\n            Buffer.from(\" Quix\").toString('base64'),\n            Buffer.from(\"!\").toString('base64')\n        ]\n    }\n    TagValues: {\n        \"ParameterA\": [null, null, \"tag-1\"]\n    }\n});\n\nconst options = {\n    hostname: 'your-workspace-id.portal.quix.ai',\n    path: '/topics/your-topic-name/streams/your-stream-id/parameters/data',\n    method: 'POST',\n    headers: {\n        'Authorization': 'Bearer your_access_token',\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options, res =&gt; {\n    console.log(`Status Code: ${res.statusCode}`);\n});\n\nreq.write(data);\nreq.end();\n</code></pre> <p>In the preceding example, <code>data</code> has two different parameter types, numeric and strings. If your data only contains numeric data, you do not need to include the <code>StringValues</code> property. In the case of binary values, the items in the array must be a base64 encoded string.</p> <p><code>TagValues</code> is another optional field in the data request that allows you to add context to data points by means of tagging them. Index of the <code>Timestamps</code> array is used when matching the parameter data values as well as tag values. Therefore, the order of the arrays is important.</p>","location":"platform/how-to/webapps/write/#writing-parameter-data-to-a-stream"},{"title":"Defining parameters","text":"<p>In the above examples, parameters are created in Quix as you write data to the stream. However, what if you would like to add more information like acceptable value ranges, measurement units, etc. to your parameters? You can use the following HTTP request to update your parameter definitions.</p> <pre><code>const https = require(https);\n\nconst data = JSON.stringify([\n    {\n        Id: \"ParameterA\",\n        Name: \"Parameter A\",\n        Description: \"Temperature measurements from unit 1234A\",\n        MinimumValue: 0.0,\n        MaximumValue: 100.0,\n        Unit: \"\u00b0C\",\n        CustomProperties: \"{\\\"OptimalMinimum\\\": 30.0, \\\"OptimalMaximum\\\": 50.0}\",\n        Location: \"/chassis/engine\"\n    }\n]);\n\nconst options = {\n    hostname: 'your-workspace-id.portal.quix.ai',\n    path: '/topics/your-topic-name/streams/your-stream-id/parameters',\n    method: 'PUT',\n    headers: {\n        'Authorization': 'Bearer your_access_token',\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options, res =&gt; {\n    console.log(`Status Code: ${res.statusCode}`);\n});\n\nreq.write(data);\nreq.end();\n</code></pre> <p>In the preceding request, the <code>Id</code> must match the parameter id you set when writing data to the stream. <code>Name</code> allows you to set a more readable name for the parameter. You can also add a description, minimum and maximum values, unit of measurement to your parameter. <code>Location</code> allows you to organise/group your parameters in a hierarchical manner like with the streams. If you have a custom parameter definition that is not covered by the primary fields of the request, you can use <code>CustomProperties</code> field to add your custom definition as a string.</p>","location":"platform/how-to/webapps/write/#defining-parameters"},{"title":"Writing event data to a stream","text":"<p>Writing event data to a stream is similar to writing parameter data using the web api. The main difference in the two requests is in the request body.</p> <pre><code>const data = JSON.stringify([\n    {\n        Id: \"EventA\",\n        Timestamp: 1591733989000000000,\n        Value: \"Lap1\",\n        Tags: {\n            TagA: \"val1\",\n            TagB: \"val2\"\n        }\n    },\n    {\n        Id: \"EventA\",\n        Timestamp: 1591734989000000000,\n        Value: \"Lap2\",\n        Tags: {\n            TagA: \"val1\",\n            TagB: \"val2\"\n        }\n    },\n    {\n        Id: \"EventA\",\n        Timestamp: 1591735989000000000,\n        Value: \"Lap3\",\n        Tags: {\n            TagA: \"val1\"\n        }\n    },\n]);\n\nconst options = {\n    hostname: 'your-workspace-id.portal.quix.ai',\n    path: '/topics/your-topic-name/streams/your-stream-id/events/data',\n    method: 'POST',\n    headers: {\n        'Authorization': 'Bearer your_access_token',\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options, res =&gt; {\n    console.log(`Status Code: ${res.statusCode}`);\n});\n\nreq.write(data);\nreq.end();\n</code></pre> <p>In the preceding example, tags in the event data request are optional. Tags add context to your data points and help you to execute efficient queries over them on your data like using indexes in traditional databases.</p>","location":"platform/how-to/webapps/write/#writing-event-data-to-a-stream"},{"title":"Defining events","text":"<p>In the above examples, events are created in Quix as you write data to the stream. If you want to add more descriptions to your events, you can use event definitions api similar to parameter definitions to update your events.</p> <pre><code>const https = require(https);\n\nconst data = JSON.stringify([\n    {\n        Id: \"EventA\",\n        Name: \"Event A\",\n        Description: \"New lap event\",\n        CustomProperties: \"{\\\"Tarmac\\\": \\\"Open-graded\\\"}\",\n        Location: \"/drive/lap\",\n        Level: \"Information\"\n    }\n]);\n\nconst options = {\n    hostname: 'your-workspace-id.portal.quix.ai',\n    path: '/topics/your-topic-name/streams/your-stream-id/events',\n    method: 'PUT',\n    headers: {\n        'Authorization': 'Bearer your_access_token',\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options, res =&gt; {\n    console.log(`Status Code: ${res.statusCode}`);\n});\n\nreq.write(data);\nreq.end();\n</code></pre> <p>In the preceding request, the <code>Id</code> must match the event id you set when writing events to the stream. <code>Name</code> allows you to set a more readable name for the event. <code>Location</code> allows you to organise/group your events in a hierarchy like with the parameters. If you have a custom event definition that is not covered by the primary fields of the request, you can use <code>CustomProperties</code> field to add your custom definition as a string. You can also set an optional event <code>Level</code>. Accepted event levels are Trace, Debug, Information, Warning, Error and Critical. Event level defaults to Information if not specified.</p>","location":"platform/how-to/webapps/write/#defining-events"},{"title":"Closing a stream","text":"<p>After finishing sending data, you can proceed to close the stream using the request below.</p> <pre><code>const https = require(https);\n\nconst options = {\n    hostname: 'your-workspace-id.portal.quix.ai',\n    path: '/topics/your-topic-name/streams/your-stream-id/close',\n    method: 'POST',\n    headers: {\n        'Authorization': 'Bearer your_access_token',\n        'Content-Type': 'application/json'\n    }\n};\n\nconst req = https.request(options, res =&gt; {\n    console.log(`Status Code: ${res.statusCode}`);\n});\n\nreq.end();\n</code></pre>","location":"platform/how-to/webapps/write/#closing-a-stream"},{"title":"Code Samples","text":"<p>The Quix Portal includes a Library of templates and sample projects that you can use to start working with the platform.</p> <p>Quix allows explore the samples and save them as a new Project and immidiatelly Run or Deploy them. If you don\u2019t have a Quix account yet, go sign-up to Quix and create one.</p> <p>The backend of the Quix Library is handled by a public Open source repository on GitHub, so you can become a contributor of our Library generating new samples or updating existing ones.</p> <p></p>","location":"platform/samples/samples/"},{"title":"Security","text":"<p>This section describes the basic security features of Quix.</p>","location":"platform/security/security/"},{"title":"Data in flight","text":"","location":"platform/security/security/#data-in-flight"},{"title":"Authentication","text":"<ul> <li> <p>Our APIs are authenticated using     OAuth 2.0 token. We     are using Auth0     as our provider.</p> </li> <li> <p>Each Kafka server is authenticated using certificate, which is     provided for each project created and can also be downloaded from     topics view. The client is authenticated using SASL (username,     password).</p> </li> </ul>","location":"platform/security/security/#authentication"},{"title":"Authorization","text":"<ul> <li> <p>The APIs is using RBAC. You are limited in what you can do based on     your token and the role configured for your user.</p> </li> <li> <p>Each kafka client is authrozied to only read and write to the topics     or query consumer group information regarding topics owned by the     organisation the client belongs to.</p> </li> </ul>","location":"platform/security/security/#authorization"},{"title":"Encryption","text":"<ul> <li>All our APIs communicate with TLS 1.2</li> </ul>","location":"platform/security/security/#encryption"},{"title":"Data at rest","text":"<ul> <li> <p>Your data is encrypted at rest using cloud provider (Azure) managed     keys.</p> </li> <li> <p>Your data is phyisically protected at our cloud provider\u2019s location.</p> </li> </ul>","location":"platform/security/security/#data-at-rest"},{"title":"Troubleshooting","text":"<p>This section contains solutions, fixes, hints and tips to help you solve the most common issues encountered when using Quix.</p>","location":"platform/troubleshooting/troubleshooting/"},{"title":"Data is not being received into a Topic","text":"<ul> <li> <p>Ensure the Topic Name or Id is correct in Topics option of Quix     Portal.</p> </li> <li> <p>You can check the data in / out rates on the Topics tab.</p> </li> <li> <p>If you want to see the data in the Data Catalogue please make sure     you are persisting the data to the Topic otherwise it may appear     that there is no data.</p> </li> <li> <p>If you are using a consumer group, check that no other services are     using the same group. If you run your code locally and deployed     somewhere and they are both using the same consumer group one of     them may consume all of the data.</p> </li> </ul>","location":"platform/troubleshooting/troubleshooting/#data-is-not-being-received-into-a-topic"},{"title":"Topic Authentication Error","text":"<p>If you see errors like these in your service or job logs then you may have used the wrong credentials or it could be that you have specified the wrong Topic Id.</p> <p>Authentication failed during authentication due to invalid credentials with SASL mechanism SCRAM-SHA-256\\&lt;/programlisting&gt; Exception receiving package from Kafka\\&lt;/programlisting&gt; 3/3 brokers are down\\&lt;/programlisting&gt; Broker: Topic authorization failed\\&lt;/programlisting&gt;</p> <p>Check very carefully each of the details.</p> <p>The following must be correct:</p> <ul> <li> <p>TopicId or TopicName</p> </li> <li> <p>Sdk Token</p> </li> </ul> <p>These can all be found in Topics option of Quix Portal.</p>","location":"platform/troubleshooting/troubleshooting/#topic-authentication-error"},{"title":"Broker Transport Failure","text":"<p>If you have deployed a service or job and the logs mention broker transport failure then check the workspace name and password in the SecurityOptions.</p> <p>Also check the broker address list. You should have these by default:</p> <p>kafka-k1.quix.ai:9093,kafka-k2.quix.ai:9093,kafka-k3.quix.ai:9093\\&lt;/programlisting&gt;</p>","location":"platform/troubleshooting/troubleshooting/#broker-transport-failure"},{"title":"401 Error","text":"<p>When attempting to access the web API\u2019s you may encounter a 401 error. Check that the bearer token is correct and has not expired. If necessary generate a new bearer token.</p> <p>Example of the error received when trying to connect to the Streaming Reader API with an expired bearer token</p> <p>signalrcore.hub.errors.UnAuthorizedHubError\\&lt;/programlisting&gt;</p> <p>The API\u2019s that require a valid bearer token are:</p> <ol> <li> <p>Portal API</p> <ul> <li>https://portal-api.platform.quix.ai/swagger/index.html</li> </ul> </li> <li> <p>Streaming Writer API</p> <ul> <li>https://writer-[YOUR_ORGANISATION_ID]-[YOUR_WORKSPACE_ID].platform.quix.ai/index.html</li> </ul> </li> <li> <p>Telemetry Query API</p> <ul> <li>https://telemetry-query-[YOUR_ORGANISATION_ID]-[YOUR_WORKSPACE_ID].platform.quix.ai/swagger/index.html</li> </ul> </li> </ol>","location":"platform/troubleshooting/troubleshooting/#401-error"},{"title":"Error Handling in the SDK callbacks","text":"<p>Errors generated in the SDK callback can be swallowed or hard to read. To prevent this and make it easier to determine the root cause you should use a traceback</p> <p>Begin by importing traceback</p> <pre><code>import traceback\n</code></pre> <p>Then, inside the SDK callback where you might have an issue place code similar to this:</p> <pre><code>def read_stream(new_stream: StreamReader):\n\n    def on_parameter_data_handler(data: ParameterData):\n        try:\n            data.timestamps[19191919] # this does not exist\n\n        except Exception:\n            print(traceback.format_exc())\n\n    new_stream.parameters.create_buffer().on_read += on_parameter_data_handler\n\ninput_topic.on_stream_received += read_stream\n</code></pre> <p>Notice that the try clause is within the handler and the except clause prints a formatted exception (below)</p> <pre><code>Traceback (most recent call last):\n  File \"main.py\", line 20, in on_parameter_data_handler\n    data.timestamps[19191919]\n  File \"/usr/local/lib/python3.8/dist-packages/quixstreaming/models/netlist.py\", line 22, in __getitem__\n    item = self.__wrapped[key]\nIndexError: list index out of range\n</code></pre>","location":"platform/troubleshooting/troubleshooting/#error-handling-in-the-sdk-callbacks"},{"title":"Service keeps failing and restarting","text":"<p>If your service continually fails and restarts you will not be able to view the logs. Redeploy your service as a job instead. This will allow you to inspect the logs and get a better idea about what is happening.</p>","location":"platform/troubleshooting/troubleshooting/#service-keeps-failing-and-restarting"},{"title":"Possible DNS Propagation Errors","text":"<p>There are currently 2 scenarios in which you might encounter an issue caused by DNS propagation.</p> <ul> <li>1. Data catalogue has been deployed but DNS entries have not fully     propagated. In this scenario you might see a banner when accessing     the data catalogue.</li> </ul> <p></p> <ul> <li>2. A dashboard or other publicly visible deployment is not yet     accessible, again due to DNS propagation.</li> </ul> <p></p>  <p>Tip</p> <p>In these scenarios simply wait while the DNS records propagate. It can take up to 10 minutes for DNS to records to propagate fully.</p>","location":"platform/troubleshooting/troubleshooting/#possible-dns-propagation-errors"},{"title":"Python Version","text":"<p>If you get strange errors when trying to compile your Python code locally please check that you are using Python version 3.8</p> <p>For example you may encounter a ModuleNotFoundError</p> <pre><code>ModuleNotFoundError: No module named 'quixstreaming'\n</code></pre> <p>For information on how to setup your IDE for working with Quix please check out this section on the SDK documentation.</p>","location":"platform/troubleshooting/troubleshooting/#python-version"},{"title":"Jupyter Notebooks","text":"<p>If you are having trouble with Jupyter Notebooks or another consumer of Quix data try using aggregation to reduce the number of records returned.</p> <p>For more info on aggregation check out this short video.</p>","location":"platform/troubleshooting/troubleshooting/#jupyter-notebooks"},{"title":"Process Killed or Out of memory","text":"<p>If your deployment\u2019s logs report \"Killed\" or \"Out of memory\" then you may need to increase the amount of memory assgned to the deployment.</p> <p>You may experience this:</p> <ul> <li> <p>At build time if you want to load large third party packages into     your code</p> </li> <li> <p>At runtime if you are storing large datasets in memory.</p> </li> </ul>","location":"platform/troubleshooting/troubleshooting/#process-killed-or-out-of-memory"},{"title":"Missing Dependency in online IDE","text":"<p>Currently the online IDE does not use the same docker image as the one used for deployment due to time it would take to build it and make it available to you. (Likely feature for future however) Because of this you might have some OS level dependencies that you need to install from within your python code to be able to make use of the Run feature in the IDE. The section below should give you guidance how to achieve this.</p> <p>In your <code>main.py</code> (or similar) file, add as the first line: <code>import preinstall</code>. Now create the file <code>preinstall.py</code> and add content based on example below:</p> <ul> <li> <p>TA-Lib     This script will check if TA-Lib is already installed (like from     docker deployment). If not then installs it.</p> <pre><code>import os\nimport sys\n\nta_lib_pip_details = os.system(\"python3 -m pip show TA-Lib\")\nif ta_lib_pip_details == 0:\n    print(\"TA-Lib already installed\")\nelse:\n    if os.system(\"apt-get update\") != 0:\n        print(\"Failed apt-get update\")\n        sys.exit(1)\n    if os.popen(\"if [ -e ta-lib-0.4.0-src.tar.gz ]; then echo \\\"ok\\\"; else echo \\\"nok\\\"; fi\").read().strip() == \"ok\":\n        print(\"TA-Lib already downloaded\")\n    else:\n        print(\"Downloading ta-lib\")\n        if os.system(\"apt-get install curl -y\") != 0:\n            print(\"Failed apt-get install curl -y\")\n            sys.exit(1)\n        if os.system(\"curl https://jztkft.dl.sourceforge.net/project/ta-lib/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz -O\") != 0:\n            print(\"Failed to download ta-lib\")\n            sys.exit(1)\n\n    zipmdsum = os.popen(\"md5sum ta-lib-0.4.0-src.tar.gz | cut -d ' ' -f 1\").read().strip()\n    if zipmdsum == \"308e53b9644213fc29262f36b9d3d9b9\":\n        print(\"TA-Lib validated\")\n    else:\n        print(\"TA-Lib has incorrect hash value, can't trust it. Found hash: '\" + str(zipmdsum) + \"'\")\n        sys.exit(1)\n\n    if os.system(\"tar -xzf ta-lib-0.4.0-src.tar.gz\") != 0:\n        print(\"Failed to extract TA-Lib zip\")\n        sys.exit(1)\n\n    if os.system(\"apt-get install build-essential -y\") != 0:\n        print(\"Failed apt-get install build-essential -y\")\n        sys.exit(1)\n\n    os.chdir(os.path.abspath(\".\") + \"/ta-lib\")\n\n    if os.system(\"./configure --prefix=/usr\") != 0:\n        print(\"Failed to configure TA-Lib for build\")\n        sys.exit(1)\n\n    if os.system(\"make\") != 0:\n        print(\"Failed to make TA-Lib\")\n        sys.exit(1)\n\n    if os.system(\"make install\") != 0:\n        print(\"Failed to make install TA-Lib\")\n        sys.exit(1)\n\n    print(\"Installed dependencies for TA-Lib pip package\")\n\n    if os.system(\"python3 -m pip install TA-Lib\") != 0:\n        print(\"Failed to pip install TA-Lib\")\n        sys.exit(1)\n\n    print(\"Installed TA-Lib pip package\")\n</code></pre> </li> </ul> <p>With this, the first time you press Run, the dependency should install. Any subsequent run should already work without having to install.</p>","location":"platform/troubleshooting/troubleshooting/#missing-dependency-in-online-ide"},{"title":"Image Processing","text":"<p>We show you how to build a video processing pipeline using the Transport for London (TfL) traffic cameras, known as Jam Cams, and a YOLO v3 model. We provide a fully functional UI to show you where the recognized objects are located around London.</p> <p></p>","location":"platform/tutorials/ImageProcessing/"},{"title":"Get Started","text":"<p>To get started make sure you have a Quix account: signup for a completely free account at https://quix.io/</p> <p>You\u2019ll also need a free TfL account which you can register for here: https://api-portal.tfl.gov.uk/ A rough guide to finding your TfL API key is as follows:</p> <ul> <li> <p>Register for an account.</p> </li> <li> <p>Login and click the \"Products\" menu item.</p> </li> <li> <p>You should have 1 product to choose from \"500 Requests per min.\"</p> </li> <li> <p>Click \"500 Requests per min.\"</p> </li> <li> <p>Enter a name for your subscription into the box, e.g. QuixFeed, and     click \"Register.\"</p> </li> <li> <p>You can now find your API Keys in the profile page.</p> </li> </ul> <p>If you need any assistance while following the tutorial, we\u2019re here to help in The Stream, our free Slack community.</p>","location":"platform/tutorials/ImageProcessing/#get-started"},{"title":"Library","text":"<p>Most of the code you need has already been written for you and it\u2019s located in our library. We\u2019ll be referring to the library often so make sure you know where it is: the left hand side menu bar.</p>","location":"platform/tutorials/ImageProcessing/#library"},{"title":"Pipeline Services","text":"<p>There are 5 stages to the processing pipeline you are about to build.</p>","location":"platform/tutorials/ImageProcessing/#pipeline-services"},{"title":"Video Feeds","text":"<ul> <li> <p>TfL Camera feed or \u201cJam Cams\u201d</p> </li> <li> <p>Webcam image capture</p> </li> </ul>","location":"platform/tutorials/ImageProcessing/#video-feeds"},{"title":"Pre Processing","text":"<ul> <li> <p>Frame extraction</p> </li> <li> <p>Decoding</p> </li> </ul>","location":"platform/tutorials/ImageProcessing/#pre-processing"},{"title":"Object Detection","text":"<ul> <li>Detect objects within images</li> </ul>","location":"platform/tutorials/ImageProcessing/#object-detection"},{"title":"Stream Merge","text":"<ul> <li>Merge images from all of the individual TfL cameras into one stream</li> </ul>","location":"platform/tutorials/ImageProcessing/#stream-merge"},{"title":"Web UI","text":"<ul> <li> <p>A simple UI showing:</p> <ul> <li> <p>Images with identified objects</p> </li> <li> <p>Map with count of objects at each cameras location</p> </li> </ul> </li> </ul>","location":"platform/tutorials/ImageProcessing/#web-ui"},{"title":"Built It","text":"<p>Now you know which components will be needed in the image processing pipeline, let\u2019s create and deploy each service.</p>","location":"platform/tutorials/ImageProcessing/#built-it"},{"title":"1. Video Feeds","text":"<p>Follow these steps to deploy the traffic camera feed service.</p> <ol> <li> <p>Navigate to the Library and locate \u201cTfL Camera Feed\u201d.</p> </li> <li> <p>Click \u201cSetup &amp; deploy\u201d</p> </li> <li> <p>Paste your TfL API Key into the appropriate input</p> </li> <li> <p>Click \u201cDeploy\u201d</p> </li> </ol>  <p>Deploying will start the service in our pre-provisioned infrastructure.</p> <p>This service will stream data from the TfL cameras to the \u201ctfl-cameras\u201d topic.</p>  <p>Once deployed:</p> <ol> <li>Stop the service</li> </ol>  <p>You will restart it later, but for now it can be stopped.</p>  <p>INFO: At this point you should be looking at your pipeline view and have one service deployed. When it has started the arrow pointing out of the service will be green. This indicates that data is flowing out of the service into a topic. Now, we need to deploy something to consume the data streaming into that topic.</p> <p>Follow these steps to deploy the webcam service.</p> <ol> <li> <p>Navigate to the Library and locate \u201cWebcam image capture service\u201d.</p> </li> <li> <p>Click \u201cSetup &amp; deploy\u201d</p> </li> <li> <p>Click \u201cDeploy\u201d</p> </li> </ol>  <p>This service will stream data from your webcam to the \u201cimage-base64\u201d topic.</p>  <ol> <li>Click the service tile</li> </ol> <p></p> <ol> <li>Click the \u201cPublic URL\u201d</li> </ol>  <p>This opens the deployed website which uses your webcam to stream images to Quix</p>   <p>Note</p> <p>Your browser may prompt you to allow access to your webcam.</p>","location":"platform/tutorials/ImageProcessing/#1-video-feeds"},{"title":"2. Pre Processing","text":"<p>Follow these steps to deploy the frame extraction service.</p> <ol> <li> <p>Navigate to the Library and locate \u201cTfL traffic camera frame     grabber\u201d.</p> </li> <li> <p>Click \u201cSetup &amp; deploy\u201d</p> </li> <li> <p>Click Deploy</p> </li> </ol>  <p>This service receives data from the \u201ctfl-cameras\u201d topic and streams data to the \u201cimage-raw\u201d topic.</p>  <p>Follow these steps to deploy the decoder service.</p> <ol> <li> <p>Navigate to the Library and locate \u201cBase64 Decoder\u201d.</p> </li> <li> <p>Click \u201cSetup &amp; deploy\u201d</p> </li> <li> <p>Click \u201cDeploy\u201d</p> </li> </ol>  <p>This service receives data from the \u201cimage-base64\u201d topic and streams data to the \u201cimage-raw\u201d topic.</p>","location":"platform/tutorials/ImageProcessing/#2-pre-processing"},{"title":"3. Computer Vision","text":"<p>Follow these steps to deploy the object detection service.</p> <ol> <li> <p>Navigate to the Library and locate \u201cComputer Vision object     detection\u201d.</p> </li> <li> <p>Click \u201cSetup &amp; deploy\u201d</p> </li> <li> <p>Click \u201cDeploy\u201d</p> </li> </ol>  <p>This service receives data from the \u201cimage-raw\u201d topic and streams data to the \u201cimage-processed\u201d topic.</p>","location":"platform/tutorials/ImageProcessing/#3-computer-vision"},{"title":"4. Stream Merge","text":"<p>Follow these steps to deploy the object detection service.</p> <ol> <li> <p>Navigate to the Library and locate \u201cStream merge\u201d.</p> </li> <li> <p>Click \u201cSetup &amp; deploy\u201d</p> </li> <li> <p>Click \u201cDeploy\u201d</p> </li> </ol>  <p>This service receives data from the \u201cimage-processed\u201d topic and streams data to the \u201cimage-processed-merged\u201d topic.</p>","location":"platform/tutorials/ImageProcessing/#4-stream-merge"},{"title":"5. Web UI","text":"<p>Follow these steps to deploy the object detection service.</p> <ol> <li> <p>Navigate to the Library and locate \u201cTFL image processing UI\u201d.</p> </li> <li> <p>Click \u201cEdit code\u201d</p> </li> <li> <p>Click \u201cSave as project\u201d</p> </li> </ol>  <p>The code for this Angular UI is now saved to your workspace</p>  <ol> <li>Find and select the <code>environment-variables.service.ts</code> file.</li> </ol>  <p></p>  <p>You need to provide the \u201cToken\u201d that this website will need to communicate with Quix.</p> <ol> <li> <p>Click the user avatar icon in the top right corner of the Quix     portal</p> </li> <li> <p>Click Tokens</p> </li> <li> <p>Generate a token</p> <ol> <li>Give it a name and expiry date</li> </ol> </li> <li> <p>Copy the token</p> </li> <li> <p>Click back on your browser and relocate the     <code>environment-variables.service.ts</code> file.</p> </li> <li> <p>Paste the token into the double quotes as indicated in the file</p> </li> </ol>  <pre><code>Token: string = \"your token here\";\n</code></pre> <p>Tag the code and deploy the UI</p> <ol> <li> <p>Click the +tag button at the top of the code file</p> </li> <li> <p>Enter v1 and press enter</p> </li> <li> <p>Click Deploy near the top right corner</p> </li> <li> <p>Select v1 under the \u201cVersion Tag\u201d</p> </li> </ol> <p>This is the tag you created in step 2</p> <ol> <li>Click \u201cService\u201d in \u201cDeployment Settings\u201d</li> </ol> <p>This ensures the service runs continuously</p> <ol> <li>Click the toggle in \u201cPublic Access\u201d</li> </ol>  <p>This enables access from anywhere on the internet</p>  <ol> <li>Click \u201cDeploy\u201d</li> </ol>  <p>The UI receives data from the \u201cimage-processed-merged\u201d topic</p>  <ol> <li> <p>Once deployed, click the service tile</p> </li> <li> <p>Click the \u201cPublic URL\u201d</p> </li> </ol>  <p>This is the user interface for the demo. This screenshot shows the last image processed from one of the TfL traffic cameras as well as the map with a count of all the cars seen so far and where they were seen.</p>  <p></p>","location":"platform/tutorials/ImageProcessing/#5-web-ui"},{"title":"Next Steps","text":"<p>You\u2019ve just made extensive use of the Quix library, our collection of open source connectors, samples and examples. Now you can build your own connectors and apps and contribute by going to our Github here, forking our library repo and submitting your code, updates and ideas.</p> <p>What will you build, let us know. We\u2019d love to feature your project or use case in our newsletter.</p> <p>If you need any assistance, we\u2019re here to help in The Stream, our free Slack community.</p>","location":"platform/tutorials/ImageProcessing/#next-steps"},{"title":"That\u2019s all folks!","text":"<p>We hope you enjoyed this tutorial on how to deploy a real time image processing pipeline in just a few minutes.</p> <p>If you have any questions or feedback please contact us on The Stream.</p> <p>Thank you and goodbye!</p>","location":"platform/tutorials/ImageProcessing/#thats-all-folks"},{"title":"RSS processing pipeline","text":"<p>This tutorial explains how to build a pipeline that gathers and processes data from an RSS feed and alerts users when specific criteria are met. It\u2019s a companion to a live coding session,window=_blank on YouTube, where we sourced data from a StackOverflow RSS feed for use in a Python service.</p> <p>This tutorial has three parts</p> <ul> <li> <p>Sourcing data</p> </li> <li> <p>Processing data</p> </li> <li> <p>Sending alerts</p> </li> </ul> <p>What you need</p> <ul> <li> <p>A free Quix account,window=_blank. It     comes with enough credits to create this project.</p> </li> <li> <p>A Slack account with access to create a webhook. (This     guide,window=_blank can     help you with this step.)</p> </li> </ul>","location":"platform/tutorials/RSSProcessingPipeline/"},{"title":"Sourcing data","text":"","location":"platform/tutorials/RSSProcessingPipeline/#sourcing-data"},{"title":"1. Get the \u201cRSS Data Source\u201d connector","text":"<p>In your Quix account, go to the library and search for \u201cRSS Data Source.\u201d (Hint: you can watch Steve prepare this code in the video tutorial if you\u2019re like to learn more about it.)</p> <p>Click \u201cSetup &amp; deploy\u201d on the \u201cRSS Data Source\u201d library item. (The card has a blue line across its top that indicates it\u2019s a source connector.)</p> <p></p>","location":"platform/tutorials/RSSProcessingPipeline/#1-get-the-rss-data-source-connector"},{"title":"2. Configure the connector","text":"<p>In the configuration panel, keep the default name and output topic. Enter the following URL into the rss_url field: https://stackoverflow.com/feeds/tag/python,window=_blank</p> <p>Click \u201cDeploy\u201d and wait a few seconds for the pre-built connector to be deployed to your workspace.</p> <p>You will then begin to receive data from the RSS feed. The data then goes into the configured output topic. Don\u2019t worry, you won\u2019t lose data. It\u2019s cached in the topic until another deployment starts reading it.</p>","location":"platform/tutorials/RSSProcessingPipeline/#2-configure-the-connector"},{"title":"Processing data","text":"<p>You can do anything you want in the processing phase of a pipeline. You might want to merge several input streams or make decisions on your data. In this tutorial, you\u2019ll filter and augment data so that only questions with certain tags get delivered to you.</p>","location":"platform/tutorials/RSSProcessingPipeline/#processing-data"},{"title":"1. Get the \u201cRSS Data Filtering\u201d connector","text":"<p>Return to the library tab in Quix and search for \u201cRSS Data Filtering.\u201d Click \u201cSetup &amp; deploy\u201d on the card.</p> <p>If you created a new workspace for this project, the fields automatically populate. If you\u2019re using the workspace for other projects, you may need to specify the input topic as \u201crss-data.\u201d</p> <p>You might also want to customize the tag_filter. It is automatically populated with a wide range of tags related to Python. This works well for this demo, because you\u2019ll see a large return of interesting posts. But you can decrease or add tags.</p>","location":"platform/tutorials/RSSProcessingPipeline/#1-get-the-rss-data-filtering-connector"},{"title":"2. Deploy \u201cRSS Data Filtering\u201d connector","text":"<p>Click \u201cDeploy\u201d on the \u201cRSS Data Filtering\u201d connector. Once deployed, the connector will begin processing the data that\u2019s been building up in the rss-data topic.</p> <p>Have a look in the logs by clicking the Data Filtering Model tile (pink outlined) on the workspace home page.</p> <p></p> <p>The transformation stage is now complete. Your project is now sending the filtered and enhanced data to the output topic.</p>","location":"platform/tutorials/RSSProcessingPipeline/#2-deploy-rss-data-filtering-connector"},{"title":"Sending alerts","text":"<p>Last in our pipeline is the destination for our RSS data. This demo uses a Slack channel as its destination.</p>","location":"platform/tutorials/RSSProcessingPipeline/#sending-alerts"},{"title":"1. Get the \u201cSlack Notification\u201d connector","text":"<p>Return to the Quix library and search for the \u201cSlack Notification.\u201d Click \u201cPreview code.\u201d You\u2019re going to modify the standard code before deploying this connector.</p> <p></p> <p>Click \u201cNext\u201d on the dialog box. Ensure \u201cfiltered-rss-data\u201d is selected as the input topic and provide a Slack \u201cwebhook_url.\u201d</p>  <p>Note</p> <p>If you have your own slack, head over to the Slack API pages,window=_blank and create a webhook following their guide \u201cGetting started with Incoming Webhooks.\u201d If you don\u2019t have your own Slack or don\u2019t have the account privileges to create the webhook, you can choose another destination from the library, such as Twilio.</p>  <p>Warning: Use a dev or demo or unimportant Slack channel while you\u2019re developing this. Trust me.</p>","location":"platform/tutorials/RSSProcessingPipeline/#1-get-the-slack-notification-connector"},{"title":"2. Modify and deploy the \u201cSlack Notification\u201d connector","text":"<p>Enter your webhook into the webhook_url field. Click \u201cSave as project.\u201d This will save the code to your workspace, which is a GitLab repository.</p> <p>Once saved, you\u2019ll see the code again. The quix_function.py file should be open. This is what you\u2019ll alter. The default code dumps everything in the parameter data and event data to the Slack channel. It\u2019ll do to get you up and going, but we want something more refined. \ud83d\ude09</p> <p>Go to our GitHub library of tutorial code here,window=_blank. The code picks out several field values from the parameter data and combines them to form the desired Slack alert.</p> <p>Copy the code and paste it over the quix_function.py file in your project in the Quix portal.</p> <p>Save it by clicking \u201cCTRL+S\u201d or \u201cCommand + S\u201d or click the tick in the top right.</p> <p>Then deploy by clicking the \u201cDeploy\u201d button in the top right. On the dialogue, change the deployment type to \u201cService\u201d and click \u201cDeploy\u201d.</p>","location":"platform/tutorials/RSSProcessingPipeline/#2-modify-and-deploy-the-slack-notification-connector"},{"title":"Congratulations","text":"<p>You have deployed all three stages of the pipeline and should be receiving thousands of Slack messages. You might be thinking that Quix has led you on the path to destroying your Slack server. But don\u2019t worry \u2014 the pipeline is processing all the cached messages and will stop soon, honest. If this were a production pipeline, you\u2019d be very happy you haven\u2019t lost all those precious live messages.</p> <p></p>","location":"platform/tutorials/RSSProcessingPipeline/#congratulations"},{"title":"Help","text":"<p>If you run into trouble with the tutorial, want to chat with us about your project or anything else associated with streaming processing you can join our public Slack community called The Stream,window=_blank.</p>","location":"platform/tutorials/RSSProcessingPipeline/#help"},{"title":"Aim","text":"<p>Quix allows you to create complex and efficient infrastructure in a quick and simple way. To show you how, this tutorial will guide you through the steps to build a real time streaming pipeline that sends alerts to your phone when the Bitcoin price reaches certain threshold.</p>  <p>Note</p> <p>This guide will take you through the steps to start working with Twilio (developer platform for communications) and the coinAPI (platform that provides data APIs to cryptocurrency) using Quix.</p>  <p>By the end you will have:</p> <ul> <li> <p>Deployed a currency exchange rate capture connector</p> </li> <li> <p>Deployed Twilio connector to send alerts to your mobile phone</p> </li> </ul> <p></p>","location":"platform/tutorials/currency-alerting/"},{"title":"Prerequisites","text":"<p>We assume that all you have is a Quix account that you haven\u2019t started using yet.</p>  <p>Tip</p> <p>If you don\u2019t have a Quix account yet, go here,window=_blank and create one.</p>","location":"platform/tutorials/currency-alerting/#prerequisites"},{"title":"Overview","text":"<p>This walkthrough covers the following steps:</p> <ol> <li> <p>Create third party accounts: Twilio and CoinApi</p> </li> <li> <p>Deploy connectors from the Quix Library for:</p> <ol> <li> <p>the Coin API data source</p> </li> <li> <p>the Twilio sink or output</p> </li> </ol> </li> </ol>","location":"platform/tutorials/currency-alerting/#overview"},{"title":"Create free third party accounts","text":"","location":"platform/tutorials/currency-alerting/#create-free-third-party-accounts"},{"title":"Create a free CoinApi account","text":"<p>Note</p> <p>CoinAPI is a platform that provides data APIs for many financial instruments including cryptocurrency. We will be using it to get live Bitcoin (BTC) data.</p>  <p>Let\u2019s create a free coinAPI account:</p> <ol> <li> <p>Go to https://www.coinapi.io/,     window=_blank.</p> </li> <li> <p>Click the \"Get a free API key\" button and complete the dialog to     create a free account.</p> </li> <li> <p>You will receive your API key on your provided email. Keep it safe     for later.</p> </li> </ol>","location":"platform/tutorials/currency-alerting/#create-a-free-coinapi-account"},{"title":"Create a free Twilio account","text":"<p>Note</p> <p>Twilio is a developer platform for communications. We can use Twilio, for instance, to send SMS messages.</p>  <p>Let\u2019s create a free Twilio account:</p> <ol> <li> <p>Go to https://www.twilio.com/,     window=_blank.</p> </li> <li> <p>Click the \"Sign Up\" button and complete the dialog. Do the email and     text message verifications.</p> </li> <li> <p>Then, we will need to setup the Twilio account and create a     messaging service. It\u2019s not too hard but there are a couple of     sticky points, so we\u2019ll take you through it.</p> </li> <li> <p>Step 3 video-instructions</p>  <p>Tip</p> <p>Complete step 3 following our short 2 min video here, window=_blank.</p>  <p>Remember to gather your IDS and keep them safe for later: the Messaging Service SID in the Properties section and the Account SID and Auth Token in the Dashboard.</p> </li> <li> <p>Step 3 written-instructions</p> <ol> <li> <p>On your first login, tell Twilio:</p> <ul> <li> <p>Which product to use: SMS</p> </li> <li> <p>What to build: Alerts &amp; Notifications</p> </li> <li> <p>How to build it: With no code at all</p> </li> <li> <p>What is your goal: Build something myself</p> </li> </ul> </li> <li> <p>Click \"Get Started with Twilio\".</p> </li> <li> <p>Once in the Dashboard menu, get a phone number for sending     messages:</p> <ul> <li> <p>Click \"Get Trial Number\"</p> </li> <li> <p>You will be assigned a unique phone number</p> </li> <li> <p>That\u2019s the number Twilio is going to be sending SMSs from     (and you can receive SMSs there too)</p> </li> </ul> </li> <li> <p>In the menu on the left hand side expand \"Messaging\".</p> <ul> <li> <p>Click Services section</p> </li> <li> <p>Click the \"Create Messaging Service\" button</p> <ul> <li> <p>Name: Something like \"QuixAlerting\"</p> </li> <li> <p>Use Case: Notify my users</p> </li> <li> <p>Click \"Create Messaging Service\"</p> </li> </ul> </li> <li> <p>On the next page you\u2019ll assign the \"Sender\", i.e. your new     phone number:</p> <ul> <li> <p>Click \"Add Senders\" button</p> </li> <li> <p>In the dialog: check that Sender Type is on \"Phone     Number\" and click \"Continue\"</p> </li> <li> <p>Select or Tick the phone number (it is the one created     for you earlier)</p> </li> <li> <p>Click \"Add Phone Numbers\"</p> </li> </ul> </li> </ul> </li> </ol> </li> </ol>  <p>Note</p> <p>If you are in the Twilio wizard the next step is to \"Setup Integration\". You don\u2019t need to do this so just click the \"Skip\" button towards the bottom of the screen. You\u2019ll do the integration in Quix.</p>  <ol> <li> <p>Finally, gather your ID\u2019s and keep them safe for later:</p> <ul> <li> <p>Find the Messaging Service SID in the \"Messaging\" then     \"Services\" menus on the left hand side. The SID is listed next     to the \"QuixAlerting\" messaging service</p> </li> <li> <p>Find the Account SID and Auth Token in the \"Account\" menu under     \"API keys &amp; tokens\"</p> </li> </ul> </li> </ol>","location":"platform/tutorials/currency-alerting/#create-a-free-twilio-account"},{"title":"Quix Library","text":"<p>Now let\u2019s find and deploy the 2 connectors needed to make this project work.</p>","location":"platform/tutorials/currency-alerting/#quix-library"},{"title":"Coin API","text":"<p>We\u2019ll start with the Coin API. So head over to the Quix Library and search for \"Coin API\" and click \"Setup and Deploy\" on the Coin API tile.</p> <p></p> <p>The connector needs some values for it to work, some of these have default values which you can change now or later if you want to. The only thing you have to provide is the API key you got when you signed up to Coin API.</p> <p>Once you have entered your Coin API key just click Deploy.</p>  <p>Note</p> <p>This version of the Coin API connector is rate limited so that your free trial account doesn\u2019t exceed the quotas imposed by Coin API. It will fetch data every 15 minutes. You can unlock this limit by saving the code to your workspace and removing the limit. (Let us know if you want help with this)</p>","location":"platform/tutorials/currency-alerting/#coin-api"},{"title":"Logs","text":"<p>Once the connector has deployed it will start automatically and you\u2019ll be redirected to the workspace home page.</p> <p>You can click on the Coin API card where you will see the logs.</p> <p>If you see an error you might have to wait a few minutes for your API key to propagate to all of the Coin API servers.</p>","location":"platform/tutorials/currency-alerting/#logs"},{"title":"Twilio","text":"<p>Head back to the Quix Library and search for \"Twilio\" and click \"Setup and Deploy\" on the Twilio Sink tile.</p> <p></p> <p>Just like the Coin API connector, this one also needs some values for it to work. Use the guidance in the connectors setup guide to fill in the required details.</p> <ol> <li> <p>Ensure the input topic is \"coin-data\".</p> </li> <li> <p>\"Numbers\" should be the command separated list of phone numbers to     send SMS alerts to.</p> </li> <li> <p>The account_sid, auth_token and messaging_service_sid can be     populated using the values you saved while setting up your Twilio     account.</p> </li> <li> <p>message_limit can be left at 2 or changed to suit your needs.</p> </li> </ol> <p>Click Deploy and wait for the pre-built connector to be depolyed to your workspace.</p>","location":"platform/tutorials/currency-alerting/#twilio"},{"title":"Logs","text":"<p>Once deployed you will again be redirected to the workspace home page. You can click on the Twilio Sink tile and view the logs if you wish.</p>  <p>Note</p> <p>The logs may display a message similar to \"skipped due to message limit reached\". It\u2019s just an informational display. Not an error and is due to the per minute message limit we configured for the Twilio connector in the previous stage.</p>  <p></p>","location":"platform/tutorials/currency-alerting/#logs_1"},{"title":"Congratulations","text":"<p>At this point you should be receiving messages to your mobile phone. Congratulations you\u2019ve done it!</p>","location":"platform/tutorials/currency-alerting/#congratulations"},{"title":"Recap - What did you just do!","text":"<ul> <li> <p>[x] You created a realtime, always on solution on the Quix     serverless compute environment. All for Free!</p> </li> <li> <p>[x] You deployed two connectors. One to read data from another     platform and one to make decisions based on that data and send     notifications via SMS. In real time.</p> </li> <li> <p>[x] You gained some experience of navigating the Quix portal and     how to deploy connectors. All without doing any coding!</p> </li> </ul>","location":"platform/tutorials/currency-alerting/#recap-what-did-you-just-do"},{"title":"What\u2019s Next","text":"<p>What else can you use Quix for?</p> <p>You can stream any kind of data into the Quix platform, from other apps, websites and services. From hardware, machinery or wearables or from anything you can think of that outputs data. You can then process that data in any imaginable way.</p> <p>See what you can do with it and please share your experiences with us. In fact, share it with your colleagues, friends and family. Blog, tweet and post about it and tell anyone who will listen about the possibilities.</p>  <p>Tip</p>  <p>If you run into trouble please reach out to us. We\u2019ll be more than happy to help. We hang out at The Stream, window=_blank. Come and say hi.</p>","location":"platform/tutorials/currency-alerting/#whats-next"},{"title":"No code sentiment analysis","text":"<p>This tutorial shows how to build a data processing pipeline without code. You\u2019ll analyze tweets that contain information about Bitcoin and stream both raw and transformed data into Snowflake, a storage platform, using the Twitter, HuggingFace\\&lt;/superscript&gt; and Snowflake connectors.</p> <p>I\u2019ve made a video of this tutorial if you prefer watching to reading.</p>","location":"platform/tutorials/nocode-sentiment-analysis/"},{"title":"What you need for this tutorial","text":"<ol> <li> <p>Free Quix account</p> </li> <li> <p>Snowflake account</p> </li> <li> <p>Twitter developer account     (You can follow this tutorial to set up a developer account )</p> </li> </ol>","location":"platform/tutorials/nocode-sentiment-analysis/#what-you-need-for-this-tutorial"},{"title":"Step one: create a database","text":"<p>Sign in to your Snowflake account to create the Snowflake database which will receive your data. Call this \"demodata\" and click \"Create.\"</p>","location":"platform/tutorials/nocode-sentiment-analysis/#step-one-create-a-database"},{"title":"Step two: get your data","text":"<p>In Quix, click into the library and search for the Twitter source connector.</p> <p>Click \"Add new.\" This adds the source to your pipeline and brings you back to the library.</p> <p>Fill in the necessary fields:</p> <ul> <li> <p>Name: Twitter Data - Source</p> </li> <li> <p>Output: twitter-data</p> </li> <li> <p>Twitter bearer token: paste your Twitter Dev token here</p> </li> <li> <p>Twitter_search_paramaters: (#BTC OR btc #btc OR BTC)</p> </li> </ul>  <p>Tip</p> <p>Use search parameters to obtain Tweets on a subject that interests you! e.g. (#dolphins OR dolphins)</p>  <p>Click \"Deploy\"</p> <p></p>","location":"platform/tutorials/nocode-sentiment-analysis/#step-two-get-your-data"},{"title":"Step three: transformation for sentiment analysis","text":"<ul> <li> <p>Click the \"Add transformation\" button</p> </li> <li> <p>In the library, search for \"HuggingFace\"</p> </li> <li> <p>Click \"Set up and deploy\" on the HuggingFace connector</p> </li> <li> <p>Choose \"Twitter data\" as the input topic</p> </li> <li> <p>The output field should be set to \"hugging-face-output\" by default,     leave this as it is or enter this if it\u2019s not pre-populated.</p> </li> <li> <p>Leave all other values with their defaults</p> </li> <li> <p>Click \"Deploy\"</p> </li> </ul>  <p>Note</p> <p>Find out more about the hugging face model and the other models you could use at huggingface.co^</p>  <p></p>","location":"platform/tutorials/nocode-sentiment-analysis/#step-three-transformation-for-sentiment-analysis"},{"title":"Step five: delivery to your Snowflake database","text":"<ul> <li> <p>Click the \"Add destination\" button on the home screen</p> </li> <li> <p>Search the library for the Snowflake connector</p> </li> <li> <p>Click \"Set up and deploy\" on the connector</p> </li> <li> <p>Fill in the necessary fields:</p> </li> <li> <p>Choose the hugging-face-output output topic</p> </li> <li> <p>The \"Broker__TopicName\" field should be set to     \"hugging-face-output\". This means it will receive the data being     output by the sentiment analysis model.</p> </li> </ul> <p>To fill in the Snowflake locator and region (these are similar to a unique ID for your Snowflake instance), navigate to your Snowflake account. Copy the locator and region from the URL and paste them into the corresponding fields in the connector setup in Quix. Lastly, input your username and password.</p> <p></p> <p>Click \"Deploy\" on the Snowflake connector. If the credentials and connection details are correct, you\u2019ll see the \"Connected\" status in the log and will be redirected to your workspace.</p> <p></p> <p>Congratulations! You built a no-code pipeline that filters and collects data from Twitter, transforms it with a HuggingFace model and delivers it to a Snowflake database.</p> <p>You can now go back over to Snowflake and find the \"Databases\" menu. Expand the \"demodata\" database and then find the tables under \"public\".</p> <p></p>  <p>Tip</p> <p>If you need help with this tutorial or want to chat about anything related to Quix or stream processing in general please come and say hi on The Stream^, our slack community.</p>","location":"platform/tutorials/nocode-sentiment-analysis/#step-five-delivery-to-your-snowflake-database"},{"title":"Quick Start","text":"<p>This quick start gets you up and running with Quix. It shows how to set up a pipeline that ingests real-time Formula 1 data and sends an alert to your phone whenever a race car passes 300 kilometers per hour. It uses one source, one transformation and one destination from the Quix library along with a Twilio integration.</p> <p>You\u2019ll learn how to create a topic and how to connect, customize and write sources, transformations and destinations to set up a stream of real-time data. In the end, you\u2019ll be more comfortable using the platform and able to apply it in your own projects.</p> <p>When you\u2019re done with this quick start guide, your pipeline should look something like this:</p> <p></p> <p>Requirements</p> <ul> <li> <p>Free Quix account</p> </li> <li> <p>GitHub account</p> </li> <li> <p>Twilio account</p> </li> </ul> <p>Installation</p> <ul> <li>No installations are required for cloud deployment</li> </ul> <p>In this guide</p> <ol> <li> <p>Set up a Quix workspace</p> </li> <li> <p>Add a data source to your project</p> </li> <li> <p>Set up data transformation</p> </li> <li> <p>Deliver your data with Twilio</p> </li> <li> <p>Run the pipeline</p> </li> </ol>","location":"platform/tutorials/quick-start/"},{"title":"1. Set up a Quix workspace","text":"<p>Sign up for Quix. The free account provides enough resources to build a single streaming data project. You\u2019ll arrive in a workspace when you sign up.</p>","location":"platform/tutorials/quick-start/#1-set-up-a-quix-workspace"},{"title":"2. Add a data source to your project from the Quix library","text":"<p>The data sources in the Quix library are out-of-the-box code samples that you can plug and play or customize. It\u2019s an open source library, so you can also contribute your own when you\u2019re ready!</p> <p>To add a data source to your pipeline, click Add data source . This takes you to the library, which you can also access via GitHub.</p> <p>You\u2019ll use Formula 1 Data for this example. This source includes the speed, acceleration, brake usage and other detailed data from a real F1 car formatted into real-time data. Find the Formula 1 Data card in the Quix library and click Set up connector. Using the prepared data saves the time of importing or building data sets. Setting up a connector will simultaneously create a topic (topics are channels that carry real-time data, one per data source) and bring the Formula 1 data stream into the topic.</p> <p>You can use the topic name that automatically populates the field or re-name it.</p> <p>Click Connect.</p> <p>Now the source is connected. You can go into the source and check the data in the live preview visualization. The data source includes more data types than needed, so you can click the tick boxes next to Speed and RPM under Select parameters or events.</p>","location":"platform/tutorials/quick-start/#2-add-a-data-source-to-your-project-from-the-quix-library"},{"title":"3. Set up a data transformation using the no-code method","text":"<p>You want your pipeline to look at the data and see when a vehicle goes faster than 300 kilometers per hour. This requires a threshold alert.</p> <p>Click Add transform service to open the library. Find the Threshold transformation and click Setup. This transformation connector generates an alert when a certain numeric threshold is crossed.</p> <p>The Project name, Input, Output, ParameterName and ThresholdValue will auto populate. Change ThresholdValue to 300 to receive an alert whenever the car passes the 300 milometers per hour point. Save by clicking Save as a project.</p> <p>Click Deploy. This brings up a dialog box with options to change variables, network and state. For this project, you can choose Service and leave the remaining configuration as is. Click Deploy. A service is any application code that continuously runs in a serverless environment.</p>","location":"platform/tutorials/quick-start/#3-set-up-a-data-transformation-using-the-no-code-method"},{"title":"4. Deliver your data using the Twilio destination","text":"<p>Now that the data is ready to go, you need to tell it where to go. Click Add new next to \u201cDestinations\u201d on the workplace screen. This is where Twilio comes in to send speed alerts to your phone.</p> <p>Click Setup. Choose your threshold-alert-output as the input. You can find the numbers, account_sid, auth_token', and messaging_service_sid in your free Twilio account.</p> <p>Message_limit refers to the total number of messages you\u2019ll receive. It\u2019s set at two by default, as some message charges may apply.</p> <p>Click Save as project once you have added the requested Twilio information.</p>","location":"platform/tutorials/quick-start/#4-deliver-your-data-using-the-twilio-destination"},{"title":"5. Run the pipeline","text":"<p>Here\u2019s the fun part! The three dots (one in the upper-right corner of each card) indicate that everything works correctly. All you need to do is click Deploy to start receiving notifications on your phone whenever the car you\u2019re tracking goes faster than 300 kilometers per hour.</p> <p>Conclusion</p> <p>You\u2019ve successfully built a data pipeline that transforms one stream of data, which is just the beginning!</p> <p>Speak with a technical expert about how to set up and use Quix in your own projects.</p> <p>Additional resources</p> <ul> <li> <p>The Stream community on Slack</p> </li> <li> <p>Stream processing     glossary</p> </li> <li> <p>Developer docs main navigation</p> </li> </ul>","location":"platform/tutorials/quick-start/#5-run-the-pipeline"},{"title":"Sentiment analysis","text":"<p></p> <p>Build a real time sentiment analysis pipeline. Use the power and ease of Quix to process high volume tweets or user sourced conversation messages.</p>","location":"platform/tutorials/sentimentAnalysis/"},{"title":"Get Started","text":"<p>To get started make sure you have a Quix account, signup for a completely free account at https://quix.io</p> <p>You\u2019ll also need a Twitter developer account.</p> <p>A rough guide to getting this is as follows:</p> <p>Register for an account here https://developer.twitter.com/en/apply-for-access</p> <ol> <li> <p>Fill out the forms, there are quite a few!</p> </li> <li> <p>Wait for Twitter to provide you with your account.</p> </li> <li> <p>Once you have your account locate the \u201cKeys and Tokens\u201d page</p> </li> <li> <p>Save your bearer token for later</p> </li> </ol> <p>Use one of the many guides on YouTube or try this one if you need some help creating your Twitter developer account and filling in all those forms.</p> <p>If you need any assistance, we\u2019re here to help in The Stream, our free Slack community.</p>","location":"platform/tutorials/sentimentAnalysis/#get-started"},{"title":"Library","text":"<p>Most of the code you need has already been written for you and it\u2019s located in our library. We\u2019ll be referring to the library often so make sure you know where it is.</p>","location":"platform/tutorials/sentimentAnalysis/#library"},{"title":"Pipeline Services","text":"<p>There are 4 stages to the pipeline you will build in this tutorial.</p>","location":"platform/tutorials/sentimentAnalysis/#pipeline-services"},{"title":"Twitter Data Source","text":"<p>Our Twitter connector, pre-built to save you time.</p>","location":"platform/tutorials/sentimentAnalysis/#twitter-data-source"},{"title":"Twitter to chat","text":"<p>Convert Twitter text to chat messages, so they can be used alongside manually entered chat messages.</p>","location":"platform/tutorials/sentimentAnalysis/#twitter-to-chat"},{"title":"Sentiment analysis","text":"<p>Do the real work of analyzing chat and tweet sentiment.</p>","location":"platform/tutorials/sentimentAnalysis/#sentiment-analysis"},{"title":"Web UI","text":"<p>A web UI allowing you to enter and see messages as well as see the sentiment changes in real time.</p>","location":"platform/tutorials/sentimentAnalysis/#web-ui"},{"title":"Build It","text":"<p>This guide will show you how to deploy each stage of the processing pipeline, starting with the Web UI.</p>","location":"platform/tutorials/sentimentAnalysis/#build-it"},{"title":"1. Web UI","text":"<p>Follow these steps to deploy the Web UI.</p> <ol> <li> <p>Navigate to the Library and locate \u201cSentiment Demo UI\u201d.</p> </li> <li> <p>Click \u201cEdit code\u201d</p> </li> <li> <p>Click \u201cSave as project\u201d The code for this Angular UI is now saved to     your workspace</p> </li> </ol>","location":"platform/tutorials/sentimentAnalysis/#1-web-ui"},{"title":"Tag the code and deploy the UI","text":"<ol> <li> <p>Click the +tag button at the top of any code file</p> </li> <li> <p>Enter v1 and press enter</p> </li> <li> <p>Click Deploy near the top right corner</p> </li> <li> <p>In the deployment dialog, select v1 under the \u201cVersion Tag\u201d</p> <p>This is the tag you just created</p> </li> <li> <p>Click \u201cService\u201d in \u201cDeployment Settings\u201d</p> <p>This ensures the service runs continuously</p> </li> <li> <p>Click the toggle in \u201cPublic Access\u201d</p> </li> <li> <p>This enables access from anywhere on the internet</p> </li> <li> <p>Click \u201cDeploy\u201d</p> <p>The UI will stream data from the \u201csentiment\u201d and \u201cmessages\u201d topics as well as send messages to the \u201cmessages\u201d topic.</p> </li> <li> <p>Once deployed, click the service tile</p> </li> <li> <p>Click the \u201cPublic URL\u201d</p>  <p>Info</p> <p>This is the user interface for the demo. This screenshot shows the view you\u2019ll see after creating a \u201croom\u201d to chat in.</p> <p></p>  </li> </ol>","location":"platform/tutorials/sentimentAnalysis/#tag-the-code-and-deploy-the-ui"},{"title":"2. Sentiment analysis","text":"<p>Follow these steps to deploy the sentiment analysis stage.</p> <ol> <li> <p>Navigate to the Library and locate \u201cSentiment analysis\u201d     transformation.</p> </li> <li> <p>Click \u201cEdit code\u201d</p> </li> <li> <p>Click \u201cSave as project\u201d</p> <p>The code for this transformation is now saved to your workspace</p> </li> <li> <p>Locate main.py</p> </li> <li> <p>Locate the line of code that creates the output stream</p> <pre><code>output_stream=output_topic.create_stream(input_stream.stream_id)\n</code></pre> </li> <li> <p>Append \"-output\" to the stream id.</p> <p>This will ensure the Web UI is able to locate the sentiment being output by this transformation service.</p> <pre><code>output_stream=output_topic.create_stream(input_stream.stream_id + \"-output\")\n</code></pre> </li> </ol>","location":"platform/tutorials/sentimentAnalysis/#2-sentiment-analysis"},{"title":"Tag the code and deploy the service","text":"<ol> <li> <p>Click the +tag button at the top of any code file</p> </li> <li> <p>Enter v1 and press enter</p> </li> <li> <p>Click Deploy near the top right corner</p> </li> <li> <p>In the deployment dialog, select v1 under the \u201cVersion Tag\u201d</p> <p>This is the tag you just created</p> </li> <li> <p>Click \u201cService\u201d in \u201cDeployment Settings\u201d</p> <p>This ensures the service runs continuously</p> </li> <li> <p>Click \u201cDeploy\u201d</p> <p>This service receives data from the \u201cmessages\u201d topic and streams data out to the \u201csentiment\u201d topic. . You can now go to the public URL of the Web UI project you deployed in step one. . Enter values for \u201cRoom\u201d and \u201cName\u201d . Click connect.</p> <p></p> <p>You can now enter \u201cchat\u201d messages.</p> <p></p>  <p>Info</p> <p>The messages will be passed to the sentiment analysis service you deployed in step two. The sentiment is returned to the Web UI and displayed both in the chart and next to the comment in the chat window by colorizing the chat user\u2019s name.</p>  </li> </ol>","location":"platform/tutorials/sentimentAnalysis/#tag-the-code-and-deploy-the-service"},{"title":"3. Tweet to chat conversion","text":"<p>Follow these steps to deploy the tweet-to-chat conversion stage.</p> <ol> <li> <p>Navigate to the Library and apply the following filters</p> <ol> <li> <p>Languages = Python</p> </li> <li> <p>Pipeline Stage = Transformation</p> </li> <li> <p>Type = Basic templates</p> </li> </ol> </li> <li> <p>Select \u201cEmpty template- Transformation\u201d.</p> </li> <li> <p>Click \u201cEdit code\u201d</p> </li> <li> <p>Change the name to \u201ctweet-to-chat\u201d</p> </li> <li> <p>Change the input to \u201ctwitter-data\u201d by either selecting it or typing     it.</p> </li> <li> <p>Ensure the output is set to \u201cmessages\u201d</p> </li> <li> <p>Click \u201cSave as project\u201d</p> <p>The code for this transformation is now saved to your workspace</p> </li> <li> <p>Locate main.py</p> </li> <li> <p>Locate the line of code that creates the output stream</p> <pre><code>output_stream = output_topic.create_stream(input_stream.stream_id)\n</code></pre> </li> <li> <p>Change this line to get or create a stream called \u201ctweets\u201d</p> <pre><code>output_stream = output_topic.get_or_create_stream(\"tweets\")\n</code></pre> </li> <li> <p>Now locate quix_function.py</p> <p>Alter the on_pandas_frame_handler to match the code below</p> <pre><code>def on_pandas_frame_handler(self, df: pd.DataFrame):\n\nfor index, row in df.iterrows():\n\ntext = row[\"text\"]\n\nself.output_stream.events.add_timestamp_nanoseconds(row.time)\\\n\n.add_tag(\"name\", \"Twitter\") \\\n\n.add_value(\"chat-message\", text) \\\n\n.write()\n</code></pre>  <p>Info</p> <p>This will take \u201ctext\u201d from incoming tweets and stream them to the output topics tweets stream as event values with a key of \u201cchat-message\u201d which the other stages of the pipeline will recognize.</p>  </li> </ol>","location":"platform/tutorials/sentimentAnalysis/#3-tweet-to-chat-conversion"},{"title":"Now tag the code and deploy the service with these steps:","text":"<ol> <li> <p>Click the +tag button at the top of any code file</p> </li> <li> <p>Enter v1 and press enter</p> </li> <li> <p>Click Deploy near the top right corner</p> </li> <li> <p>In the deployment dialog, select v1 under the \u201cVersion Tag\u201d</p> <p>This is the tag you just created . Click \u201cService\u201d in \u201cDeployment Settings\u201d . This ensures the service runs continuously . Click \u201cDeploy\u201d</p> <p>This service receives data from the \u201ctwitter-data\u201d topic and streams data out to the \u201cmessages\u201d topic.</p>  <p>Success</p> <p>You now have a service that is ready to receive tweets and pass them onto the sentiment processing stage.</p>  </li> </ol>","location":"platform/tutorials/sentimentAnalysis/#now-tag-the-code-and-deploy-the-service-with-these-steps"},{"title":"4. Twitter Data Source","text":"<p>Follow these steps to deploy the Web UI.</p> <ol> <li> <p>Navigate to the Library and locate \u201cTwitter Data - Source\u201d.</p> </li> <li> <p>Click \u201cSetup &amp; deploy\u201d</p> </li> <li> <p>Enter your Twitter bearer token into the appropriate field</p> </li> <li> <p>Click \u201cDeploy\u201d</p> <p>This service receives data from Twitter and streams it to the \u201ctwitter-data\u201d topic. NOTE: that the default Twitter search criteria is looking for Bitcoin tweets, it\u2019s a high traffic subject and great for the demo. Feel free to change this once you\u2019ve got the demo working. . Locate the Web UI you deployed in step one . Navigate to the lobby . Enter \u201ctweets\u201d for the chatroom and provide your name in the \u201cname\u201d field . Click connect</p> <p>You will see Bitcoin tweets arriving in the chat along with the calculated average sentiment in a chart.</p> <p></p> </li> </ol>  <p>Success</p> <p>Your pipeline is now complete, you can send and view chat messages, receive tweets and analyze the sentiment of all of the messages.</p> <p>Share the QR code with colleagues and friends to talk about anything you like while Quix analyzes the sentiment in the room in real time.</p>","location":"platform/tutorials/sentimentAnalysis/#4-twitter-data-source"},{"title":"Next Steps","text":"<p>You\u2019ve just made extensive use of the Quix library, our collection of open source connectors, samples and examples. Now you can build your own connectors and apps and contribute by going to our Github here, forking our library repo and submitting your code, updates and ideas.</p> <p>What will you build, let us know. We\u2019d love to feature your project or use case in our newsletter.</p> <p>If you need any assistance, we\u2019re here to help in The Stream, our free Slack community.</p>","location":"platform/tutorials/sentimentAnalysis/#next-steps"},{"title":"Goodbye, for now..","text":"<p>We hope you enjoyed this tutorial on how to Build a real time chat and tweet sentiment analysis pipeline.</p> <p>If you have any questions or feedback please contact us on The Stream.</p> <p>Thank you and goodbye for now!</p>","location":"platform/tutorials/sentimentAnalysis/#goodbye-for-now"},{"title":"Stream and visualize real-time telemetry data with an Android app and Streamlit","text":"<p></p> <p>Learn how to build an end-to-end telemetry data pipeline for IoT applications. Use our Android companion app to stream sensor data from your phone and visualize it in a Streamlit app.</p>","location":"platform/tutorials/telemetry-data/"},{"title":"Get Started","text":"<p>To get started make sure you have a Quix account, signup for a completely free account at https://quix.io</p> <p>You will need an Android mobile phone for this tutorial (we're working on the Apple app).</p> <p>If you need any assistance, we're here to help in The Stream, our free Slack community.</p>","location":"platform/tutorials/telemetry-data/#get-started"},{"title":"Library","text":"<p>Most of the code you'll need has already been written. It lives in our library, which is accessible from inside the Quix portal or directly via our open source Github repo. We'll be referring to the library often so make sure you know where it is.</p>","location":"platform/tutorials/telemetry-data/#library"},{"title":"Components","text":"<p>Android App - Our companion app for collecting real-time sensor data from your phone. It'spre-built and published to the Play store to save you time. You can also access the source code in our Github repo.</p> <p>Streamlit App - See your location on a map and other activity metrics.</p> <p>QR Settings Share - A lightweight UI and API for sharing settings with the Android app.</p>","location":"platform/tutorials/telemetry-data/#components"},{"title":"Build It","text":"<p>This guide will show you how to deploy each of the components, starting with QR Settings Share.</p>","location":"platform/tutorials/telemetry-data/#build-it"},{"title":"1. QR Settings Share","text":"<p>Follow these steps to deploy the QR Settings Share.</p> <ol> <li> <p>Navigate to the Library and locate \"QR Settings Share\"</p> </li> <li> <p>Click \"Setup &amp; deploy\"</p> </li> <li> <p>Click \"Deploy\"</p> </li> </ol>  <p>Info</p> <p>The service will be deployed to your workspace</p>  <p>Open the UI with these steps</p> <ol> <li> <p>Once deployed, click the service tile</p> </li> <li> <p>Click the \"Public URL\"</p> </li> <li> <p>Append the following querystring to the url in the address bar</p> </li> </ol>  <p>?topic=phone-data&amp;notificationsTopic=notifications-topic</p>   <p>These are needed by the Android app and will be passed to it via the QR code you'll generate in a moment</p>  <ol> <li>You can now enter a username and device name into the relevant inputs</li> </ol> <p></p>  <p>These can be anything! But sensible values will help understand which token belongs to which user</p>  <ol> <li> <p>Click OK</p> </li> <li> <p>You can now click Generate Token</p> </li> </ol> <p>Several things happened when you clicked the button.</p> <p>A token was generated in Quix with an expiry time in around 1 month.</p> <p>The token was published to an API and a link to the token was generated.</p> <p>The QR shown on screen is a short-lived link to the longer lasting token.</p>  <p>Info</p> <p>Scanning the token with the Android App will allow the app to receive its configuration information without you having to enter any long token strings by hand.</p>","location":"platform/tutorials/telemetry-data/#1-qr-settings-share"},{"title":"2. Android App","text":"<p>It's time to install the Android app!</p> <ol> <li>Go to the Google Play store and search for \"Quix Tracker App\" ensuring it's the one published by us.</li> </ol> <p>[Screenshot of Play store coming soon]</p> <ol> <li> <p>Install the app.</p> </li> <li> <p>Tap the menu hamburger</p> </li> <li> <p>Tap settings</p> </li> <li> <p>Tap SCAN QR CODE</p> </li> <li> <p>Generate a new QR code in the QR Settings Share website you deployed in step 1</p> </li> <li> <p>Scan the QR code with your device</p>  <p>The Token, Workspace and topics should now be configured</p>  </li> <li> <p>Tap the menu hamburger once again</p> </li> <li> <p>Tap Dashboard</p> </li> <li> <p>Tap START at the bottom of the screen</p>  Your device is now streaming to the topic called 'phone-data' in your workspace <p>These parameters are being streamed from your phone:</p> <ul> <li>Accuracy</li> <li>Altitude</li> <li>BatteryLevel</li> <li>BatteryPowerSource</li> <li>BatteryState</li> <li>EnergySaverStatus</li> <li>gForceX</li> <li>gForceY</li> <li>gForceZ</li> <li>Heading</li> <li>Latitude</li> <li>LogInfo</li> <li>Longitude</li> <li>Speed</li> </ul>   <p>Tip</p> <p>Leave the app running until you've completed the next step.</p>  </li> </ol>","location":"platform/tutorials/telemetry-data/#2-android-app"},{"title":"3. Streamlit app","text":"<p>Explore the data inside Quix and then deploy a frontend app.</p> <p>Within Quix:</p> <ol> <li> <p>Click Data Explorer on the left hand menu</p> </li> <li> <p>Select the \"phone-data\" topic</p> </li> <li> <p>You should have at least one stream, select it</p> </li> <li> <p>Select as many parameters as you want. The gForce ones are great for this tutorial</p> </li> <li> <p>You can see the gForce from your device displayed on the waveform in real time</p> </li> </ol> <p>Deploy an app:</p> <ol> <li> <p>Click Library on the left hand menu</p> </li> <li> <p>Search for Streamlit Dashboard</p> </li> <li> <p>Click Preview code</p> </li> <li> <p>Click Edit code</p> </li> <li> <p>Ensure phone-data is selected for the input field</p> </li> <li> <p>Click Save as project</p>  <p>The code for the dashboard is now saved to your workspace. You can now edit it to ensure it works with the data coming from the device</p>  </li> <li> <p>Locate the following line in streamlit_file.py</p> </li> </ol> <pre><code>st.line_chart(local_df[[\"datetime\",'Speed','EngineRPM']].set_index(\"datetime\"))\n</code></pre> <ol> <li> <p>Replace 'Speed' with 'gForceX'</p> </li> <li> <p>Replace 'EngineRPM' with 'gForceY'</p> </li> <li> <p>Remove the following code block</p> </li> </ol> <pre><code>with fig_col2:\n\nst.markdown(\"### Chart 2 Title\")\n\nst.line_chart(local_df[[\"datetime\", 'Gear', 'Brake']].set_index(\"datetime\"))\n</code></pre> <ol> <li> <p>Click Run near the top right corner of the code editor</p>  <p>The dashboard will run right here in the development environment</p>   <p>Click the icon that appears next to the project name (circled here in green)</p> <p></p>  </li> <li> <p>Once the dashboard has loaded you will see sensor data from your device in real time</p> <p></p>  <p>Below this is also the raw data showing all the values from the device</p>  </li> <li> <p>Close the dashboard</p>  <p>The dashboard you were just using was deployed to a temporary URL in the development area</p> <p>You need to deploy it to its permanent home!</p>  </li> <li> <p>Click the Deploy button in the top right corner.</p> </li> <li> <p>Click Service in the Deployment Settings</p> </li> <li> <p>Click the toggle button under Public Access</p> </li> <li> <p>Click Deploy</p> <p>You will be redirected to the home page</p> </li> <li> <p>Once the dashboard has been built and deployed click the deployment tile</p> </li> <li> <p>Click the Public URL</p> </li> </ol>  <p>You are now looking at your dashboard in its permanent home</p>   <p>Be aware that there is no security on the dashboard</p>","location":"platform/tutorials/telemetry-data/#3-streamlit-app"},{"title":"Next Steps","text":"<p>You've just connected your mobile device to Quix and deployed a Streamlit app to view the data in real time.</p> <p>Now try showing some different parameters in the app, or build a transformation in Quix to make better use of the raw data.</p> <p>What will you build? Let us know. We'd love to feature your project or use case in our newsletter.</p> <p>If you need any assistance, we're here to help in The Stream, our free Slack community.</p>","location":"platform/tutorials/telemetry-data/#next-steps"},{"title":"Ciao for now!","text":"<p>We hope you enjoyed this tutorial. If you have any questions or feedback please contact us on The Stream.</p> <p>Thank you.!</p>","location":"platform/tutorials/telemetry-data/#ciao-for-now"},{"title":"Connect to Quix","text":"<p>The Quix SDK comes with a streaming client that enables you to connect to Quix easily, to read data from Quix, and to write data to Quix. The streaming client manages the connections between your application and Quix and makes sure that the data is delivered reliably to and from your application.</p>","location":"sdk/connect/"},{"title":"Using QuixStreamingClient","text":"<p>Starting with 0.4.0, we\u2019re offering QuixStreamingClient, which handles the cumbersome part of setting up your streaming credentials using the Quix Api. When you\u2019re running the app in the online IDE or as a deployment, all you have to do is the following:</p> PythonC#   <pre><code>client = QuixStreamingClient()\n</code></pre>   <pre><code>var client = new Quix.Sdk.Streaming.QuixStreamingClient();\n</code></pre>    <p>If you wish to run the same code locally, you\u2019ll have to provide an OAuth2.0 bearer token. We have created a purpose made token for this, called SDK token. Once you have the token you will have to provide it as an argument to QuixStreamingClient or set <code>Quix__Sdk__Token</code> environment variable.</p> PythonC#   <pre><code>client = QuixStreamingClient('your_token')\n</code></pre>   <pre><code>var client = new Quix.Sdk.Streaming.QuixStreamingClient(\"your_token\");\n</code></pre>    <p>Using the streaming client is another way to talk with a broker. It is a Kafka specific client implementation that requires some explicit configuration but allows you to connect to any Kafka cluster even outside Quix platform. It involves the following steps:</p> <ul> <li> <p>Obtain a client certificate and credentials (security context)     for your application.</p> </li> <li> <p>Create a streaming client.</p> </li> </ul> <p>A security context consists of a client certificate, username, and password. Quix generates these automatically for you when you create a project using the templates provided on the Quix Portal. If necessary, you can download these credentials separately from Topics option on Quix Portal.</p> <p>The following code shows you how to set up the <code>SecurityOptions</code> for your connection and how to create a <code>StreamingClient</code> instance to start Reading and Writing real-time time series data with Quix:</p> PythonC#Javascript   <pre><code>security = SecurityOptions(CERTIFICATES_FOLDER, QUIX_USER, QUIX_PASSWORD)\nclient = StreamingClient('kafka-k1.quix.ai:9093,kafka-k2.quix.ai:9093,kafka-k3.quix.ai:9093', security)\n</code></pre>   <pre><code>var security = new SecurityOptions(CERTIFICATES_FOLDER, QUIX_USER, QUIX_PASSWORD);\nvar client = new Quix.Sdk.Streaming.StreamingClient(\"kafka-k1.quix.ai:9093,kafka-k2.quix.ai:9093,kafka-k3.quix.ai:9093\", security);\n</code></pre>   <p>Quix web APIs are secured with OAuth2.0 bearer scheme. Therefore, all HTTP requests to Quix must contain a valid bearer token. You can generate a personal access token (PAT) for use as a bearer token from the portal by following the following steps.</p> <ul> <li> <p>Navigate to your profile by clicking on your avatar and     selecting \"Profile\" from the drop-down menu.</p> </li> <li> <p>Select \"Personal Access Tokens\" tab on the profile page.</p> </li> <li> <p>Click on \"Generate Token\" button to open a dialog to set a     name and an expiry date for the PAT and click on \"Create\" to     generate the PAT.</p> </li> </ul>  <p>Tip</p> <p>For your convenience, when you create a new project on the Quix platform, the credentials are generated and set in the code for you. However, it is good practice to move them out of the code to a more secure location like environment variables or a keystore, depending on your development platform.</p>  <p>When you deploy your application to Quix, you can store them on Quix as environment variables.</p>","location":"sdk/connect/#using-quixstreamingclient"},{"title":"Set up docker environment","text":"<p>Docker is an alternative to the classic environment setup for local development ( see Set up your local IDE )</p>  <p>Note</p> <p>Docker knowledge is not required to develop or deploy applications with Quix. This guide is only for people that prefer using local development environments using Docker.</p>  <p>It enables you to install and run your project in isolation from the rest of the system which provides the advantage of removing system dependencies and conflicts.</p> <p>Isolation is achieved by creating a lightweight container (Docker Container) which behaves like a virtual machine (in our case Ubuntu Linux).</p>","location":"sdk/docker-setup/"},{"title":"Install Prerequisities","text":"<p>In order to use the Quix SDK in Docker you need to have installed these prerequisities.</p> <ul> <li> <p>Docker (tested on version 20.10.17)</p> </li> <li> <p>Docker Compose (tested on version 1.29.2)</p> </li> </ul>","location":"sdk/docker-setup/#install-prerequisities"},{"title":"Install Docker ( step 1 )","text":"<p>To install the Docker on your environment you need to follow this guide here (https://docs.docker.com/get-docker/).</p>  <p>Note</p> <p>On Windows we tested this setup using the WSL2 backend.</p>","location":"sdk/docker-setup/#install-docker-step-1"},{"title":"Install docker-compose ( step 2 )","text":"<p>We\u2019ll be using the docker-compose tool which is designed for easy configuration of local Docker setups.</p> <p>To install docker-compose, please follow the guide here ( https://docs.docker.com/compose/install/ ).</p>  <p>Note</p> <p>If you are on Windows then you can skip this step because the Docker installation package from the step 1 already contains the docker-compose tool.</p>","location":"sdk/docker-setup/#install-docker-compose-step-2"},{"title":"Start with downloading a couple of things","text":"<ul> <li> <p>Download the project then navigate to your solution\u2019s root folder.</p> </li> <li> <p>Navigate to     quix-library     and download <code>/docker/</code> folder content.</p> </li> </ul>","location":"sdk/docker-setup/#start-with-downloading-a-couple-of-things"},{"title":"Build and run project","text":"<p>Open a command line within your new <code>docker</code> folder.</p> <p>You can start the build by running the following command.</p> <pre><code>docker-compose run --rm server\n</code></pre>  <p>Note</p> <p>On the first run the compile script may take while (around 10 minutes) to build all the project dependencies. The subsequent builds will be much faster.</p>  <p>Using the docker file provided, you should now be in a running server, which has all requirements installed for Quix. As the running image is nothing more than the <code>quixpythonbaseimage</code> with your code folder mounted at <code>/app</code>, in order to get your application working, you\u2019ll need to install your python requirements.</p> <p>You can do this using the following, executed in the <code>/app</code> folder</p> <pre><code>python3 -m pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/\n</code></pre> <p>Use the resulting environment as you would your own machine, such as run your python application by executing <code>python3 main.py</code></p>  <p>Note</p> <p>As <code>/apps</code> folder is a mounted directory, any file or folder change in the container will be synced to your original folder in your machine and vice-versa.</p>   <p>Note</p> <p>As your environment variables will greatly depend on what your application needs, make sure to update <code>docker/.env</code> as needed. By default all values are placeholder and this might be something you need to configure or add to before the application can correctly run. Several of these environment values could be considered \"secrets\", therefore be mindful of what you end up committing to your repository.</p>","location":"sdk/docker-setup/#build-and-run-project"},{"title":"Additional documentation","text":"<p>To get the additional information on Docker and Docker compose commands please follow up with the documentation:</p> <ul> <li> <p>Docker documentation ( https://docs.docker.com/reference/     )</p> </li> <li> <p>Docker compose documentation ( https://docs.docker.com/compose/     )</p> </li> </ul>","location":"sdk/docker-setup/#additional-documentation"},{"title":"Introduction","text":"<p>The Quix SDK makes it quick and easy to develop streaming applications. It\u2019s designed to be used for high performance telemetry services where you need to process high volumes of data in a nanosecond response time.</p> <p>The SDK is available for Python and C#.</p> <p>Using the Quix SDK, you can:</p> <ul> <li> <p>Write time-series data to a Kafka Topic</p> </li> <li> <p>Read time-series data from a Kafka Topic</p> </li> <li> <p>Process data by reading it from one     Topic and writing the results to another one.</p> </li> </ul> <p>To support these operations, the SDK provides several useful features out of the box, and solves all the common problems you should face when developing real-time streaming applications:</p>","location":"sdk/introduction/"},{"title":"Streaming context","text":"<p>The Quix SDK handles stream contexts for you, so all the data from one data source is bundled in the same scope. This allows you to attach metadata to streams.</p> <p>The SDK simplifies the processing of streams by providing callbacks on the reading side. When processing stream data, you can identify data from different streams more easily than with the key-value approach used by other technologies.</p> <p>Refer to the Streaming context section of this documentation for more information.</p>","location":"sdk/introduction/#streaming-context"},{"title":"In-memory data processing","text":"<p>The Quix SDK is designed to make in-memory data processing extremely efficient. We use high-performance SDK features in conjunction with the message broker capabilities to achieve maximum throughput with the very minimum latency.</p> <p>Refer to the In-memory data processing section of this documentation for more information.</p>","location":"sdk/introduction/#in-memory-data-processing"},{"title":"Built-in buffers","text":"<p>If you\u2019re sending data at high frequency, processing each message can be costly. The SDK provides a built-in buffers features for reading and writing to give you absolute freedom in balancing between latency and cost.</p> <p>Refer to the Built-in buffers section of this documentation for more information.</p>","location":"sdk/introduction/#built-in-buffers"},{"title":"Support for data frames","text":"<p>In many use cases, multiple parameters are emitted at the same time, so they share one timestamp. Handling this data independently is wasteful. The SDK uses a rows system, and can work with Pandas DataFrames natively. Each row has a timestamp and user-defined tags as indexes.</p> <p>Refer to the Support for Data Frames section of this documentation for more information.</p>","location":"sdk/introduction/#support-for-data-frames"},{"title":"Message splitting","text":"<p>The SDK automatically handles large messages on the producer side, splitting them up if required. You no longer need to worry about Kafka message limits. On the consumer side, those messages are automatically merged back.</p> <p>Refer to the Message splitting section of this documentation for more information.</p>","location":"sdk/introduction/#message-splitting"},{"title":"Message compression","text":"<p>The Quix SDK automatically compresses your messages, reducing them by an average factor of 10 times. You save money via added efficiency.</p> <p>The SDK also sends parameter values as the delta between timestamps, converting strings to flags, and in general reduces payload size for each message. This happens before compression is applied, so the final compression ratio is even higher.</p> <p>Refer to the Message compression section of this documentation for more information.</p>","location":"sdk/introduction/#message-compression"},{"title":"Data serialization and de-serialization","text":"<p>The Quix SDK automatically serializes data from native types in your language. You can work with familiar data types, such as Pandas DataFrames, without worrying about conversion. Serialization can be painful, especially if it is done with performance in mind. We serialize native types using our codecs so you don\u2019t have to worry about that.</p> <p>Refer to the Data serialization section of this documentation for more information.</p>","location":"sdk/introduction/#data-serialization-and-de-serialization"},{"title":"Multiple data types","text":"<p>The SDK allows you to attach any type of data to your timestamps, like Numbers, Strings or even raw Binary data. This gives the SDK the ability to adapt to any streaming application use case.</p> <p>Refer to the Multiple data types section of this documentation for more information.</p>","location":"sdk/introduction/#multiple-data-types"},{"title":"Message Broker configuration including authentication and authorization","text":"<p>Quix handles Kafka configuration efficiently and reliably. Our templates come with pre-configured certificates and connection settings. Many configuration settings are needed to use Kafka at its best, and the ideal configuration takes time! We take care of this in the SDK so you don\u2019t have to.</p> <p>Refer to the Broker configuration section of this documentation for more information.</p>","location":"sdk/introduction/#message-broker-configuration-including-authentication-and-authorization"},{"title":"Checkpointing","text":"<p>The SDK allows you to do manual checkpointing when you read data from a Topic. This provides the ability to inform the Message Broker that you have already processed messages up to one point, usually called a checkpoint.</p> <p>This is a very important concept when you are developing high performance streaming applications.</p> <p>Refer to the Checkpointing section of this documentation for more information.</p>","location":"sdk/introduction/#checkpointing"},{"title":"Horizontal scaling","text":"<p>The Quix SDK provides horizontal scale out of the box via the streaming context feature. This means a data scientist or data engineer does not have to implement parallel processing themselves. You can scale the processing models, from one replica to many and back to one, and use the callback system inside the SDK to ensure that your data load is always shared between your model replicas.</p> <p>Refer to the Horizontal scaling section of this documentation for more information.</p>","location":"sdk/introduction/#horizontal-scaling"},{"title":"Integrations","text":"<p>The SDK offers integrations out of the box, including data persistence and historic or real-time APIs with other systems. That means you don\u2019t have to implement them by yourself.</p> <p>Refer to the Integrations section of this documentation for more information.</p>","location":"sdk/introduction/#integrations"},{"title":"Portability","text":"<p>The Quix SDK is an abstraction layer over a concrete broker technology. You\u2019re not locked into a specific broker and can innovate over time.</p> <p>Refer to the Portability section of this documentation for more information.</p>","location":"sdk/introduction/#portability"},{"title":"Raw messages","text":"<p>The Quix SDK uses an internal protocol which is both data and speed optimized so we do encourage you to use it. For that you need to use the SDK on both producer ( writer ) and consumer ( reader ) sides.</p> <p>However, in some cases, you simply do not have the ability to run the Quix SDK on both sides.</p> <p>To cater for these cases we added the ability to both write and read the raw, unformatted, messages as byte array. This is giving you the freedom to implement the protocol as needed ( e.g. JSON, comma-separated rows ).</p>","location":"sdk/introduction/#raw-messages"},{"title":"Kafka and Quix SDK","text":"<p>The Quix SDK helps you to leverage Kafka\u2019s powerful features with ease.</p>","location":"sdk/kafka/"},{"title":"Why this is important","text":"<p>Kafka is a powerful but complex technology to master. Using the Quix SDK, you can leverage the power of Kafka without worrying about mastering it. There are just a couple of important concepts to grasp, the rest is handled in the background by the SDK.</p>","location":"sdk/kafka/#why-this-is-important"},{"title":"Concepts","text":"","location":"sdk/kafka/#concepts"},{"title":"Topic replica","text":"<p>Each topic can be set to replicate over the Kafka cluster for increased resilience, so a failure of a node will not cause downtime of your processing pipeline. For example, if you set replica to 2, every message you send to the topic will be replicated twice in the cluster.</p>  <p>Tip</p> <p>If you set replication to two, data streamed to the cluster is billed twice.</p>","location":"sdk/kafka/#topic-replica"},{"title":"Topic retention","text":"<p>Each topic has temporary storage. Every message sent to the topic will live in Kafka for a configured amount of time or size. That means that a consumer can join the topic later and still consume messages. If your processing pipeline has downtime, no data is lost.</p>","location":"sdk/kafka/#topic-retention"},{"title":"Topic partitions","text":"<p>Each Kafka topic is created with a number of partitions. You can add more partitions later, but you can\u2019t remove them. Each partition is an independent queue that preserves the order of messages. The Quix SDK restricts all messages inside one stream to the same single partition. That means that inside one stream, a consumer can rely on the order of messages. Partitions are spread across your Kafka cluster, over different Kafka nodes, for improved performance.</p>","location":"sdk/kafka/#topic-partitions"},{"title":"Redistribution of load","text":"<p>Streams are redistributed over available partitions. With an increasing number of streams, each partition will end up with approximately the same number of streams.</p>  <p>Warning</p> <p>The number of partitions sets the limit for how many parallel instances of one model can process the topic. For example: A topic with three partitions can be processed with up to 3 instances of a model. The fourth instance will remain idle.</p>","location":"sdk/kafka/#redistribution-of-load"},{"title":"Consumer group","text":"<p>The Consumer group is a concept of how to horizontally scale topic processing. Each consumer group has an ID, which you set when opening a connection to the topic:</p> <pre><code>output_topic = client.open_input_topic(\"{topic}\",\"{your-consumer-group-id}\")\n</code></pre> <p>If you deploy this model with a replica set to 3, your model will be deployed in three instances as members of one consumer group. This group will share partitions between each other and therefore share the load.</p> <ul> <li> <p>If you increase the number, some partitions will get reassigned to     new instances of the model.</p> </li> <li> <p>If you decrease the number, partitions left by leaving instances get     reassigned to remaining processing instances in the consumer group.</p> </li> </ul>","location":"sdk/kafka/#consumer-group"},{"title":"Checkpointing","text":"<p>We can think of Kafka temporary storage as a processing queue for each partition. Consumer groups read from this queue and regularly commit offsets to track which messages were already processed. By default, this is done by the Quix SDK automatically, but you can override that by manually committing an offset when you are done processing a set of rows.</p> <pre><code>input_topic = client.open_input_topic('Telemetry', commit_settings=CommitMode.Manual)\ninput_topic.commit()\n</code></pre> <p>The consumer group is playing an important role here as offset commits are associated with the consumer group ID. That means that if you connect to the same topic with a different consumer group ID, the model will start reading from the start of the Kafka queue.</p>  <p>Tip</p> <p>If you want to consume data from the topic locally for debugging purposes, and the model is deployed in the Quix serverless environment at the same time, make sure that you change consumer group ID to prevent clashing with the cloud deployment.</p>   <p>Note</p> <p>When you open a topic you can also choose where to start reading data from. Either read all the data from the start or only read the new data as it arrives. Read more here</p>","location":"sdk/kafka/#checkpointing"},{"title":"Data Grouping","text":"<p>Topics group data streams from a single type of source. The golden rule for maximum performance is to always maintain one schema per topic.</p> <p>For example:</p> <ul> <li> <p>For connected car data you could create individual topics to group     data from different systems like the engine, transmission,     electronics, chassis, infotainment systems.</p> </li> <li> <p>For games you might create individual topics to separate player,     game and machine data.</p> </li> <li> <p>For consumer apps you could create a topic each source i.e one for     your IOS app, one for your Android app, and one for your web app.</p> </li> <li> <p>For live ML pipelines you\u2019ll want to create a topic for each stage     of the pipeline ie raw-data-topic \u2192 cleaning model \u2192     clean-data-topic \u2192 ML model \u2192 results topic</p> </li> </ul>","location":"sdk/kafka/#data-grouping"},{"title":"Data Governance","text":"<p>Topics are key to good data governance. Use them to organise your data by:</p> <ul> <li> <p>Group data streams by type or source.</p> </li> <li> <p>Use separate topics for raw, clean or processed data.</p> </li> <li> <p>Create prototyping topics to publish results of models in     development.</p> </li> </ul>","location":"sdk/kafka/#data-governance"},{"title":"Scale","text":"<p>Topics automatically scale. We have designed the underlying infrastructure to automatically stream any amount of data from any number of sources. With Quux you can connect one source - like a car, wearable or web app - to do R\\&amp;D, then scale your solution to millions of cars, wearables or apps in production, all on the same topic.</p>","location":"sdk/kafka/#scale"},{"title":"Security","text":"<p>Our topics are secured with industry standard SSL data encryption and SASL ACL authorisation and authentication. You can safely send data over public networks and trust that it is encrypted in-flight.</p>","location":"sdk/kafka/#security"},{"title":"Processing data","text":"<p>The Quix SDK is specifically designed to make real time data processing very easy. We provide high-performance technology, inherited from F1, in a way that anybody with basic development skills can understand and use it very quickly.</p>  <p>Tip</p> <p>The Quix Portal offers you easy-to-use, auto-generated examples for reading, writing, and processing data. These examples work directly with your workspace Topics. You can deploy these examples in our serverless environment with just a few clicks. For a quick test of the capabilities of the SDK, we recommend starting with those auto-generated examples.</p>  <p>Other streaming platforms are tied to a narrow set of functions, queries, or syntax to set up a data processor or Model. This limited approach is often only suitable for some use cases, and tends to have a steep learning curve. The feature limitations and time investment required form a barrier to entry for inexperienced developers and Data scientists alike.</p> <p>Our approach is simpler and far more powerful than other streaming solutions. So much so that we can\u2019t show you any SDK related functions here because you literally don\u2019t need them if you use the Quix SDK.</p> <p>With the Quix SDK, you are not tied to complicated Functions, Lambdas, Maps or Query libraries to be able to deploy and process data in real time. You just need to know how to read and write data with the SDK \u2014 that\u2019s it, the rest is up to you and your imagination.</p> <p>Let\u2019s see some examples of how to read and write data in a Data processor using the Quix SDK. We just read data from the message broker, process it, and write it back to the stream.</p> Python - Data FramePython - PlainC#   <pre><code># Callback triggered for each new data frame\ndef on_parameter_data_handler(data: ParameterData):\n\n    df = data.to_panda_frame()  # Input data frame\n    output_df = pd.DataFrame()\n    output_df[\"time\"] = df[\"time\"]\n    output_df[\"TAG__LapNumber\"] = df[\"TAG__LapNumber\"]\n\n    # If braking force applied is more than 50%, we mark HardBraking with True\n    output_df[\"HardBraking\"] = df.apply(lambda row: \"True\" if row.Brake &gt; 0.5 else \"False\", axis=1)\n\n    stream_output.parameters.write(output_df)  # Send data back to the stream\n</code></pre>   <pre><code># Callback triggered for each new data frame\ndef on_parameter_data_handler(data: ParameterData):\n\n    for row in data.timestamps:\n        # If braking force applied is more than 50%, we mark HardBraking with True\n        hard_braking = row.parameters[\"Brake\"].numeric_value &gt; 0.5\n\n        stream_output.parameters \\\n            .add_timestamp(row.timestamp) \\\n            .add_tag(\"LapNumber\", row.tags[\"LapNumber\"]) \\\n            .add_value(\"HardBraking\", hard_braking) \\\n            .write()\n</code></pre>   <pre><code>buffer.OnRead += (data) =&gt;\n{\n    var outputData = new ParameterData();\n\n    // We calculate mean value for each second of data to effectively down-sample source topic to 1Hz.\n    outputData.AddTimestamp(data.Timestamps.First().Timestamp)\n        .AddValue(\"ParameterA 10Hz\", data.Timestamps.Average(s =&gt; s.Parameters[\"ParameterA\"].NumericValue.GetValueOrDefault()))\n        .AddValue(\"ParameterA source frequency\", data.Timestamps.Count);\n\n    // Send data back to the stream\n    streamOutput.Parameters.Write(outputData);\n};\n</code></pre>    <p>So, because you are not tied to any narrow processing architecture, you can use any methods, classes or libraries that you are already familiar with to implement your model or data processor.</p> <p>Check the complete code example in GitHub for further information.</p>","location":"sdk/process/"},{"title":"Set up local environment","text":"<p>The Quix Portal offers you an online IDE, ready to use without any additional setup to develop or deploy your applications. However, you might sometimes want to work with your application in a local IDE.</p> <p>In such cases, Python development needs some setup before you can use our SDK. In this section are detailed instructions of how to set up your Python environment on Linux, Mac or Windows.</p>  <p>Note</p> <p>This guide assumes you have already cloned a project of your own or download a library sample provided within the platform. In addition some stages may require files from GitHub.</p>   <p>Warning</p> <p>Make sure you\u2019re using Python version 3.7&gt;=, \\&lt;3.9</p>","location":"sdk/python-setup/"},{"title":"Install Dependencies","text":"<p>To get started, install the SDK dependencies.</p> MacOS IntelMacOS M1WindowsLinux   <p>You can install dependencies via a script or manually.</p> <p>Install dependencies via script</p> <p>Download the script named <code>quix-dependency-installer-mac.sh</code> from GitHub, which installs all necessary requirements. Download the project then run the script by copy-pasting the following into a terminal from the project\u2019s top directory.</p> <pre><code>chmod +x ./quix-dependency-installer-mac.sh &amp;&amp; ./quix-dependency-installer-mac.sh\n</code></pre> <p>Install dependencies manually</p> <ul> <li> <p>Install and configure PythonNet dependencies</p> <ul> <li> <p>Install the Brew package manager (from     brew.sh). To add brew to your path:</p> <pre><code>echo \"export PATH=/usr/local/bin:$PATH\" &gt;&gt; ~/.bash_profile &amp;&amp; source ~/.bash_profile &amp;&amp; echo \"Worked\" || echo \"Failed\"\n</code></pre> </li> <li> <p>Install Mono Version</p> <pre><code>brew install mono\necho \"export PATH=/Library/Frameworks/Mono.framework/Versions/Current/Commands:$PATH\" &gt;&gt; ~/.bash_profile &amp;&amp; source ~/.bash_profile &amp;&amp; echo \"Worked\" || echo \"Failed\"\necho \"export PKG_CONFIG_PATH=/Library/Frameworks/Mono.framework/Versions/Current/lib/pkgconfig:$PKG_CONFIG_PATH\" &gt;&gt; ~/.bash_profile &amp;&amp; source ~/.bash_profile &amp;&amp; echo \"Worked\" || echo \"Failed\"\n</code></pre> </li> </ul> </li> <li> <p>Additional things to install:</p> <pre><code>brew install pkg-config\npython3 -m pip install wheel\npython3 -m pip install pycparser\n</code></pre> </li> </ul>   <p>You can install dependencies via a script or manually.</p> <p>Install dependencies via script</p> <p>Download the script named <code>quix-dependency-installer-mac.sh</code> from GitHub, which installs all necessary requirements. Download the project then run the script by copy-pasting the following into a terminal from the project\u2019s top directory.</p> <pre><code>chmod +x ./quix-dependency-installer-mac.sh &amp;&amp; ./quix-dependency-installer-mac.sh\n</code></pre> <p>Install dependencies manually</p> <ul> <li> <p>Install and configure PythonNet dependencies</p> <ul> <li> <p>Install the Brew package manager (from     brew.sh). To add brew to your path:</p> <pre><code>echo \"export PATH=/opt/homebrew/bin:$PATH\" &gt;&gt; ~/.bash_profile &amp;&amp; source ~/.bash_profile &amp;&amp; echo \"Worked\" || echo \"Failed\"\n</code></pre> </li> <li> <p>Install rosetta</p> <pre><code>sudo softwareupdate --install-rosetta --agree-to-license\n</code></pre> </li> <li> <p>Install Mono Version</p> <pre><code>curl https://download.mono-project.com/archive/6.12.0/macos-10-universal/MonoFramework-MDK-6.12.0.122.macos10.xamarin.universal.pkg -O\nmonoPkgPath=./MonoFramework-MDK-6.12.0.122.macos10.xamarin.universal.pkg\nsudo installer -pkg $monoPkgPath -target /\necho \"export PATH=/Library/Frameworks/Mono.framework/Versions/Current/Commands:$PATH\" &gt;&gt; ~/.bash_profile &amp;&amp; source ~/.bash_profile &amp;&amp; echo \"Worked\" || echo \"Failed\"\necho \"export PKG_CONFIG_PATH=/Library/Frameworks/Mono.framework/Versions/Current/lib/pkgconfig:$PKG_CONFIG_PATH\" &gt;&gt; ~/.bash_profile &amp;&amp; source ~/.bash_profile &amp;&amp; echo \"Worked\" || echo \"Failed\"\n</code></pre> </li> </ul> </li> <li> <p>Additional things to install:</p> <pre><code>brew install pkg-config\npython3 -m pip install wheel\npython3 -m pip install pycparser\n</code></pre> </li> </ul>   <p>Install the latest .Net Core runtime (https://dotnet.microsoft.com/download/dotnet-core/current/runtime)</p>    <p>Note</p> <p>Because of the variability between linux distributions we highly recommend using a Docker environment ( see Docker setup ) in case you experience any issues during the installation.</p>  <ul> <li> <p>Install and configure PythonNet dependencies - Ubuntu 20.04</p> <ul> <li>Install Mono version 6.10.0</li> </ul> </li> </ul>  <pre><code>sudo apt install gnupg ca-certificates\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF\necho \"deb https://download.mono-project.com/repo/ubuntu stable-focal/snapshots/6.10.0 main\" | sudo tee /etc/apt/sources.list.d/mono-official-stable.list &amp;&amp; apt-get update\nsudo apt update\nsudo apt install mono-devel\n</code></pre> <p>For other linux distributions, please see the Mono current release page and older Releases page</p> <ul> <li>Additional things to install:</li> </ul>  <pre><code>sudo apt install python3-pip clang libglib2.0-0 build-essential librdkafka-dev\npython3 -m pip install pycparser\n</code></pre> <p>In some distributions librdkafka-dev will not be readily available and guide (https://github.com/edenhill/librdkafka) must be followed. For ubuntu 20.04, the following steps are readily curated for you:</p> <pre><code>sudo apt-get install -y wget software-properties-common\nwget -qO - https://packages.confluent.io/deb/7.2/archive.key | sudo apt-key add -\nsudo add-apt-repository \"deb [arch=amd64] https://packages.confluent.io/deb/7.2 stable main\"\nsudo add-apt-repository \"deb https://packages.confluent.io/clients/deb $(lsb_release -cs) main\"\nsudo apt-get update\nsudo apt install librdkafka-dev -y\n</code></pre>","location":"sdk/python-setup/#install-dependencies"},{"title":"Create New Python Environment","text":"<p>It\u2019s good practice to use a python virtual environment and it is doubly true when using the Quix streaming library. The library currently relies on some dll redirecting, which is achieved by adding a file to your python environment. This is done automatically, but if you have other python application(s)/package(s) that also rely on similar redirection then a virtual environment is advised.</p> MacOSWindowsLinux   <p>To create a new virtual environment, execute the following in a terminal at your desired location (such as the root folder of the downloaded sample):</p> <pre><code>python3 -m pip install virtualenv\npython3 -m virtualenv env --python=python3.8.7\nchmod +x ./env/bin/activate\nsource ./env/bin/activate\n</code></pre> <p>You will know you succeeded in activating the environment if your terminal line starts with (env). Future steps will assume you have the virtual environment activated or are happy to install globally.</p>   <p>To create a new virtual environment, execute the following in a terminal at your desired location (such as the root folder of the downloaded sample):</p> <pre><code>pip install virtualenv\npython -m virtualenv env --python=python3.8\n\"env/Scripts/activate\"\n</code></pre> <p>You will know you succeeded in activating the environment if your command line starts with (env). Future steps will assume you have the virtual environment activated or are happy to install globally.</p>  <p>Note</p> <p>You might need to use a new command line after installing Python, because PATH isn\u2019t refreshed for existing command lines when something is installed.</p>    <p>To create a new virtual environment, execute the following in a terminal at your desired location:</p> <pre><code>python3 -m pip install virtualenv\npython3 -m virtualenv env --python=python3.8\nchmod +x ./env/bin/activate\nsource ./env/bin/activate\n</code></pre> <p>You will know you succeeded in activating the environment if your terminal line starts with (env). Future steps will assume you have the virtual environment activated or are happy to install globally.</p>","location":"sdk/python-setup/#create-new-python-environment"},{"title":"Install code requirements","text":"MacOSWindowsLinux   <p>In the same terminal you activated the virtual environment, navigate to the folder where <code>requirements.txt</code> (in the sample you downloaded) is located and execute the following:</p> <pre><code>python3 -m pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/\n</code></pre>   <p>In the same console you activated the virtual enviornment, navigate to the folder where <code>requirements.txt</code> (in the sample you downloaded) is located and execute the following:</p> <pre><code>pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/\n</code></pre>   <p>In the same terminal you activated the virtual environment, navigate to the folder where <code>requirements.txt</code> (in the sample you downloaded) is located and execute the following:</p> <pre><code>python3 -m pip install -r requirements.txt --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/\n</code></pre>    <p>Your environment should be ready to run the code!</p> MacOSWindowsLinux   <pre><code>python3 main.py\n</code></pre>   <pre><code>python main.py\n</code></pre>   <pre><code>python3 main.py\n</code></pre>","location":"sdk/python-setup/#install-code-requirements"},{"title":"Reading data","text":"<p>The Quix SDK allows you to read data in real time from the existing streams of your Topics.</p> <p>All the necessary code to read data from your Quix Workspace is auto-generated when you create a project using the existing templates. In this section, we explain more in-depth how to read data using the Quix SDK.</p>  <p>Tip</p> <p>The Quix Portal offers you easy-to-use, auto-generated examples for reading, writing, and processing data. These examples work directly with your workspace Topics. You can deploy these examples in our serverless environment with just a few clicks. For a quick test of the capabilities of the SDK, we recommend starting with those auto-generated examples.</p>","location":"sdk/read/"},{"title":"Connect to Quix","text":"<p>In order to start reading data from Quix you need an instance of the Quix client, <code>QuixStreamingClient</code>. This is the central point where you interact with the main SDK operations.</p> <p>You can create an instance of <code>QuixStreamingClient</code> using the proper constructor of the SDK.</p> PythonC#   <pre><code>client = QuixStreamingClient()\n</code></pre>   <pre><code>var client = new Quix.Sdk.Streaming.QuixStreamingClient();\n</code></pre>    <p>You can find more advanced information on how to connect to Quix in the Connect to Quix section.</p>","location":"sdk/read/#connect-to-quix"},{"title":"Open a topic for reading","text":"<p>Topics are the default environment for input/output operations on Quix.</p> <p>In order to access that topic for reading you need an instance of <code>InputTopic</code>. This instance allow you to read all the incoming streams on the specified Topic. You can create an instance of <code>InputTopic</code> using the client\u2019s <code>open_input_topic</code> method, passing the <code>TOPIC_ID</code> or the <code>TOPIC_NAME</code> as a parameter.</p> PythonC#   <pre><code>input_topic = client.open_input_topic(TOPIC_ID)\n</code></pre>   <pre><code>var inputTopic = client.OpenInputTopic(TOPIC_ID);\n</code></pre>","location":"sdk/read/#open-a-topic-for-reading"},{"title":"Consumer group","text":"<p>The Consumer group is a concept used when you want to scale horizontally. Each consumer group is identified using an ID, which you set optionally when opening a connection to the topic for reading:</p> PythonC#   <pre><code>input_topic = client.open_input_topic(\"{topic}\",\"{your-consumer-group-id}\")\n</code></pre>   <pre><code>var inputTopic = client.OpenInputTopic(\"{topic}\",\"{your-consumer-group-id}\");\n</code></pre>    <p>When you want to enable horizontal scalability, all the replicas of your process should use the same <code>ConsumerId</code>. This is how the message broker knows that all the replicas of your process want to share the load of the incoming streams between replicas. Each replica will receive only a subset of the streams incoming to the Input Topic.</p>  <p>Warning</p> <p>If you want to consume data from the topic locally for debugging purposes, and the model is also deployed in the Quix serverless environment, make sure that you change the consumer group ID to prevent clashing with the cloud deployment. If the clash happens, only one instance will be able to read data of the Stream at a time, and you will probably notice that your code is not receiving data at some point, either locally or in the cloud environment.</p>","location":"sdk/read/#consumer-group"},{"title":"Reading Streams","text":"PythonC#   <p>Once you have the <code>InputTopic</code> instance you can start reading streams. For each stream received to the specified topic, <code>InputTopic</code> will execute the event <code>on_stream_received</code>. You can attach a callback to this event to execute code that reacts when you receive a new Stream. For example, the following code prints the StreamId for each <code>newStream</code> received on that Topic:</p> <pre><code>def read_stream(new_stream: StreamReader):\n    print(\"New stream read:\" + new_stream.stream_id)\n\ninput_topic.on_stream_received += read_stream\ninput_topic.start_reading()\n</code></pre>   <p>Once you have the <code>InputTopic</code> instance you can start reading streams. For each stream received to the specified topic, <code>InputTopic</code> will execute the event <code>OnStreamReceived</code>. You can attach a callback to this event to execute code that reacts when you receive a new Stream. For example the following code prints the StreamId for each <code>newStream</code> received on that Topic:</p> <pre><code>inputTopic.OnStreamReceived += (s, newStream) =&gt;\n{\n    Console.WriteLine($\"New stream read: {newStream.StreamId}\");\n};\n\ninputTopic.StartReading();\n</code></pre>     <p>Tip</p> <p>The <code>StartReading</code> method indicates to the SDK the moment to start reading streams and data from your Topic. This should normally happen after you\u2019ve registered callbacks for all the events you want to listen to.</p>","location":"sdk/read/#reading-streams"},{"title":"Reading Parameter Data","text":"<p>You can read real-time data from Streams using the <code>on_read</code> event of the <code>StreamReader</code> instance received in the previous callback when you receive a new stream in your Topic.</p> <p>For instance, in the following example we read and print the first timestamp and value of the parameter <code>ParameterA</code> received in the ParameterData packet:</p> PythonC#   <pre><code>def on_stream_received_handler(new_stream: StreamReader):\n\n    def on_parameter_data_handler(data: ParameterData):\n\n        timestamp = data.timestamps[0].timestamp\n        num_value = data.timestamps[0].parameters['ParameterA'].numeric_value\n        print(\"ParameterA - \" + str(timestamp) + \": \" + str(num_value))\n\n    new_stream.on_read += on_parameter_data_handler\n\ninput_topic.on_stream_received += on_stream_received_handler\ninput_topic.start_reading()\n</code></pre>   <pre><code>inputTopic.OnStreamReceived += (s, streamReader) =&gt;\n{\n    streamReader.Parameters.OnRead += parameterData =&gt;\n    {\n        var timestamp = parameterData.Timestamps[0].Timestamp;\n        var numValue = parameterData.Timestamps[0].Parameters[\"ParameterA\"].NumericValue;\n        Console.WriteLine($\"ParameterA - {timestamp}: {numValue}\");\n    };\n};\n\ninputTopic.StartReading();\n</code></pre>    <p>We use ParameterData packages to read data from the stream. This class handles reading and writing of time series data. The Quix SDK provides multiple helpers for reading and writing data using ParameterData.</p>  <p>Tip</p> <p>If you\u2019re using Python you can convert ParameterData to a Pandas DataFrames or read them directly from the SDK. Refer to Using Data Frames for more information.</p>","location":"sdk/read/#reading-parameter-data"},{"title":"Parameter Data format","text":"<p>ParameterData is the formal class in the SDK which represents a time series data packet in memory.</p> <p>ParameterData consists of a list of Timestamps with their corresponding Parameter Names and Values for each timestamp.</p> <p>You should imagine a Parameter Data as a table where the Timestamp is the first column of that table and where the Parameters are the columns for the Values of that table.</p>    Timestamp Speed Gear     1 120 3   2 123 3   3 125 3   6 110 2    <p>An example of ParameterData</p> <p>You can use the <code>timestamps</code> property of a ParameterData instance to access each row of that table, and the <code>parameters</code> property to access the values of that timestamp.</p> <p>The Quix SDK supports Numeric, String, and Binary values and you should use the proper property depending of the value type of your Parameter:</p> PythonC#   <ul> <li> <p><code>numeric_value</code>: Returns the Numeric value of the Parameter,     represented as a <code>float</code> type.</p> </li> <li> <p><code>string_value</code>: Returns the String value of the Parameter,     represented as a <code>string</code> type.</p> </li> <li> <p><code>binary_value</code>: Returns the Binary value of the Parameter,     represented as a <code>bytearray</code> type.</p> </li> </ul>   <ul> <li> <p><code>NumericValue</code>: Returns the Numeric value of the Parameter,     represented as a <code>double</code> type.</p> </li> <li> <p><code>StringValue</code>: Returns the String value of the Parameter,     represented as a <code>string</code> type.</p> </li> <li> <p><code>BinaryValue</code>: Returns the Binary value of the Parameter,     represented as an array of <code>byte</code>.</p> </li> </ul>    <p>This is a simple example showing how to read Speed values of the ParameterData used in the previous example:</p> PythonC#   <pre><code>for ts in data.timestamps:\n    timestamp = ts.timestamp_nanoseconds\n    numValue = ts.parameters['Speed'].numeric_value\n    print(\"Speed - \" + str(timestamp) \": \" + str(numValue))\n</code></pre>   <pre><code>foreach (var timestamp in data.Timestamps)\n{\n       var timestamp = timestamp.TimestampNanoseconds;\n       var numValue = timestamp.Parameters[\"Speed\"].NumericValue;\n       Console.WriteLine($\"Speed - {timestamp}: {numValue}\");\n}\n</code></pre>    <p>output:</p> <pre><code>Speed - 1: 120\nSpeed - 2: 123\nSpeed - 3: 125\nSpeed - 6: 110\n</code></pre>","location":"sdk/read/#parameter-data-format"},{"title":"Buffer","text":"<p>The Quix SDK provides you with a programmable buffer which you can tailor to your needs. Using buffers to read data enhances the throughput of your application. This helps you to develop Models with a high performance throughput.</p> <p>You can use the <code>buffer</code> property embedded in the <code>Parameters</code> property of your <code>stream</code>, or create a separate instance of that buffer using the <code>create_buffer</code> method:</p> PythonC#   <pre><code>buffer = newStream.parameters.create_buffer()\n</code></pre>   <pre><code>var buffer = newStream.Parameters.CreateBuffer();\n</code></pre>    <p>You can configure a buffer\u2019s input requirements using built-in properties. For example, the following configuration means that the Buffer will release a packet when the time span between first and last timestamp inside the buffer reaches 100 milliseconds:</p> PythonC#   <pre><code>buffer.time_span_in_milliseconds = 100\n</code></pre>   <pre><code>buffer.TimeSpanInMilliseconds = 100;\n</code></pre>    <p>Reading data from that buffer is as simple as using its <code>OnRead</code> event. For each ParameterData packet released from the buffer, the SDK will execute the <code>OnRead</code> event with the parameter data as a given parameter. For example the following code prints the ParameterA value of the first timestamp of each packet released from the buffer:</p> PythonC#   <pre><code>def on_parameter_data_handler(data: ParameterData):\n    timestamp = data.timestamps[0].timestamp\n    num_value = data.timestamps[0].parameters['ParameterA'].numeric_value\n    print(\"ParameterA - \" + str(timestamp) + \": \" + str(num_value))\n\nbuffer.on_read += on_parameter_data_handler\n</code></pre>   <pre><code>buffer.OnRead += (data) =&gt;\n{\n    var timestamp = data.Timestamps[0].Timestamp;\n    var numValue = data.Timestamps[0].Parameters[\"ParameterA\"].NumericValue;\n    Console.WriteLine($\"ParameterA - {timestamp}: {numValue}\");\n};\n</code></pre>    <p>You can configure multiple conditions to determine when the Buffer has to release data, if any of these conditions become true, the buffer will release a new packet of data and that data is cleared from the buffer:</p> PythonC#   <ul> <li> <p><code>buffer.buffer_timeout</code>: The maximum duration in milliseconds     for which the buffer will be held before releasing the data. A     packet of data is released when the configured timeout value has     elapsed from the last data received in the buffer.</p> </li> <li> <p><code>buffer.packet_size</code>: The maximum packet size in terms of number     of timestamps. Each time the buffer has this amount of     timestamps, the packet of data is released.</p> </li> <li> <p><code>buffer.time_span_in_nanoseconds</code>: The maximum time between     timestamps in nanoseconds. When the difference between the     earliest and latest buffered timestamp surpasses this number,     the packet of data is released.</p> </li> <li> <p><code>buffer.time_span_in_milliseconds</code>: The maximum time between     timestamps in milliseconds. When the difference between the     earliest and latest buffered timestamp surpasses this number,     the packet of data is released. Note: This is a millisecond     converter on top of <code>time_span_in_nanoseconds</code>. They both work     with the same underlying value.</p> </li> <li> <p><code>buffer.custom_trigger_before_enqueue</code>: A custom function which     is invoked before adding a new timestamp to the buffer. If     it returns true, the packet of data is released before adding     the timestamp to it.</p> </li> <li> <p><code>buffer.custom_trigger</code>: A custom function which is invoked     after adding a new timestamp to the buffer. If it returns     true, the packet of data is released with the entire buffer     content.</p> </li> <li> <p><code>buffer.filter</code>: A custom function to filter the incoming data     before adding it to the buffer. If it returns true, data is     added, otherwise it isn\u2019t.</p> </li> </ul>   <ul> <li> <p><code>Buffer.BufferTimeout</code>: The maximum duration in milliseconds for     which the buffer will be held before releasing the data. A     packet of data is released when the configured timeout value has     elapsed from the last data received in the buffer.</p> </li> <li> <p><code>Buffer.PacketSize</code>: The maximum packet size in terms of number     of timestamps. Each time the buffer has this amount of     timestamps, the packet of data is released.</p> </li> <li> <p><code>Buffer.TimeSpanInNanoseconds</code>: The maximum time between     timestamps in nanoseconds. When the difference between the     earliest and latest buffered timestamp surpasses this number,     the packet of data is released.</p> </li> <li> <p><code>Buffer.TimeSpanInMilliseconds</code>: The maximum time between     timestamps in milliseconds. When the difference between the     earliest and latest buffered timestamp surpasses this number,     the packet of data is released. Note: This is a millisecond     converter on top of <code>time_span_in_nanoseconds</code>. They both work     with the same underlying value.</p> </li> <li> <p><code>Buffer.CustomTriggerBeforeEnqueue</code>: A custom function which is     invoked before adding a new timestamp to the buffer. If it     returns true, the packet of data is released before adding the     timestamp to it.</p> </li> <li> <p><code>Buffer.CustomTrigger</code>: A custom function which is invoked     after adding a new timestamp to the buffer. If it returns     true, the packet of data is released with the entire buffer     content.</p> </li> <li> <p><code>Buffer.Filter</code>: A custom function to filter the incoming data     before adding it to the buffer. If it returns true, data is     added, otherwise it isn\u2019t.</p> </li> </ul>","location":"sdk/read/#buffer"},{"title":"Examples","text":"<p>This buffer configuration will send data every 100ms or, if no data is buffered in the 1 second timeout period, it will empty the buffer and send the pending data anyway.</p> PythonC#   <pre><code>stream.parameters.buffer.packet_size = 100\nstream.parameters.buffer.buffer_timeout = 1000\n</code></pre>   <pre><code>stream.Parameters.Buffer.PacketSize = 100;\nstream.Parameters.Buffer.BufferTimeout = 1000;\n</code></pre>    <p>This buffer configuration will send data every 100ms window or if critical data arrives.</p> PythonC#   <pre><code>buffer.time_span_in_milliseconds = 100\nbuffer.custom_trigger = lambda data: data.timestamps[0].tags[\"is_critical\"] == 'True'\n</code></pre>   <pre><code>stream.Parameters.Buffer.TimeSpanInMilliseconds = 100;\nstream.Parameters.Buffer.CustomTrigger = data =&gt; data.Timestamps[0].Tags\n[\"is_critical\"] == \"True\";\n</code></pre>","location":"sdk/read/#examples"},{"title":"Using Data Frames","text":"<p>If you use the Python version of the SDK you can use Pandas DataFrames for reading and writing ParameterData to Quix. The Pandas DataFrames format is just a representation of ParameterData format, where the Timestamp is mapped to a column named <code>time</code> and the rest of the parameters are mapped as columns named as the ParameterId of the parameter. Tags are mapped as columns with the prefix <code>TAG__</code> and the TagId of the tag.</p> <p>For example, the following ParameterData:</p>    Timestamp CarId (tag) Speed Gear     1 car-1 120 3   2 car-2 123 3   3 car-1 125 3   6 car-2 110 2    <p>An example of ParameterData</p> <p>Is represented as the following Pandas DataFrame:</p>    time TAG__CarId Speed Gear     1 car-1 120 3   2 car-2 123 3   3 car-1 125 3   6 car-2 110 2    <p>A representation of ParameterData in a Pandas DataFrame</p> <p>One simple way to read data from Quix using Pandas DataFrames is using the event <code>on_read_pandas</code> instead of the common event <code>on_read</code> when reading from a <code>stream</code>, or when reading data from a buffer:</p> <pre><code>def read_stream(new_stream: StreamReader):\n\n    buffer = new_stream.parameters.create_buffer()\n\n    def on_pandas_frame_handler(df: pd.DataFrame):\n        print(df.to_string())\n\n    buffer.on_read_pandas += on_pandas_frame_handler\n\ninput_topic.on_stream_received += read_stream\ninput_topic.start_reading()\n</code></pre> <p>Alternatively, you can always convert a ParameterData to a Pandas DataFrame using the method <code>to_panda_frame</code>:</p> <pre><code>def read_stream(new_stream: StreamReader):\n\n    buffer = new_stream.parameters.create_buffer()\n\n    def on_parameter_data_handler(data: ParameterData):\n\n        # read from input stream\n        df = data.to_panda_frame()\n        print(df.to_string())\n\n    buffer.on_read += on_parameter_data_handler\n\ninput_topic.on_stream_received += read_stream\ninput_topic.start_reading()\n</code></pre>  <p>Tip</p> <p>The conversions from ParameterData to Pandas DataFrames have an intrinsic cost overhead. For high-performance models using Pandas DataFrames, you probably want to use the <code>on_read_pandas</code> event provided by the SDK, which is optimized for doing as few conversions as possible.</p>","location":"sdk/read/#using-data-frames"},{"title":"Reading Events","text":"<p>EventData is the formal class in the SDK which represents an Event data packet in memory. EventData is meant to be used for time series data coming from sources that generate data at irregular intervals or without a defined structure.</p>","location":"sdk/read/#reading-events"},{"title":"Event Data format","text":"<p>EventData consists of a record with a Timestamp, an EventId and an EventValue.</p> <p>You should imagine a list of Event Data instances as a simple table of three columns where the Timestamp is the first column of that table and the EventId and EventValue are the second and third columns.</p>    Timestamp EventId EventValue     1 failure23 Gearbox has a failure   2 box-event2 Car has entered to the box   3 motor-off Motor has stopped   6 race-event3 Race has finished    <p>An example of a list of EventData</p> <p>Reading events from a stream is as easy as reading parameter data. In this case, the SDK does not use a Buffer because we don\u2019t need high performance throughput, but the way we read Event Data from a <code>Stream</code> is identical.</p> PythonC#   <pre><code>def on_event_data_handler(data: EventData):\n    print(\"Event read for stream. Event Id: \" + data.Id)\n\nnew_stream.events.on_read += on_event_data_handler\n</code></pre>   <pre><code>newStream.Events.OnRead += (data) =&gt;\n{\n    Console.WriteLine($\"Event read for stream. Event Id: {data.Id}\");\n};\n</code></pre>    <p>output:</p> <pre><code>Event read for stream. Event Id: failure23\nEvent read for stream. Event Id: box-event2\nEvent read for stream. Event Id: motor-off\nEvent read for stream. Event Id: race-event3\n</code></pre>","location":"sdk/read/#event-data-format"},{"title":"Committing / checkpointing","text":"<p>It is important to be aware of the commit concept when working with a broker. Committing allows one to mark how far data has been processed, also known as creating a checkpoint. In the event of a restart or rebalance, the client only processes messages from the last commit position. In Kafka this is equivalent to commits for a consumer group.</p> <p>Commits are done for each consumer group, so if you have several consumer groups in use, they do not affect each another when commiting to one of them.</p>  <p>Tip</p> <p>Commits are done at a partition level when you use Kafka as a Message Broker, which means that streams that belong to the same partition are committed using the same position. The SDK currently does not expose the option to subscribe to only specific partitions of a topic, but commits will only ever affect partitions that are currently assigned to your client.</p> <p>Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation of the Quix SDK. You mainly don\u2019t even need to worry about it because everything is abstracted within the Streaming Context feature of the SDK.</p>","location":"sdk/read/#committing-checkpointing"},{"title":"Automatic committing","text":"<p>By default, the SDK automatically commits messages for which all handlers returned at a regular default interval, which is every 5 seconds or 5,000 messages, whichever happens sooner. However this is subject to change.</p> <p>If you wish to use different automatic commit intervals, use the following code:</p> PythonC#   <pre><code>from quixstreaming import CommitOptions\n\ncommit_settings = CommitOptions()\ncommit_settings.commit_every = 100 # note, you can set this to none\ncommit_settings.commit_interval = 500 # note, you can set this to none\ncommit_settings.auto_commit_enabled = True\ninput_topic = client.open_input_topic('yourtopic', commit_settings=commit_settings)\n</code></pre>   <pre><code>var inputTopic = client.OpenInputTopic(topic, consumerGroup, new CommitOptions()\n{\n        CommitEvery = 100,\n        CommitInterval = 500,\n        AutoCommitEnabled = true // optional, defaults to true\n});\n</code></pre>    <p>The code above will commit every 100 processed messages or 500 ms, whichever is sooner.</p>","location":"sdk/read/#automatic-committing"},{"title":"Manual committing","text":"<p>Some use cases need manual committing to mark completion of work, for example when you wish to batch process data, so the frequency of commit depends on the data. This can be achieved by first enabling manual commit for the topic:</p> PythonC#   <pre><code>from quixstreaming import CommitMode\n\ninput_topic = client.open_input_topic('yourtopic', commit_settings=CommitMode.Manual)\n</code></pre>   <pre><code>client.OpenInputTopic(topic, consumerGroup, CommitMode.Manual);\n</code></pre>    <p>Then, whenever your commit condition fulfils, call:</p> PythonC#   <pre><code>input_topic.commit()\n</code></pre>   <pre><code>inputTopic.Commit();\n</code></pre>    <p>The piece of code above will commit anything \u2013 like parameter, event or metadata - read and served to you from the input topic up to this point.</p>","location":"sdk/read/#manual-committing"},{"title":"Commit callback","text":"<p>Whenever a commit occurs, an event is raised to let you know. This event is raised for both manual and automatic commits. You can subscribe to this event using the following code:</p> PythonC#   <pre><code>def on_committed_handler():\n    # your code doing something when committed to broker\n\ninput_topic.on_committed += on_committed_handler\n</code></pre>   <pre><code>inputTopic.OnCommitted += (sender, args) =&gt;\n{\n    //... your code \u2026\n};\n</code></pre>","location":"sdk/read/#commit-callback"},{"title":"Auto Offset Reset","text":"<p>You can control the offset that data is read from by optionally specifying AutoOffsetReset when you open the topic.</p> <p>When setting the AutoOffsetReset you can specify one of three options.</p>    Option Description     Latest Read only the latest data as it arrives, dont include older data   Earliest Read from the beginning, i.e. as much as possible   Error Throws exception if no previous offset is found    <p>The possible options for AutoOffsetReset</p> PythonC#   <pre><code>input_topic = client.open_input_topic(test_topic, auto_offset_reset=AutoOffsetReset.Latest)\nor\ninput_topic = client.open_input_topic(test_topic, auto_offset_reset=AutoOffsetReset.Earliest)\n</code></pre>   <pre><code>var inputTopic = client.OpenInputTopic(\"MyTopic\", autoOffset: AutoOffsetReset.Latest);\nor\nvar inputTopic = client.OpenInputTopic(\"MyTopic\", autoOffset: AutoOffsetReset.Earliest);\n</code></pre>","location":"sdk/read/#auto-offset-reset"},{"title":"Revocation","text":"<p>When working with a broker, you have a certain number of topic streams assigned to your consumer. Over the course of the client\u2019s lifetime, there may be several events causing a stream to be revoked, like another client joining or leaving the consumer group, so your application should be prepared to handle these scenarios in order to avoid data loss and/or avoidable reprocessing of messages.</p>  <p>Tip</p> <p>Kafka revokes entire partitions, but the SDK makes it easy to determine which streams are affected by providing two events you can listen to.</p> <p>Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation of the Quix SDK. You mainly don\u2019t even need to worry about it because everything is abstracted within the Streaming Context feature of the SDK.</p>","location":"sdk/read/#revocation"},{"title":"Streams revoking","text":"<p>One or more streams are about to be revoked from your client, but you have a limited time frame \u2013 according to your broker configuration \u2013 to react to this and even commit to the broker:</p> PythonC#   <pre><code>def on_revoking_handler():\n    # your code\n\ninput_topic.on_revoking += on_revoking_handler\n</code></pre>   <pre><code>inputTopic.OnRevoking += (sender, args) =&gt;\n    {\n        // ... your code ...\n    };\n</code></pre>","location":"sdk/read/#streams-revoking"},{"title":"Streams Revoked","text":"<p>One or more streams are revoked from your client. You can no longer commit to these streams, you can only handle the revocation in your client.</p> PythonC#   <pre><code>from quixstreaming import StreamReader\n\ndef on_streams_revoked_handler(readers: [StreamReader]):\n    for reader in readers:\n        print(\"Stream \" + reader.stream_id + \" got revoked\")\n\ninput_topic.on_streams_revoked += on_streams_revoked_handler\n</code></pre>   <pre><code>inputTopic.OnStreamsRevoked += (sender, revokedStreams) =&gt;\n    {\n        // revoked streams are provided to the handler\n    };\n</code></pre>","location":"sdk/read/#streams-revoked"},{"title":"Stream Closure","text":"<p>You can detect stream closure with the stream closed callback which receives the StreamEndType, to help determine the closure reason if required.</p> PythonC#   <pre><code>def on_stream_closed_handler(end_type: StreamEndType):\n        print(\"Stream closed with {}\".format(end_type))\n\nnew_stream.on_stream_closed += on_stream_closed_handler\n</code></pre>   <pre><code>inputTopic.OnStreamReceived += (s, streamReader) =&gt;\n{\n        streamReader.OnStreamClosed += (reader, type) =&gt;\n        {\n                Console.WriteLine(\"Stream closed with {0}\", type);\n        };\n};\n</code></pre>    <p>The StreamEndType can be one of:</p>    StreamEndType Description     Closed The stream was closed normally   Aborted The stream was aborted by your code for your own reasons   Terminated The stream was terminated unexpectedly while data was being written    <p>Possible end types</p>","location":"sdk/read/#stream-closure"},{"title":"Minimal example","text":"<p>This is a minimal code example you can use to read data from a topic using the Quix SDK.</p> PythonC#   <pre><code>from quixstreaming import *\nfrom quixstreaming.app import App\nfrom quixstreaming.models.parametersbufferconfiguration import ParametersBufferConfiguration\nimport sys\nimport signal\nimport threading\n\n# Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument.\nclient = QuixStreamingClient()\n\ninput_topic = client.open_input_topic(TOPIC_ID)\n\n# read streams\ndef read_stream(new_stream: StreamReader):\n\n    buffer = new_stream.parameters.create_buffer()\n\n    def on_parameter_data_handler(data: ParameterData):\n\n        df = data.to_panda_frame()\n        print(df.to_string())\n\n    buffer.on_read += on_parameter_data_handler\n\n# Hook up events before initiating read to avoid losing out on any data\ninput_topic.on_stream_received += read_stream\n\n# Hook up to termination signal (for docker image) and CTRL-C\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n\n# Handle graceful exit\nApp.run()\n</code></pre>   <pre><code>using System;\nusing System.Linq;\nusing System.Threading;\nusing Quix.Sdk.Streaming;\nusing Quix.Sdk.Streaming.Configuration;\nusing Quix.Sdk.Streaming.Models;\n\n\nnamespace ReadHelloWorld\n{\n    class Program\n    {\n        /// &lt;summary&gt;\n        /// Main will be invoked when you run the application\n        /// &lt;/summary&gt;\n        static void Main()\n        {\n            // Create a client which holds generic details for creating input and output topics\n            var client = new Quix.Sdk.Streaming.QuixStreamingClient();\n\n            using var inputTopic = client.OpenInputTopic(TOPIC_ID);\n\n            // Hook up events before initiating read to avoid losing out on any data\n            inputTopic.OnStreamReceived += (s, streamReader) =&gt;\n            {\n                Console.WriteLine($\"New stream read: {streamReader.StreamId}\");\n\n                var buffer = streamReader.Parameters.CreateBuffer();\n\n                buffer.OnRead += parameterData =&gt;\n                {\n                    Console.WriteLine(\n                        $\"ParameterA - {parameterData.Timestamps[0].Timestamp}: {parameterData.Timestamps.Average(a =&gt; a.Parameters[\"ParameterA\"].NumericValue)}\");\n                };\n            };\n\n            Console.WriteLine(\"Listening for streams\");\n\n            // Hook up to termination signal (for docker image) and CTRL-C and open streams\n            App.Run();\n\n            Console.WriteLine(\"Exiting\");\n        }\n    }\n}\n</code></pre>","location":"sdk/read/#minimal-example"},{"title":"Read raw kafka messages","text":"<p>The Quix SDK uses the message brokers' internal protocol for data transmission. This protocol is both data and speed optimized so we do encourage you to use it. For that you need to use the SDK on both producer ( writer ) and consumer ( reader ) sides.</p> <p>However, in some cases, you simply do not have the ability to run the Quix SDK on both sides and you need to have the ability to connect to the data in different ways.</p> <p>To cater for these cases we added the ability to read the raw, unformatted, messages. Using this feature you have the ability to access the raw, unmodified content of each Kafka message from the topic. The data is a byte array, giving you the freedom to implement the protocol as needed ( e.g. JSON, comma-separated rows ).</p> PythonC#   <pre><code>inp = client.open_raw_input_topic(TOPIC_ID)\n\ndef on_raw_message(msg):\n    #bytearray containing bytes received from kafka\n    data = msg.value\n\n    #broker metadata as dict\n    meta = msg.metadata\n\ninp.on_message_read += on_raw_message\ninp.start_reading()\n</code></pre>   <pre><code>var inp = client.OpenRawInputTopic(TOPIC_ID)\n\ninp.OnMessageRead += (message) =&gt;\n{\n    var data = (byte[])message.Value;\n};\n\ninp.StartReading()\n</code></pre>","location":"sdk/read/#read-raw-kafka-messages"},{"title":"State management","text":"<p>A Quix deployment and its underlying code can be restarted multiple times. This can happen because of either user intervention (manually stopping and starting the deployment) or because of a runtime error in the code. In the second case, where a runtime error is detected, the Quix Serverless Environment automatically detects the problem and restarts the underlying service in an attempt to recover from the fault.</p> <p>Due to the code being run in memory, each time a deployment restarts, internal variables will be reset. For example, if you were to calculate the count of the elements in the stream, this counter would get reset on each restart. The counter would then starts at the default variable not knowing what was the last known value in the state of previous run before program stopped working.</p> <p>The Quix SDK has state management built in to allow values to be used and persisted across restarts of a given deployment. We persist the state to dedicated and private key-value pair storage in your workspace. == Usage</p> <p>To use the SDK\u2019s state management feature create an instance of LocalFileStorage. This is in quixstreaming.state.localfilestorage. Then use the set, get, containsKey and clear methods to manipulate the state as needed.</p> PythonC#   <pre><code>from quixstreaming.state.localfilestorage import LocalFileStorage\n\nstorage = LocalFileStorage()\n\n#clear storage ( remove all keys )\nstorage.clear()\n\n#storage class supports handling of\n#   `str`, `int`, `float`, `bool`, `bytes`, `bytearray` types.\n\n#set value\nstorage.set(\"KEY1\", 12.51)\nstorage.set(\"KEY2\", \"str\")\nstorage.set(\"KEY3\", True)\nstorage.set(\"KEY4\", False)\n\n#check if the storage contains key\nstorage.containsKey(\"KEY1\")\n\n#get value\nvalue = storage.get(\"KEY1\")\n</code></pre>   <p>C# supports two ways to call the Storage API.</p> <ul> <li> <p>Synchronous</p> </li> <li> <p>Asynchronous ( methods are with Async suffix )</p> </li> </ul> <p>The Synchronous API. During a call to these synchronous methods, the program thread execution is blocked.</p> <pre><code>var storage = new LocalFileStorage();\n\n//clear storage ( remove all keys )\nawait storage.Clear();\n\n//set value to specific key\nawait storage.Set(\"KEY1\", 123);  //long\nawait storage.Set(\"KEY2\", 1.23); //double\nawait storage.Set(\"KEY3\", \"1.23\"); //string\nawait storage.Set(\"KEY4\", new byte[]{12,53,23}); //binary\nawait storage.Set(\"KEY5\", false); //boolean\n\n//check if the key exists\nawait storage.ContainsKey(\"KEY1\");\n\n//retrieve value from key\nawait storage.GetLong(\"KEY1\");\nawait storage.GetDouble(\"KEY2\");\nawait storage.GetString(\"KEY3\");\nawait storage.GetBinary(\"KEY4\");\nawait storage.GetBinary(\"KEY5\");\n\n//list all keys in the storage\nawait storage.GetAllKeys();\n</code></pre> <p>The asynchronous API in which methods do contain Async suffix. These methods use the Task-Based Asynchronous Pattern (TAP) and return Tasks. TAP allows us to use async / await and avoid blocking the main thread on longer-running operations. In this case internal I/O.</p> <pre><code>var storage = new LocalFileStorage();\n\n//clear storage ( remove all keys )\nawait storage.ClearAsync();\n\n//set value to specific key\nawait storage.SetAsync(\"KEY1\", 123);  //long\nawait storage.SetAsync(\"KEY2\", 1.23); //double\nawait storage.SetAsync(\"KEY3\", \"1.23\"); //string\nawait storage.SetAsync(\"KEY4\", new byte[]{12,53,23}); //binary\nawait storage.SetAsync(\"KEY5\", false); //boolean\n\n//check if the key exists\nawait storage.ContainsKeyAsync(\"KEY1\");\n\n//retrieve value from key\nawait storage.GetLongAsync(\"KEY1\");\nawait storage.GetDoubleAsync(\"KEY2\");\nawait storage.GetStringAsync(\"KEY3\");\nawait storage.GetBinaryAsync(\"KEY4\");\nawait storage.GetBinaryAsync(\"KEY5\");\n\n//list all keys in the storage\nawait storage.GetAllKeysAsync();\n</code></pre>","location":"sdk/state-management/"},{"title":"Writing data","text":"<p>You write data to Quix using streams in your topic. The Quix SDK allows you to create new streams, append data to existing streams, organize streams in folders, and add context data to the streams.</p> <p>All the necessary code to write data to your Quix Workspace is auto-generated when you create a project using the existing templates. In this section, we explain more in-depth how to write data using the Quix SDK.</p>  <p>Tip</p> <p>The Quix Portal offers you easy-to-use, auto-generated examples for reading, writing, and processing data. These examples work directly with your workspace Topics. You can deploy these examples in our serverless environment with just a few clicks. For a quick test of the capabilities of the SDK, we recommend starting with those auto-generated examples.</p>","location":"sdk/write/"},{"title":"Connect to Quix","text":"<p>In order to start writing data to Quix you need an instance of the Quix client, <code>QuixStreamingClient</code>. This is the central point where you interact with the main SDK operations.</p> <p>You can create an instance of <code>QuixStreamingClient</code> using the proper constructor of the SDK.</p> PythonC#   <pre><code>client = QuixStreamingClient()\n</code></pre>   <pre><code>var client = new Quix.Sdk.Streaming.QuixStreamingClient();\n</code></pre>    <p>You can find more advanced information on how to connect to Quix in the Connect to Quix section.</p>","location":"sdk/write/#connect-to-quix"},{"title":"Open a topic","text":"<p>Topics are the default environment for input/output real-time operations on Quix.</p> <p>In order to access that topic for writing you need an instance of <code>OutputTopic</code>. You can create an instance of <code>OutputTopic</code> using the client\u2019s <code>open_output_topic</code> method, passing the <code>TOPIC_ID</code> or the <code>TOPIC_NAME</code> as a parameter.</p> PythonC#   <pre><code>output_topic = client.open_output_topic(TOPIC_ID)\n</code></pre>   <pre><code>var outputTopic = client.OpenOutputTopic(TOPIC_ID);\n</code></pre>","location":"sdk/write/#open-a-topic"},{"title":"Create / Attach to a Stream","text":"<p>Streams are the central context of data in Quix. Streams make easy to manage, discover, and work with your data. They are key to good data governance in your organisation. Also, Streams are vital for parallelizing huge data loads with an infinite number of data sources.</p> <p>You can create as many streams as you want using the <code>create_stream</code> method of your <code>OutputTopic</code> instance.</p> PythonC#   <pre><code>stream = output_topic.create_stream()\n</code></pre>   <pre><code>var stream = outputTopic.CreateStream();\n</code></pre>    <p>A stream ID is auto-generated but you can also pass a <code>StreamId</code> to the method to append or update data of an existing stream.</p> PythonC#   <pre><code>stream = output_topic.create_stream(\"existing-stream-id\")\n</code></pre>   <pre><code>var stream = outputTopic.CreateStream(\"existing-stream-id\");\n</code></pre>","location":"sdk/write/#create-attach-to-a-stream"},{"title":"Stream Properties","text":"<p>As an option, you can add context to your streams by adding a name, some metadata, or a default location.</p> <p>You can add this metadata to a stream using the <code>Properties</code> options of the generated <code>stream</code> instance.</p> PythonC#   <pre><code>stream.properties.name = \"Hello World Python stream\"\nstream.properties.location = \"/test/location\"\nstream.properties.metadata[\"meta\"] = \"is\"\nstream.properties.metadata[\"working\"] = \"well\"\n</code></pre>   <pre><code>stream.Properties.Name = \"Hello World C# stream\";\nstream.Properties.Location = \"/test/location\";\nstream.Properties.Metadata[\"meta\"] = \"is\";\nstream.Properties.Metadata[\"working\"] = \"well\";\n</code></pre>","location":"sdk/write/#stream-properties"},{"title":"Stream Name","text":"<p>The stream name is the display name of your stream in the platform. If you specify one, Quix will use it instead of the Stream Id to represent your stream inside the platform.</p> <p>For example, the following name:</p> PythonC#   <pre><code>stream.properties.name = \"Hello World my first stream\"\n</code></pre>   <pre><code>stream.Properties.Name = \"Hello World my first stream\";\n</code></pre>    <p>Would result in this visualization in the list of streams of your workspace:</p> <p></p>","location":"sdk/write/#stream-name"},{"title":"Stream Location","text":"<p>The stream location property defines a default folder for the stream in the folder structure of your Persisted steams.</p> <p>For example, the following location:</p> PythonC#   <pre><code>stream.properties.location = \"/Game/Codemasters/F1-2019/{track}\"\n</code></pre>   <pre><code>stream.Properties.Location = $\"/Game/Codemasters/F1-2019/{track}\"\n</code></pre>    <p>Would result in this hierarchy:</p> <p></p> <p>Any streams sent without a location property will be located under the \"Root\" level by default.</p>","location":"sdk/write/#stream-location"},{"title":"Close a Stream","text":"<p>Streams can be left open 24/7 if you aren\u2019t sure when the next data will arrive, but they can and should be closed when you know that you have all the data you need. They will also be closed automatically when your service stops.</p> <p>However, sometimes a stream can be closed for other reasons e.g. if an error occurrs in the writer code or something unexpected happens.</p> <p>These snippets show you how to close a stream and how to specify the StreamEndType.</p> PythonC#   <pre><code>stream.close()\nstream.close(StreamEndType.Closed)\nstream.close(StreamEndType.Aborted)\nstream.close(StreamEndType.Terminated)\n</code></pre>   <pre><code>stream.Close();\nstream.Close(StreamEndType.Closed);\nstream.Close(StreamEndType.Aborted);\nstream.Close(StreamEndType.Terminated);\n</code></pre>    <p>The StreamEndType can be one of:</p>    StreamEndType Description     Closed The stream was closed normally   Aborted The stream was aborted by your code for your own reasons   Terminated The stream was terminated unexpectedly while data was being written    <p>Possible end types</p>","location":"sdk/write/#close-a-stream"},{"title":"Writing Parameter Data","text":"<p>You can now start writing data to your stream. ParameterData is the formal class in the SDK which represents a time-series data packet in memory. ParameterData is meant to be used for time-series data coming from sources that generate data at a regular time basis and with a fixed number of Parameters.</p>  <p>Tip</p> <p>If your data source generates data at irregular time intervals and you don\u2019t have a defined list of regular Parameters, the EventData format is probably a better fit for your time-series data.</p>","location":"sdk/write/#writing-parameter-data"},{"title":"Parameter Data format","text":"<p>ParameterData is the formal class in the SDK which represents a time series data packet in memory.</p> <p>ParameterData consists of a list of Timestamps with their corresponding Parameter Names and Values for each timestamp.</p> <p>You should imagine a Parameter Data as a table where the Timestamp is the first column of that table and where the Parameters are the columns for the Values of that table.</p>    Timestamp Speed Gear     1 120 3   2 123 3   3 125 3   6 110 2    <p>An example of ParameterData</p>  <p>Tip</p> <p>The Timestamp column plus the Tags assigned to it work as the index of that table. If you add values for the same Timestamp and Tags combination, only the last Values will be sent to the stream.</p>  <p>The Quix SDK provides several helpers to create and send ParameterData packets through the stream.</p> <p>The following code would generate the previous ParameterData and send it to the stream:</p> PythonC#   <pre><code>data = ParameterData()\n\ndata.add_timestamp_nanoseconds(1) \\\n    .add_value(\"Speed\", 120) \\\n    .add_value(\"Gear\", 3)\ndata.add_timestamp_nanoseconds(2) \\\n    .add_value(\"Speed\", 123) \\\n    .add_value(\"Gear\", 3)\ndata.add_timestamp_nanoseconds(3) \\\n    .add_value(\"Speed\", 125) \\\n    .add_value(\"Gear\", 3)\ndata.add_timestamp_nanoseconds(6) \\\n    .add_value(\"Speed\", 110) \\\n    .add_value(\"Gear\", 2)\n\nstream.parameters.write(data)\n</code></pre>   <pre><code>var data = new ParameterData();\n\ndata.AddTimestampNanoseconds(1)\n    .AddValue(\"Speed\", 120)\n    .AddValue(\"Gear\", 3);\ndata.AddTimestampNanoseconds(2)\n    .AddValue(\"Speed\", 123)\n    .AddValue(\"Gear\", 3);\ndata.AddTimestampNanoseconds(3)\n    .AddValue(\"Speed\", 125)\n    .AddValue(\"Gear\", 3);\ndata.AddTimestampNanoseconds(6)\n    .AddValue(\"Speed\", 110)\n    .AddValue(\"Gear\", 2);\n\nstream.Parameters.Write(data);\n</code></pre>    <p>Although Quix allows you to send ParameterData to a stream directly, without any buffering, we recommended you use the built-in Buffer feature to achieve high throughput speeds. The following code would send the same ParameterData through a buffer:</p> PythonC#   <pre><code>stream.parameters.buffer.write(data)\n</code></pre>   <pre><code>stream.Parameters.Buffer.Write(data);\n</code></pre>    <p>Visit the Buffer section of this documentation to find out more about the built-in Buffer feature.</p> <p>The Quix SDK allows you to attach any type of data \u2014 Numbers, Strings, or raw Binary data \u2014 to your timestamps. The following code will attach one of each to the same timestamp:</p> PythonC#   <pre><code>data = ParameterData()\n\ndata.add_timestamp(datetime.datetime.utcnow()) \\\n    .add_value(\"ParameterA\", 10) \\\n    .add_value(\"ParameterB\", \"hello\") \\\n    .add_value(\"ParameterC\", bytearray(\"hello, Quix!\", 'utf-8')) # use bytearray to write binary data to a stream.\n</code></pre>   <pre><code>var data = new ParameterData();\n\ndata.AddTimestamp(DateTime.UtcNow)\n    .AddValue(\"ParameterA\", 10)\n    .AddValue(\"ParameterB\", \"hello\")\n    .AddValue(\"ParameterC\", Encoding.ASCII.GetBytes(\"Hello Quix!\")); // Write binary data as a byte array.\n</code></pre>","location":"sdk/write/#parameter-data-format"},{"title":"Timestamps","text":"<p>The Quix SDK supports common date and time formats for timestamps when adding data to a stream.</p> <p>The SDK gives you several helper functions to add new timestamps to <code>Buffer</code>, <code>ParamaterData</code>, and <code>EventData</code> instances with several types of date/time formats.</p> <p>These are all the common helper functions:</p> PythonC#   <ul> <li> <p><code>add_timestamp(datetime: datetime)</code> : Add a new timestamp in     <code>datetime</code> format. Default <code>epoch</code> will never be added to this.</p> </li> <li> <p><code>add_timestamp(time: timedelta)</code> : Add a new timestamp in     <code>timedelta</code> format since the default <code>epoch</code> determined in the     stream.</p> </li> <li> <p><code>add_timestamp_milliseconds(milliseconds: int)</code> : Add a new     timestamp in milliseconds since the default <code>epoch</code> determined     in the stream.</p> </li> <li> <p><code>add_timestamp_nanoseconds(nanoseconds: int)</code> : Add a new     timestamp in nanoseconds since the default <code>epoch</code> determined in     the stream.</p> </li> </ul>   <ul> <li> <p><code>AddTimestamp(DateTime dateTime)</code> : Add a new timestamp in     <code>DateTime</code> format. Default <code>Epoch</code> will never be added to this.</p> </li> <li> <p><code>AddTimestamp(TimeSpan timeSpan)</code> : Add a new timestamp in     <code>TimeSpan</code> format since the default <code>Epoch</code> determined in the     stream.</p> </li> <li> <p><code>AddTimestampMilliseconds(long timeMilliseconds)</code> : Add a new     timestamp in milliseconds since the default <code>Epoch</code> determined     in the stream.</p> </li> <li> <p><code>AddTimestampNanoseconds(long timeNanoseconds)</code> : Add a new     timestamp in nanoseconds since the default <code>Epoch</code> determined in     the stream.</p> </li> </ul>","location":"sdk/write/#timestamps"},{"title":"Epoch","text":"<p>There is a stream property called <code>Epoch</code> (set to 0 by default) that is added to every timestamp (except for datetime formats) when it\u2019s added to the stream. You can use any value you like to act as a base, from which point timestamps will be relative to.</p> <p>The following code indicates to the SDK to add the current date/time to each timestamp added to the stream.</p> PythonC#   <pre><code>stream.epoch = date.today()\n</code></pre>   <pre><code>stream.Epoch = DateTime.Today;\n</code></pre>    <p>Adding data without using Epoch property:</p> PythonC#   <pre><code>stream.parameters.buffer \\\n    .add_timestamp(datetime.datetime.utcnow()) \\\n    .add_value(\"ParameterA\", 10) \\\n    .add_value(\"ParameterB\", \"hello\") \\\n    .write()\n</code></pre>   <pre><code>stream.Parameters.Buffer\n    .AddTimestamp(DateTime.UtcNow)\n    .AddValue(\"ParameterA\", 10)\n    .AddValue(\"ParameterB\", \"hello\")\n    .Write();\n</code></pre>    <p>Or we can add a timestamp 1000ms from the epoch \"Today\":</p> PythonC#   <pre><code>stream.epoch = date.today()\n\nstream.parameters.buffer \\\n    .add_timestamp_milliseconds(1000) \\\n    .add_value(\"ParameterA\", 10) \\\n    .add_value(\"ParameterB\", \"hello\") \\\n    .write()\n</code></pre>   <pre><code>stream.Epoch = DateTime.Today;\n\nstream.Parameters.Buffer\n    .AddTimestampInMilliseconds(1000)\n    .AddValue(\"ParameterA\", 10)\n    .AddValue(\"ParameterB\", \"hello\")\n    .Write();\n</code></pre>","location":"sdk/write/#epoch"},{"title":"Buffer","text":"<p>The Quix SDK provides a built in <code>Buffer</code> to help you achieve high performance data streaming without the complexity of managing underlying streaming technologies. Instead, you just have to configure the buffer with your requirements using the property <code>Buffer</code> present in the <code>Parameters</code> property of your stream.</p> <p>For example the following configuration means that the SDK will send a packet when the size of the buffer reaches 100 timestamps:</p> PythonC#   <pre><code>stream.parameters.buffer.packet_size = 100\n</code></pre>   <pre><code>stream.Parameters.Buffer.PacketSize = 100;\n</code></pre>    <p>Writing a ParameterData to that buffer is as simple as using the <code>Write</code> method of that built-in <code>Buffer</code>, passing the ParameterData to write:</p> PythonC#   <pre><code>stream.parameters.buffer.write(data)\n</code></pre>   <pre><code>stream.Parameters.Buffer.Write(data);\n</code></pre>    <p>The Quix SDK also allows you to write data to the buffer without creating a <code>ParameterData</code> instance explicitly. To do so, you can use the same helper methods that are supported by the <code>ParameterData</code> class like <code>add_timestamp</code>, <code>add_value</code> or <code>add_tag</code>. At the end, use the <code>write</code> method to write that timestamp to the buffer.</p> <p>This is an example of how to write data to the buffer without using an explicit <code>ParameterData</code> instance:</p> PythonC#   <pre><code>stream.parameters.buffer \\\n    .add_timestamp(datetime.datetime.utcnow()) \\\n    .add_value(\"ParameterA\", 10) \\\n    .add_value(\"ParameterB\", \"hello\") \\\n    .add_value(\"ParameterC\", bytearray(\"hello, Quix!\", 'utf-8')) # use bytearray to write binary data to a stream.\n    .write()\n</code></pre>   <pre><code>stream.Parameters.Buffer\n    .AddTimestamp(DateTime.UtcNow)\n    .AddValue(\"ParameterA\", 10)\n    .AddValue(\"ParameterB\", \"hello\")\n    .AddValue(\"ParameterC\", Encoding.ASCII.GetBytes(\"Hello Quix!\")) // Write binary data as a byte array.\n    .Write();\n</code></pre>    <p>You can configure multiple conditions to determine when the Buffer has to release data. If any of these conditions become true, the buffer will release a new packet of data and that data is cleared from the buffer:</p> PythonC#   <ul> <li> <p><code>buffer.buffer_timeout</code>: The maximum duration in milliseconds     for which the buffer will be held before releasing the data. A     packet of data is released when the configured timeout value has     elapsed from the last data received in the buffer.</p> </li> <li> <p><code>buffer.packet_size</code>: The maximum packet size in terms of number     of timestamps. Each time the buffer has this amount of     timestamps, the packet of data is released.</p> </li> <li> <p><code>buffer.time_span_in_nanoseconds</code>: The maximum time between     timestamps in nanoseconds. When the difference between the     earliest and latest buffered timestamp surpasses this number,     the packet of data is released.</p> </li> <li> <p><code>buffer.time_span_in_milliseconds</code>: The maximum time between     timestamps in milliseconds. When the difference between the     earliest and latest buffered timestamp surpasses this number,     the packet of data is released. Note: This is a millisecond     converter on top of <code>time_span_in_nanoseconds</code>. They both work     with the same underlying value.</p> </li> <li> <p><code>buffer.custom_trigger_before_enqueue</code>: A custom function which     is invoked before adding a new timestamp to the buffer. If     it returns true, the packet of data is released before adding     the timestamp to it.</p> </li> <li> <p><code>buffer.custom_trigger</code>: A custom function which is invoked     after adding a new timestamp to the buffer. If it returns     true, the packet of data is released with the entire buffer     content.</p> </li> <li> <p><code>buffer.filter</code>: A custom function to filter the incoming data     before adding it to the buffer. If it returns true, data is     added, otherwise it isn\u2019t.</p> </li> </ul>   <ul> <li> <p><code>Buffer.BufferTimeout</code>: The maximum duration in milliseconds for     which the buffer will be held before releasing the data. A     packet of data is released when the configured timeout value has     elapsed from the last data received in the buffer.</p> </li> <li> <p><code>Buffer.PacketSize</code>: The maximum packet size in terms of number     of timestamps. Each time the buffer has this amount of     timestamps, the packet of data is released.</p> </li> <li> <p><code>Buffer.TimeSpanInNanoseconds</code>: The maximum time between     timestamps in nanoseconds. When the difference between the     earliest and latest buffered timestamp surpasses this number,     the packet of data is released.</p> </li> <li> <p><code>Buffer.TimeSpanInMilliseconds</code>: The maximum time between     timestamps in milliseconds. When the difference between the     earliest and latest buffered timestamp surpasses this number,     the packet of data is released. Note: This is a millisecond     converter on top of <code>time_span_in_nanoseconds</code>. They both work     with the same underlying value.</p> </li> <li> <p><code>Buffer.CustomTriggerBeforeEnqueue</code>: A custom function which is     invoked before adding a new timestamp to the buffer. If it     returns true, the packet of data is released before adding the     timestamp to it.</p> </li> <li> <p><code>Buffer.CustomTrigger</code>: A custom function which is invoked     after adding a new timestamp to the buffer. If it returns     true, the packet of data is released with the entire buffer     content.</p> </li> <li> <p><code>Buffer.Filter</code>: A custom function to filter the incoming data     before adding it to the buffer. If it returns true, data is     added, otherwise it isn\u2019t.</p> </li> </ul>","location":"sdk/write/#buffer"},{"title":"Examples","text":"<p>This buffer configuration will send data every 100ms or, if no data is buffered in the 1 second timout period, it will flush and empty the buffer anyway.</p> PythonC#   <pre><code>stream.parameters.buffer.time_span_in_milliseconds = 100\nstream.parameters.buffer.buffer_timeout = 1000\n</code></pre>   <pre><code>stream.Parameters.Buffer.TimeSpanInMilliseconds = 100;\nstream.Parameters.Buffer.BufferTimeout = 1000;\n</code></pre>    <p>This buffer configuration will send data every 100ms window or if critical data arrives.</p> PythonC#   <pre><code>stream.parameters.buffer.time_span_in_milliseconds = 100\nstream.parameters.buffer.custom_trigger = lambda data: data.timestamps[0].tags[\"is_critical\"] == 'True'\n</code></pre>   <pre><code>stream.Parameters.Buffer.TimeSpanInMilliseconds = 100;\nstream.Parameters.Buffer.CustomTrigger = data =&gt; data.Timestamps[0].Tags[\"is_critical\"] == \"True\";\n</code></pre>","location":"sdk/write/#examples"},{"title":"Parameter Definitions","text":"<p>The Quix SDK allows you to define metadata for parameters and events, to describe them. You can define things like human readable names, descriptions, acceptable ranges of values, etc. Quix uses some of this configuration when visualising data on the platform, but you can use also them in your own models, bridges or visualization implementations.</p> PythonC#   <p>We call this parameter metadata <code>ParameterDefinitions</code>, and all you need to do is to use the <code>add_definition</code> helper function of the <code>stream.parameters</code> property:</p> <pre><code>parameters.add_definition(parameter_id: str, name: str = None, description: str = None)\n</code></pre>   <p>We call this parameter metadata <code>ParameterDefinitions</code>, and all you need to do is to use the <code>AddDefinition</code> helper function of the <code>stream.Parameters</code> property:</p> <pre><code>Parameters.AddDefinition(parameterId: string, name: string = null, description: string = null)\n</code></pre>    <p>Once you have added a new definition, you can attach some additional configuration to it. This is the whole list of visualization and metadata options you can attach to a <code>ParameterDefinition</code>:</p> PythonC#   <ul> <li> <p><code>set_range(minimum_value: float, maximum_value: float)</code> : Set     the minimum and maximum range of the parameter.</p> </li> <li> <p><code>set_unit(unit: str)</code> : Set the unit of the parameter.</p> </li> <li> <p><code>set_format(format: str)</code> : Set the format of the parameter.</p> </li> <li> <p><code>set_custom_properties(custom_properties: str)</code> : Set the custom     properties of the parameter.</p> </li> </ul> <p>Example:</p> <pre><code>stream.parameters \\\n    .add_location(\"vehicle/ecu\") \\\n    .add_definition(\"vehicle-speed\", \"Vehicle speed\", \"Current vehicle speed measured using wheel sensor\") \\\n    .set_unit(\"kmh\") \\\n    .set_range(0, 400)\n</code></pre>   <ul> <li> <p><code>SetRange(double minimumValue, double maximumValue)</code> : Set the     minimum and maximum range of the parameter.</p> </li> <li> <p><code>SetUnit(string unit)</code> : Set the unit of the parameter.</p> </li> <li> <p><code>SetFormat(string format)</code> : Set the format of the parameter.</p> </li> <li> <p><code>SetCustomProperties(string customProperties)</code> : Set the custom     properties of the parameter.</p> </li> </ul> <p>Example:</p> <pre><code>stream.Parameters\n    .AddLocation(\"vehicle/ecu\")\n    .AddDefinition(\"vehicle-speed\", \"Vehicle speed\", \"Current vehicle speed measured using wheel sensor\")\n    .SetUnit(\"kmh\")\n    .SetRange(0, 400);\n</code></pre>    <p>The Min and Max range definition sets the Y axis range in the waveform visualisation view. This definition:</p> PythonC#   <pre><code>.add_definition(\"Speed\").set_range(0, 400)\n</code></pre>   <pre><code>.AddDefinition(\"Speed\").SetRange(0, 400)\n</code></pre>    <p>Will set up this view in Visualise:</p> <p></p> <p>Adding additional <code>Definitions</code> for each parameter allows you to see data with different ranges on the same waveform view:</p> <p></p> <p>You can also define a <code>Location</code> before adding parameter and event definitions. Locations are used to organize the Parameters and Events in hierarchy groups in the data catalogue. To add a Location you should use the <code>add_location</code> method before adding the definitions you want to include in that group.</p> <p>For example, setting this parameter location:</p> PythonC#   <pre><code>stream.parameters \\\n    .add_location(\"/Player/Motion/Car\") \\\n    .add_definition(\"Pitch\") \\\n    .add_definition(\"Roll\") \\\n    .add_definition(\"Yaw\")\n</code></pre>   <pre><code>stream.Parameters\n    .AddLocation(\"/Player/Motion/Car\")\n    .AddDefinition(\"Pitch\")\n    .AddDefinition(\"Roll\")\n    .AddDefinition(\"Yaw\");\n</code></pre>    <p>Will result in this parameter hierarchy in the parameter selection dialogue:</p> <p></p>","location":"sdk/write/#parameter-definitions"},{"title":"Using Data Frames","text":"<p>If you use the Python version of the SDK you can use Pandas DataFrames for reading and writing ParameterData to Quix. The Pandas DataFrames format is just a representation of ParameterData format, where the Timestamp is mapped to a column named <code>time</code> and the rest of the parameters are mapped as columns named as the ParameterId of the parameter. Tags are mapped as columns with the prefix <code>TAG__</code> and the TagId of the tag.</p> <p>For example, the following ParameterData:</p>    Timestamp CarId (tag) Speed Gear     1 car-1 120 3   2 car-2 123 3   3 car-1 125 3   6 car-2 110 2    <p>An example of ParameterData</p> <p>Is represented as the following Pandas DataFrame:</p>    time TAG__CarId Speed Gear     1 car-1 120 3   2 car-2 123 3   3 car-1 125 3   6 car-2 110 2    <p>A representation of ParameterData in a Pandas DataFrame</p> <p>The SDK allows you to write data to Quix using Pandas DataFrames directly. You just need to use the common <code>write</code> methods of the <code>stream.parameters</code> and <code>buffer</code>, passing the Data Frame instead of a ParameterData:</p> <pre><code>df = data.to_panda_frame()\nstream.parameters.buffer.write(df)\n</code></pre> <p>Alternatively, you can convert a Pandas Data Frame to a ParameterData using the method <code>from_panda_frame</code>:</p> <pre><code>data = ParameterData.from_panda_frame(df)\nstream.parameters.buffer.write(data)\n</code></pre>  <p>Tip</p> <p>The conversions from Pandas DataFrames to ParameterData have an intrinsic cost overhead. For high-performance models using Pandas DataFrames, you probably want to directly use Pandas DataFrames methods provided by the SDK that are optimized for doing as few conversions as possible.</p>","location":"sdk/write/#using-data-frames"},{"title":"Writing Events","text":"<p>EventData is the formal class in the SDK which represents an Event data packet in memory. EventData is meant to be used for time series data coming from sources that generate data at irregular intervals or without a defined structure.</p>  <p>Tip</p> <p>If your data source generates data at regular time intervals, or the information can be organized in a fixed list of Parameters, the ParameterData format is probably a better fit for your time-series data.</p>  <p>Writing Events to a stream is identical to writing ParameterData values, although you don\u2019t need to use buffering features because events don\u2019t need high performance throughput.</p>","location":"sdk/write/#writing-events"},{"title":"Event Data format","text":"<p>EventData consists of a record with a Timestamp, an EventId and an EventValue.</p> <p>You should imagine a list of Event Data instances as a simple table of three columns where the Timestamp is the first column of that table and the EventId and EventValue are the second and third columns.</p>    Timestamp EventId EventValue     1 failure23 Gearbox has a failure   2 box-event2 Car has entered to the box   3 motor-off Motor has stopped   6 race-event3 Race has finished    <p>An example of a list of EventData</p> <p>The Quix SDK provides several helpers to create and send EventData packets through the stream.</p> <p>The following code would generate the list of EventData shown in the previous example and send it to the stream:</p> PythonC#   <pre><code>events = []\n\nevents.append(EventData(\"failure23\", 1, \"Gearbox has a failure\"))\nevents.append(EventData(\"box-event2\", 2, \"Car has entered to the box\"))\nevents.append(EventData(\"motor-off\", 3, \"Motor has stopped\"))\nevents.append(EventData(\"race-event3\", 6, \"Race has finished\"))\n\nstream.events.write(events)\n</code></pre>   <pre><code>var events = new List&lt;EventData&gt;();\n\nevents.Add(new EventData(\"failure23\", 1, \"Gearbox has a failure\"));\nevents.Add(new EventData(\"box-event2\", 2, \"Car has entered to the box\"));\nevents.Add(new EventData(\"motor-off\", 3, \"Motor has stopped\"));\nevents.Add(new EventData(\"race-event3\", 6, \"Race has finished\"));\n\nstream.Events.Write(events)\n</code></pre>    <p>The Quix SDK lets you write Events without creating EventData instances explicitly. To do so, you can use the same helpers present in ParameterData format like <code>add_timestamp</code>, <code>add_value</code> or <code>add_tag</code>. At the end, use the <code>write</code> method to write that timestamp to the stream.</p> <p>This is an example of how to write Events to the stream without using explicit EventData instances:</p> PythonC#   <pre><code>stream.events \\\n    .add_timestamp(1) \\\n    .add_value(\"failure23\", \"Gearbox has a failure\") \\\n    .write()\nstream.events \\\n    .add_timestamp(2) \\\n    .add_value(\"box-event2\", \"Car has entered to the box\") \\\n    .write()\nstream.events \\\n    .add_timestamp(3) \\\n    .add_value(\"motor-off\", \"Motor has stopped\") \\\n    .write()\nstream.events \\\n    .add_timestamp(6) \\\n    .add_value(\"race-event3\", \"Race has finished\") \\\n    .write()\n</code></pre>   <pre><code>stream.Events\n    .AddTimestamp(1)\n    .AddValue(\"failure23\", \"Gearbox has a failure\")\n    .Write();\nstream.Events\n    .AddTimestamp(2)\n    .AddValue(\"box-event2\", \"Car has entered to the box\")\n    .Write();\nstream.Events\n    .AddTimestamp(3)\n    .AddValue(\"motor-off\", \"Motor has stopped\")\n    .Write();\nstream.Events\n    .AddTimestamp(6)\n    .AddValue(\"race-event3\", \"Race has finished\")\n    .Write();\n</code></pre>","location":"sdk/write/#event-data-format"},{"title":"Event Definitions","text":"<p>As with parameters, you can attach <code>Definitions</code> to each event.</p> <p>This is the whole list of visualization and metadata options we can attach to a <code>EventDefinition</code>:</p> <ul> <li> <p><code>set_level(level: EventLevel)</code> : Set severity level of the event.</p> </li> <li> <p><code>set_custom_properties(custom_properties: str)</code> : Set the custom     properties of the event.</p> </li> </ul> <p>For example, the following code defines a human readable name and a Severity level for the <code>EventA</code>:</p> PythonC#   <pre><code>stream.events \\\n    .add_definition(\"EventA\", \"The Event A\") \\\n    .set_level(EventLevel.Critical)\n</code></pre>   <pre><code>stream.Events.AddDefinition(\"EventA\", \"The Event A\").SetLevel(EventLevel.Critical);\n</code></pre>","location":"sdk/write/#event-definitions"},{"title":"Tags","text":"<p>The Quix SDK allows you to tag data for ParameterData and EventData packets. Using tags alongside parameters and events helps when indexing persisted data in the database. Tags allow you to filter and group data with fast queries.</p> <p>Tags work as a part of the primary key inside ParameterData and EventData, in combination with the default Timestamp key. If you add data values with the same Timestamps, but a different combination of Tags, the timestamp will be treated as a separate row.</p> <p>For example, the following code:</p> PythonC#   <pre><code>data = ParameterData()\n\ndata.add_timestamp_nanoseconds(1) \\\n    .add_tag(\"CarId\", \"car1\") \\\n    .add_value(\"Speed\", 120) \\\n    .add_value(\"Gear\", 3)\ndata.add_timestamp_nanoseconds(2) \\\n    .add_tag(\"CarId\", \"car1\") \\\n    .add_value(\"Speed\", 123) \\\n    .add_value(\"Gear\", 3)\ndata.add_timestamp_nanoseconds(3) \\\n    .add_tag(\"CarId\", \"car1\") \\\n    .add_value(\"Speed\", 125) \\\n    .add_value(\"Gear\", 3)\n\ndata.add_timestamp_nanoseconds(1) \\\n    .add_tag(\"CarId\", \"car2\") \\\n    .add_value(\"Speed\", 95) \\\n    .add_value(\"Gear\", 2)\ndata.add_timestamp_nanoseconds(2) \\\n    .add_tag(\"CarId\", \"car2\") \\\n    .add_value(\"Speed\", 98) \\\n    .add_value(\"Gear\", 2)\ndata.add_timestamp_nanoseconds(3) \\\n    .add_tag(\"CarId\", \"car2\") \\\n    .add_value(\"Speed\", 105) \\\n    .add_value(\"Gear\", 2)\n</code></pre>   <pre><code>var data = new ParameterData();\n\ndata.AddTimestampNanoseconds(1)\n    .AddTag(\"CarId\", \"car1\")\n    .AddValue(\"Speed\", 120)\n    .AddValue(\"Gear\", 3);\ndata.AddTimestampNanoseconds(2)\n    .AddTag(\"CarId\", \"car1\")\n    .AddValue(\"Speed\", 123)\n    .AddValue(\"Gear\", 3);\ndata.AddTimestampNanoseconds(3)\n    .AddTag(\"CarId\", \"car1\")\n    .AddValue(\"Speed\", 125)\n    .AddValue(\"Gear\", 3);\n\ndata.AddTimestampNanoseconds(1)\n    .AddTag(\"CarId\", \"car2\")\n    .AddValue(\"Speed\", 95)\n    .AddValue(\"Gear\", 2);\ndata.AddTimestampNanoseconds(2)\n    .AddTag(\"CarId\", \"car2\")\n    .AddValue(\"Speed\", 98)\n    .AddValue(\"Gear\", 2);\ndata.AddTimestampNanoseconds(3)\n    .AddTag(\"CarId\", \"car2\")\n    .AddValue(\"Speed\", 105)\n    .AddValue(\"Gear\", 2);\n</code></pre>    <p>Will generate the following ParameterData packet:</p>    Timestamp CarId Speed Gear     1 car1 120 3   1 car2 95 2   2 car1 123 3   2 car2 98 2   3 car1 125 3   3 car2 105 2    <p>ParameterData with tagged data</p>  <p>Warning</p> <p>Tags have to be chosen carefully as excessive cardinality leads to performance degradation in the database. You should use tags only for identifiers and not cardinal values.</p>  <p>Good tagging: This will allow you to query the maximum speed for driver identifier \"Peter\".</p> PythonC#   <pre><code>stream.parameters.buffer \\\n    .add_timestamp(datetime.datetime.utcnow()) \\\n    .add_tag(\"vehicle-plate\", \"SL96 XCX\") \\\n    .add_tag(\"driver-id\", \"Peter\") \\\n    .add_value(\"Speed\", 53) \\\n    .add_value(\"Gear\", 4) \\\n    .write()\n</code></pre>   <pre><code>stream.Parameters.Buffer\n    .AddTimestamp(DateTime.UtcNow)\n    .AddTag(\"vehicle-plate\", \"SL96 XCX\")\n    .AddTag(\"driver-id\", \"Peter\")\n    .AddValue(\"Speed\", 53)\n    .AddValue(\"Gear\", 4)\n    .Write();\n</code></pre>    <p>Bad tagging: This will lead to excessive cardinality as there will be a massive number of different values for the specified tag, Speed.</p> PythonC#   <pre><code>stream.parameters.buffer \\\n    .add_timestamp(datetime.datetime.utcnow()) \\\n    .add_tag(\"Speed\", 53) \\\n    .add_value(\"Gear\", 4) \\\n    .write()\n</code></pre>   <pre><code>stream.Parameters.Buffer\n    .AddTimestamp(DateTime.UtcNow)\n    .AddTag(\"Speed\", 53)\n    .AddValue(\"Gear\", 4)\n    .Write();\n</code></pre>","location":"sdk/write/#tags"},{"title":"Minimal example","text":"<p>This is a minimal code example you can use to write data to a topic using the Quix SDK.</p> PythonC#   <pre><code>import time\nimport datetime\nimport math\n\nfrom quixstreaming import *\n\n# Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument.\nclient = QuixStreamingClient()\n\noutput_topic = client.open_output_topic(TOPIC_ID)\n\nstream = output_topic.create_stream()\n\nstream.properties.name = \"Hello World python stream\"\n\nfor index in range(0, 3000):\n    stream.parameters \\\n        .buffer \\\n        .add_timestamp(datetime.datetime.utcnow()) \\\n        .add_value(\"ParameterA\", index) \\\n        .write()\n    time.sleep(0.01)\nprint(\"Closing stream\")\nstream.close()\n</code></pre>   <pre><code>using System;\nusing System.Threading;\nusing Quix.Sdk.Streaming.Configuration;\n\nnamespace WriteHelloWorld\n{\n    class Program\n    {\n        /// &lt;summary&gt;\n        /// Main will be invoked when you run the application\n        /// &lt;/summary&gt;\n        static void Main()\n        {\n            // Create a client which holds generic details for creating input and output topics\n            var client = new Quix.Sdk.Streaming.QuixStreamingClient();\n\n            using var outputTopic = client.OpenOutputTopic(TOPIC_ID);\n\n            var stream = outputTopic.CreateStream();\n\n            stream.Properties.Name = \"Hello World stream\";\n\n            Console.WriteLine(\"Sending values for 30 seconds\");\n            for (var index = 0; index &lt; 3000; index++)\n            {\n                stream.Parameters.Buffer\n                    .AddTimestamp(DateTime.UtcNow)\n                    .AddValue(\"ParameterA\", index)\n                    .Write();\n\n                Thread.Sleep(10);\n            }\n\n            Console.WriteLine(\"Closing stream\");\n            stream.Close();\n            Console.WriteLine(\"Done!\");\n        }\n    }\n}\n</code></pre>","location":"sdk/write/#minimal-example"},{"title":"Write raw kafka messages","text":"<p>The Quix SDK uses the message brokers' internal protocol for data transmission. This protocol is both data and speed optimized so we do encourage you to use it. For that you need to use the SDK on both producer ( writer ) and consumer ( reader ) sides.</p> <p>However, in some cases, you simply do not have the ability to run the Quix SDK on both sides.</p> <p>To cater for these cases we added the ability to write the raw, unformatted, messages as a byte array. This gives you the freedom to implement the protocol as needed ( e.g. JSON, comma-separated rows ).</p> <p>You can write messages with or without a key. In the following example, we show you how to write 2 messages to Kafka, one message with a key and one without.</p> PythonC#   <pre><code>inp = client.open_raw_output_topic(TOPIC_ID)\n\ndata = bytearray(bytes(\"TEXT CONVERTED TO BYTES\",'utf-8'))\n\n#write value with KEY to kafka\nmessage = RawMessage(data)\nmessage.key = MESSAGE_KEY\nout.write(message)\n\n#write value without key into kafka\nout.write(data)\n</code></pre>   <pre><code>var out = client.OpenRawOutputTopic(TOPIC_ID);\n\ninp = client.OpenRawInputTopic(TOPIC_NAME)\n\nvar data = new byte[]{1,3,5,7,1,43};\n\n//write value with KEY to kafka\nrawWriter.Write(new Streaming.Raw.RawMessage(\n    MESSAGE_KEY,\n    data\n));\n\n//write value withhout key into kafka\nrawWriter.Write(new Streaming.Raw.RawMessage(\n    data\n));\n\ninp.StartReading()\n</code></pre>","location":"sdk/write/#write-raw-kafka-messages"},{"title":"Broker configuration","text":"<p>Message Broker configurations are not an easy part of setting up a new streaming infrastructure. For instance, Kafka configuration has more than 500 different properties across Server, Topic, Consumer and Producer configurations.</p> <p>Quix handles Kafka configuration efficiently and reliably. We maintain these configurations in order to achieve the best performance and reliability when using the Quix SDK together with managed Kafka topics created inside Quix workspaces.</p> <p>In addition, the Quix SDK uses high-security connections using SSL certificates that encrypt all your data between producers and consumers. These types of secured connection require some complex configuration in the Broker and Client side. We take care of this in the SDK so you don\u2019t have to.</p> <p></p> <p>Our templates come with pre-configured client certificates and connection settings, so you don\u2019t need to worry about this configuration complexity.</p> <p>Refer to the Connecting to Quix section for more information.</p>","location":"sdk/features/broker-configuration/"},{"title":"Built-in buffers","text":"<p>If you\u2019re sending data at a high frequency, processing time-series without any buffering involved can be very costly. With smaller messages, data compression cannot be implemented as efficiently. The time spent in serializing and deserializing messages is too high to do it at very high rates.</p> <p>On the other hand, an incorrect buffer strategy can introduce high latency values between extremes, that could be not acceptable in some use cases.</p> <p>The Quix SDK provides you with a very high performance, low latency buffer, combined with easy-to-use configurations for reading and writing to give you absolute freedom in balancing between latency and cost.</p> <p>Buffers in the Quix SDK work at the timestamp level. A buffer accumulates certain timestamps with their related values. A packet containing those timestamps and values as a ParameterData package is then released when certain configured conditions match.</p> <p></p> <p>The logic looks simple, but it\u2019s actually quite complicated when you try to reach a very high performance level, creating at the same time an easy interface for reading and writing time-series data.</p> <p>Our buffer implementation uses short memory allocations and minimizes conversions between raw transport packages and ParameterData format to achieve low CPU and memory consumption, and high throughput. We are happy to say that we have achieved these three things in the SDK \u2014 simplicity, low resource consumption, and very high performance \u2014 and you don\u2019t need to even worry about buffering because it\u2019s provided out-of-box.</p>","location":"sdk/features/builtin-buffers/"},{"title":"Checkpointing","text":"<p>The Quix SDK allows you to do manual checkpointing when you read data from a Topic. This gives you the ability to inform the Message Broker that you have already processed messages up to one point, usually called a checkpoint.</p> <p>This is a very important concept when you are developing high-performance, streaming applications, processing tons of data in memory. You don\u2019t want to persist a state for each message received because it would cause an unaffordable processing cost, slowing down your streaming speeds and performance.</p> <p>Checkpointing lets you do some of this costly processing at a very low frequency, without having to worry about losing data. If, for some reason, your process is restarted or crashes and you haven\u2019t saved all the in-memory data you are processing, the Message Broker will resend all the messages from the last Checkpoint when you reconnect to the topic.</p> <p>Refer to the Committing / checkpointing section of this documentation to find out how to do Checkpointing when reading data with the Quix Sdk.</p>","location":"sdk/features/checkpointing/"},{"title":"Checkpointing example","text":"<p>Let\u2019s explain the checkpointing concept and its benefits with an easy example.</p> <p>One process is reading and processing data, without saving its state after each input message. This allows good performance and high throughtput of the service but, without checkpointing, risks data loss in the case of failure.</p> <ol> <li> <p>The process reads the first four messages, keeping its state in     memory.</p> <p></p> </li> <li> <p>The process commits the messages of the topic (checkpointing) just     after reading the first four and saves the in-memory state to the     database.</p> <p></p> </li> <li> <p>The process reads the next four messages, but it crashes just after     that, without time to commit the messages. This will not result in     data loss because it will begin from the last checkpoint after it     restarts.</p> <p></p> </li> <li> <p>The process restarts and reopens the input topic. It will start     reading messages from the last checkpoint resulting in no data loss     from the previous crash.</p> <p></p> </li> <li> <p>The process resumes reading the next five messages, keeping its     state in memory.</p> <p></p> </li> <li> <p>The process commits the messages of the topic just after reading the     previous five messages and saves the in-memory state to the     database.</p> <p></p> </li> </ol>","location":"sdk/features/checkpointing/#checkpointing-example"},{"title":"Support for Data Frames","text":"<p>The Quix SDK supports reading and writing data using Pandas DataFrames. If you use the Python version of the SDK you can make use of this library together with Quix, ensuring maximum optimization for your real-time applications.</p> <p>The SDK uses Pandas DataFrames just as a representation of the common ParameterData format used to read and write data to Quix.</p> <p>For example, the following ParameterData:</p>    Timestamp CarId (tag) Speed Gear     1 car-1 120 3   2 car-2 123 3   3 car-1 125 3   6 car-2 110 2    <p>An example of ParameterData</p> <p>Is represented as the following Pandas Data Frame:</p>    time TAG__CarId Speed Gear     1 car-1 120 3   2 car-2 123 3   3 car-1 125 3   6 car-2 110 2    <p>A representation of ParameterData in a Pandas Data Frame</p> <p>The Quix SDK provides multiple methods and events that work directly with Pandas DataFrames.</p> <p>Please refer to the sections Using Data Frames for reading from Quix and Using Data Frames for writing to Quix for extended information.</p>","location":"sdk/features/data-frames/"},{"title":"Data serialization","text":"<p>Serialization can be painful, especially if it\u2019s done with performance in mind. We serialize and deserialize native ParameterData transport objects, created specifically to be efficient with time series data. On top of that we use codecs like Protobuf that improve overall performance of the serialization and deserialization process by some orders of magnitude.</p> <p></p> <p>The Quix SDK automatically serializes data from native types in your language. You can work with familiar types, such as Pandas DataFrames, or use our own ParameterData class without worrying about the conversions that happen behind the scenes.</p>","location":"sdk/features/data-serialization/"},{"title":"Horizontal scaling","text":"<p>The Quix SDK provides horizontal scaling out of the box via the streaming context. This means a data scientist or data engineer does not have to implement parallel processing themselves.</p> <p>Imagine the following example:</p> <p></p> <p>Each car produces one stream with its own time-series data, and each stream is processed by each replica of the deployment, labelled \"Process\". By default the message broker will assign each stream to one replica via the RangeAssignor strategy.</p> <p>Now, one of the replicas crashes (the purple one), and the \"stream 4\" is assigned automatically to the blue replica.</p> <p></p> <p>This situation will trigger an event on the SDK in the blue replica indicating that \"stream 4\" has been received:</p> PythonC#   <pre><code>def read_stream(new_stream: StreamReader):\n    print(\"New stream received:\" + new_stream.stream_id)\n\ninput_topic.on_stream_received += read_stream\n</code></pre>   <pre><code>inputTopic.OnStreamReceived += (s, newStream) =&gt;\n{\n    Console.WriteLine($\"New stream received: {newStream.StreamId}\");\n};\n</code></pre>    <p>output on blue replica:</p> <pre><code>New stream received: stream 4\n</code></pre> <p>When the purple replica has restarted and becomes available again, it takes back control of \"stream 4\".</p> <p></p> <p>This will trigger two events, one in the blue replica indicating that \"stream 4\" has been revoked, and one in the purple replica indicating that \"stream 4\" has been assigned again:</p> PythonC#   <pre><code>def read_stream(new_stream: StreamReader):\n    print(\"New stream received:\" + new_stream.stream_id)\n\ndef streams_revoked(streams_revoked: [StreamReader]):\n    for stream in streams_revoked:\n        print(\"Stream revoked:\" + stream.stream_id)\n\ninput_topic.on_stream_received += read_stream\ninput_topic.on_streams_revoked += streams_revoked\n</code></pre>   <pre><code>inputTopic.OnStreamReceived += (s, newStream) =&gt;\n{\n    Console.WriteLine($\"New stream received: {newStream.StreamId}\");\n};\n\ninputTopic.OnStreamsRevoked += (s, streamsRevoked) =&gt;\n{\n    foreach (var stream in streamsRevoked)\n    {\n        Console.WriteLine($\"Stream revoked: {stream.StreamId}\");\n    }\n};\n</code></pre>    <p>Output on the blue replica:</p> <pre><code>Stream revoked: stream 4\n</code></pre> <p>Output on the purple replica:</p> <pre><code>New stream received: stream 4\n</code></pre> <p>The same behaviour will happen if we scale the \"Process\" deployment up or down, increasing or decreasing the number of replicas. Kafka will trigger the rebalacing mechanism internally and this will trigger the same events on the Quix SDK.</p>","location":"sdk/features/horizontal-scaling/"},{"title":"Rebalancing mechanism and Partitions","text":"<p>Kafka uses partitions and the RangeAssignor strategy to decide which consumers receive which messages. Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation behind the Quix SDK. You don\u2019t need to worry about them because everything is abstracted within the Streaming Context feature of the SDK. The events described above will remain the same, even if the SDK uses another Message Broker technology or another rebalancing mechanism in the future.</p>  <p>Warning</p> <p>Because of how the Kafka rebalancing mechanism works, you should follow one golden rule: you cannot have more replicas than the number of partitions the input Topic has.</p>","location":"sdk/features/horizontal-scaling/#rebalancing-mechanism-and-partitions"},{"title":"In-memory processing","text":"<p>Traditional architectures for applications that need to process data have always been very database-centric. This means that, when you needed to process data and get some value out of it, everything had to pass through a database several times. This approach worked when the amount of data to process was relatively low, and the latency needed was on the scale of \"days\". But with a world changing to more real-time use cases where you need results on the scale of seconds or nanoseconds, and where you can get millions of IoT devices sending data to process at the same time, traditional database-centric architectures just explode.</p> <p></p> <p>The Quix SDK uses a message broker and it puts it at the very center of the application, enabling a new approach for processing data without the need to save and pass all the information through a database. By using in-memory processing, you can persist only the data you\u2019re really interested in keeping.</p> <p></p> <p>This approach lowers the complexity and cost of real-time data processing by several orders of magnitude and, in fact, it is the only possible approach when you need to process a huge amount of data per second with low latency requirements.</p> <p>The SDK offers you a very simple way of processing time-series data in real-time. Refer to the Processing data section of this documentation for further details.</p>","location":"sdk/features/in-memory-processing/"},{"title":"Integrations","text":"<p>The Quix SDK offers very useful integrations with the Quix SaaS platform out of the box, some of them to persist and query historical data, and some of them to produce and consume data for uses cases where you cannot use the Quix SDK directly, like Web applications:</p> <ul> <li> <p>Data persistence: Make any Topic persist all the streaming data it     receives, with the click of a button on Quix Portal.</p> </li> <li> <p>Query API: Query historical     data with powerful filters and aggregation capabilities.</p> </li> <li> <p>Streaming Reader API:     Consume stream data in real time with a Websockets interface. Used     for Web applications use cases.</p> </li> <li> <p>Streaming Writer API: Push     streaming data into a Topic using REST via HTTP endpoints. Used for     Web applications use cases.</p> </li> </ul>","location":"sdk/features/integrations/"},{"title":"Message compression","text":"<p>The Quix SDK uses efficient ParameterData transport objects to transmit messages through the message broker, reducing them by an average factor of 10 times compared with plain JSON conversion.</p> <p>On top of that, we apply codecs like Protobuf and Gzip compression to achieve the best performance possible with the minimum payload.</p> <p></p> <p>All these improvements are completely transparent and you don\u2019t even need to worry about them because everything happens behind the scenes.</p>","location":"sdk/features/message-compression/"},{"title":"Message splitting","text":"<p>Message Brokers always have some limitations, by design, with the maximum size of message they can handle. For example, Kafka has a 1MB limit by default, and it\u2019s not recommended to increase this number, for performance reasons. This can be a problem in some use cases where time-series data are big binary chunks, such as sound or video.</p> <p>The Quix SDK automatically handles large messages on the producer side, splitting them up if required and merging them back together at the consumer side, in a totally transparent way.</p> <p></p> <p>This feature gives you an unlimited message size, independent of the Message Broker used behind.</p>","location":"sdk/features/message-splitting/"},{"title":"Multiple data types","text":"<p>The Quix SDK lets you attach any type of data \u2014 Numbers, Strings, or raw Binary data \u2014 to your timestamps.</p> <p>For example, with Quix you can send telemetry data from your vehicle or IoT device and attach a picture or video frame to the same timestamp.</p> <p></p> <p>This gives the SDK the ability to adapt to any streaming application use case, from plain telemetry data to video streaming, or a mix of both.</p>","location":"sdk/features/multiple-data-types/"},{"title":"Portability","text":"<p>The Quix SDK works as an abstraction layer on top of a concrete broker technology like Kafka.</p> <p>The SDK sits on top of a Kafka layer and solves all the common problems you might face when processing time-series data using the Kafka technology. However, in future, we can use different message broker technologies without changing the common SDK interface.</p> <p></p> <p>Using the Quix SDK, you\u2019re not locked into a specific broker technology. You can innovate over time and change the underlying technologies without changing a single line of code of your solution.</p>","location":"sdk/features/portability/"},{"title":"Streaming context","text":"<p>Using a plain Broker SDK out of box only enables you to send messages independently, without any relationship between them.</p> <p></p> <p>The Quix SDK handles stream context for you, so all the data from one data source is bundled in the same scope. This supports, among other things, automatic horizontal scaling of your models when you deal with a undertermined or big number of data sources.</p> <p></p> <p>The SDK simplifies the processing of streams by providing callbacks on the reading side. When processing stream data, you can identify data from different streams more easily than with the key-value approach and single messages used by other technologies.</p> <p>The SDK also allows you to attach metadata to streams, like ids, references, or any other type of information related to the data source.</p> <p></p> <p>This metadata can be read in real time by the SDK itself or via the Query API, if you choose to persist the streams into the database.</p>","location":"sdk/features/streaming-context/"}]}